{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Autocovariances and an automatic Newey–West lag rule"
      ],
      "metadata": {
        "id": "84EkFgkpjcLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FvbElucoUflF"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "def _autocovariances(x: Array, max_lag: int) -> Array:\n",
        "    \"\"\"Sample autocovariances gamma_k = cov(x_t, x_{t-k}) for k=0..max_lag (denominator n).\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x - x.mean()\n",
        "    n = x.size\n",
        "    gammas = np.empty(max_lag + 1, dtype=float)\n",
        "    for k in range(max_lag + 1):\n",
        "        gammas[k] = np.dot(x[k:], x[:n - k]) / n\n",
        "    return gammas\n",
        "\n",
        "def _nw_default_lag(n: int) -> int:\n",
        "    \"\"\"Andrews-style automatic bandwidth: floor(4 * (n/100)^(2/9)), at least 1.\"\"\"\n",
        "    L = int(np.floor(4.0 * (n / 100.0) ** (2.0 / 9.0)))\n",
        "    return max(L, 1)\n",
        "\n",
        "def hac_se_mean(x: Array, L: int | None = None) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Newey–West (HAC) standard error for the sample mean.\n",
        "    Returns (se_mean, var_mean, nw_lag). Weights are Bartlett: w_k = 1 - k/(L+1).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    n = x.size\n",
        "    if L is None:\n",
        "        L = _nw_default_lag(n)\n",
        "    gam = _autocovariances(x, L)                  # gamma_0..gamma_L\n",
        "    weights = 1.0 - (np.arange(L + 1) / (L + 1))  # Bartlett\n",
        "    var_mean = (gam[0] + 2.0 * np.sum(weights[1:] * gam[1:])) / n\n",
        "    se_mean = np.sqrt(max(var_mean, 0.0))\n",
        "    return se_mean, var_mean, L\n",
        "\n",
        "def lo_corrected_sharpe(x: Array, q: int) -> float:\n",
        "    \"\"\"\n",
        "    Lo (2002) autocorrelation-aware annualization of Sharpe.\n",
        "    x: per-bar net returns (after frictions)\n",
        "    q: bars per year at the sampling frequency of x\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    n = x.size\n",
        "    mu = x.mean()\n",
        "    sigma = x.std(ddof=1)\n",
        "    if sigma == 0.0:\n",
        "        return np.nan\n",
        "    sr_1 = mu / sigma\n",
        "\n",
        "    K = min(q - 1, n - 1) if q > 1 else 0\n",
        "    if K <= 0:\n",
        "        return sr_1 * np.sqrt(max(q, 1))\n",
        "\n",
        "    gam = _autocovariances(x, K)\n",
        "    if gam[0] <= 0:\n",
        "        return sr_1 * np.sqrt(q)\n",
        "    rho = gam[1:] / gam[0]\n",
        "\n",
        "    denom = 1.0 + 2.0 * np.sum(((q - np.arange(1, K + 1)) / q) * rho)\n",
        "    denom = max(denom, 1e-12)\n",
        "    return sr_1 * np.sqrt(q / denom)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stationary block bootstrap indices"
      ],
      "metadata": {
        "id": "arkhKpt8jn5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _stationary_bootstrap_indices(n: int, b: int, rng: np.random.Generator) -> Array:\n",
        "    \"\"\"\n",
        "    Stationary bootstrap (Politis & Romano) indices with expected block length b.\n",
        "    Geometric block lengths with parameter p=1/b; blocks start uniformly at random.\n",
        "    \"\"\"\n",
        "    p = 1.0 / float(b)\n",
        "    idx = np.empty(n, dtype=int)\n",
        "    t = 0\n",
        "    while t < n:\n",
        "        start = rng.integers(0, n)\n",
        "        L = 1\n",
        "        while (t + L < n) and (rng.random() > p):\n",
        "            L += 1\n",
        "        jmax = min(L, n - t)\n",
        "        idx[t:t + jmax] = (start + np.arange(jmax)) % n\n",
        "        t += jmax\n",
        "    return idx\n",
        "\n",
        "def lo_sharpe_bootstrap_ci(\n",
        "    x: Array, q: int, B: int = 2000, b: int | None = None,\n",
        "    alpha: float = 0.05, random_state: int = 123\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Stationary block bootstrap CI for Lo-corrected annualized Sharpe.\n",
        "    Returns (lower, upper) percentile CI.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    n = x.size\n",
        "    if b is None:\n",
        "        b = max(5, int(round(n ** (1.0 / 3.0))))\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    stats = np.empty(B, dtype=float)\n",
        "    for _ in range(B):\n",
        "        idx = _stationary_bootstrap_indices(n, b, rng)\n",
        "        xb = x[idx]\n",
        "        stats[_] = lo_corrected_sharpe(xb, q)\n",
        "    lo, hi = np.quantile(stats, [alpha / 2.0, 1.0 - alpha / 2.0])\n",
        "    return float(lo), float(hi)"
      ],
      "metadata": {
        "id": "0J_I9_CWjtnl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robustness to outliers"
      ],
      "metadata": {
        "id": "pg-oJMNTj1WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse_mae_comparison(x: np.ndarray, reference: float = 0.0) -> dict:\n",
        "    \"\"\"\n",
        "    Compare Mean Squared Error (MSE) and Mean Absolute Error (MAE)\n",
        "    of a return series x relative to a reference value (default 0.0).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    errors = x - reference\n",
        "    mse = np.mean(errors ** 2)\n",
        "    mae = np.mean(np.abs(errors))\n",
        "    return {\"MSE\": mse, \"MAE\": mae}"
      ],
      "metadata": {
        "id": "Ob7lU91Hj3n3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asymmetric penalties"
      ],
      "metadata": {
        "id": "rubAyXTHj5nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, tau: float) -> float:\n",
        "    \"\"\"\n",
        "    Quantile (pinball) loss function.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
        "    errors = y_true - y_pred\n",
        "    loss = np.where(errors >= 0, tau * errors, (1 - tau) * -errors)\n",
        "    return float(np.mean(loss))\n"
      ],
      "metadata": {
        "id": "ouMaFGr5j98R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic evaluation and model stability"
      ],
      "metadata": {
        "id": "4kwQclsSkJs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Literal, Optional, Tuple\n",
        "\n",
        "LossName = Literal[\"mse\", \"mae\", \"quantile\"]\n",
        "\n",
        "def _loss_vector(y_true: np.ndarray,\n",
        "                 y_pred: np.ndarray,\n",
        "                 loss: LossName = \"mse\",\n",
        "                 tau: Optional[float] = None) -> np.ndarray:\n",
        "    \"\"\"Pointwise loss ℓ_t = ℓ(y_t, ŷ_t).\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    e = y_true - y_pred\n",
        "    if loss == \"mse\":\n",
        "        return e**2\n",
        "    elif loss == \"mae\":\n",
        "        return np.abs(e)\n",
        "    elif loss == \"quantile\":\n",
        "        if tau is None or not (0.0 < tau < 1.0):\n",
        "            raise ValueError(\"For quantile loss, provide tau in (0,1).\")\n",
        "        return np.where(e >= 0.0, tau*e, (1.0 - tau)*(-e))\n",
        "    else:\n",
        "        raise ValueError(\"loss must be 'mse', 'mae', or 'quantile'.\")\n",
        "\n",
        "def rolling_error(y_true: np.ndarray,\n",
        "                  y_pred: np.ndarray,\n",
        "                  window: int,\n",
        "                  loss: LossName = \"mse\",\n",
        "                  tau: Optional[float] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Rolling error series: average loss over a moving window of size 'window'.\n",
        "    Returns an array of length n with NaN for the initial warmup (window-1).\n",
        "    \"\"\"\n",
        "    l = _loss_vector(y_true, y_pred, loss=loss, tau=tau)\n",
        "    n = l.size\n",
        "    out = np.full(n, np.nan)\n",
        "    # rolling mean via cumulative sum (O(n))\n",
        "    cs = np.insert(np.cumsum(l), 0, 0.0)\n",
        "    vals = (cs[window:] - cs[:-window]) / float(window)\n",
        "    out[window-1:] = vals\n",
        "    return out\n",
        "\n",
        "def ew_error(y_true: np.ndarray,\n",
        "             y_pred: np.ndarray,\n",
        "             lam: float = 0.97,\n",
        "             loss: LossName = \"mse\",\n",
        "             tau: Optional[float] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Exponentially weighted error: EW_t = (1-λ) * Σ_{j>=0} λ^j ℓ_{t-j}.\n",
        "    λ close to 1 => slower decay (more memory).\n",
        "    \"\"\"\n",
        "    l = _loss_vector(y_true, y_pred, loss=loss, tau=tau)\n",
        "    ew = np.empty_like(l)\n",
        "    acc = 0.0\n",
        "    one_minus = 1.0 - lam\n",
        "    for t, lt in enumerate(l):\n",
        "        acc = lam*acc + one_minus*lt\n",
        "        ew[t] = acc\n",
        "    return ew\n",
        "\n",
        "def cusum_alarm(series: np.ndarray,\n",
        "                mu0: Optional[float] = None,\n",
        "                k: float = 0.0,\n",
        "                h: float = 3.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    One-sided CUSUM (increase detection) on a loss series.\n",
        "    g_t = max(0, g_{t-1} + (x_t - mu0 - k)); alarm if g_t > h·σ0.\n",
        "    σ0 estimated from a robust scale of the first 10% (or use naive std).\n",
        "    Returns (g, threshold, alarms_mask).\n",
        "    \"\"\"\n",
        "    x = np.asarray(series, dtype=float)\n",
        "    n = x.size\n",
        "    # baseline mean/scale from an initial stable segment\n",
        "    m = max(5, int(np.ceil(0.1*n)))\n",
        "    x0 = x[:m][np.isfinite(x[:m])]\n",
        "    mu0 = float(np.nanmean(x0)) if mu0 is None else float(mu0)\n",
        "    sigma0 = float(np.nanstd(x0, ddof=1)) if np.isfinite(x0).any() else 1.0\n",
        "    thr = h * sigma0\n",
        "\n",
        "    g = np.zeros(n, dtype=float)\n",
        "    alarms = np.zeros(n, dtype=bool)\n",
        "    for t in range(n):\n",
        "        xt = x[t]\n",
        "        if not np.isfinite(xt):\n",
        "            g[t] = g[t-1] if t > 0 else 0.0\n",
        "            continue\n",
        "        inc = xt - mu0 - k\n",
        "        g[t] = max(0.0, (g[t-1] if t > 0 else 0.0) + inc)\n",
        "        if g[t] > thr:\n",
        "            alarms[t] = True\n",
        "            g[t] = 0.0  # optional: reset after alarm\n",
        "    thresh_series = np.full(n, thr)\n",
        "    return g, thresh_series, alarms"
      ],
      "metadata": {
        "id": "4bnsuBs-kNQt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration for a scaled Beta daily-range model"
      ],
      "metadata": {
        "id": "qE5t8-flkQpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Optional, Dict\n",
        "\n",
        "def fit_beta_mom(u: np.ndarray, eps: float = 1e-8) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Method-of-Moments fit for Beta(alpha, beta) on u in (0,1).\n",
        "    When v is very small, t can become non-positive due to numerical issues;\n",
        "    we then fall back to a large-concentration Beta centered at m.\n",
        "    \"\"\"\n",
        "    u = np.asarray(u, dtype=float)\n",
        "    u = u[np.isfinite(u)]\n",
        "    u = u[(u > 0.0) & (u < 1.0)]  # strict interior for stability\n",
        "    if u.size < 5:\n",
        "        raise ValueError(\"Need at least 5 observations strictly inside (0,1).\")\n",
        "\n",
        "    m = float(np.mean(u))\n",
        "    v = float(np.var(u))  # population variance\n",
        "    v = max(v, eps)\n",
        "    t = m * (1.0 - m) / v - 1.0\n",
        "\n",
        "    if t <= 0.0:\n",
        "        # Near-degenerate case: collapse to a high-concentration Beta around m\n",
        "        k = 1e3\n",
        "        alpha = max(m * k, eps)\n",
        "        beta = max((1.0 - m) * k, eps)\n",
        "    else:\n",
        "        alpha = max(m * t, eps)\n",
        "        beta  = max((1.0 - m) * t, eps)\n",
        "    return alpha, beta\n",
        "\n",
        "\n",
        "def fit_scaled_beta_from_hlc(\n",
        "    high: np.ndarray,\n",
        "    low: np.ndarray,\n",
        "    x_in_range: np.ndarray,\n",
        "    eps: float = 1e-12\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Fit a scaled Beta to normalized daily positions u = (x - low) / (high - low).\n",
        "    Days with non-finite values or zero/near-zero ranges are discarded.\n",
        "    Observations exactly at the bounds (u=0 or u=1) are included in diagnostics\n",
        "    but are excluded from the MoM fit to avoid instability.\n",
        "    \"\"\"\n",
        "    high = np.asarray(high, dtype=float)\n",
        "    low  = np.asarray(low,  dtype=float)\n",
        "    x    = np.asarray(x_in_range, dtype=float)\n",
        "\n",
        "    if not (len(high) == len(low) == len(x)):\n",
        "        raise ValueError(\"high, low, and x_in_range must be the same length.\")\n",
        "\n",
        "    width = high - low\n",
        "    valid = (\n",
        "        np.isfinite(high) & np.isfinite(low) & np.isfinite(x) &\n",
        "        (width > eps) & (x >= low - 1e-12) & (x <= high + 1e-12)\n",
        "    )\n",
        "    if not np.any(valid):\n",
        "        raise ValueError(\"No valid observations after filtering.\")\n",
        "\n",
        "    u_all = (x[valid] - low[valid]) / width[valid]\n",
        "    # Diagnostics over [0,1]\n",
        "    mean_u = float(np.mean(u_all))\n",
        "    var_u  = float(np.var(u_all))\n",
        "\n",
        "    # Strict interior for MoM stability\n",
        "    u_fit = u_all[(u_all > 0.0) & (u_all < 1.0)]\n",
        "    if u_fit.size < 5:\n",
        "        raise ValueError(\"Not enough interior points in (0,1) to fit Beta.\")\n",
        "\n",
        "    alpha, beta = fit_beta_mom(u_fit)\n",
        "\n",
        "    return {\n",
        "        \"alpha\": float(alpha),\n",
        "        \"beta\":  float(beta),\n",
        "        \"mean_u\": mean_u,\n",
        "        \"var_u\":  var_u,\n",
        "        \"n_used\": int(u_all.size),\n",
        "    }"
      ],
      "metadata": {
        "id": "DXU-KFSXkQ21"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration for L-moments"
      ],
      "metadata": {
        "id": "lIb_-a-zkWSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import comb\n",
        "\n",
        "def l_moments_from_scratch(returns: np.ndarray) -> dict:\n",
        "    \"\"\"\n",
        "    This implementation is based on the relationships between L-moments and\n",
        "    Probability Weighted Moments (PWMs).\n",
        "    \"\"\"\n",
        "    # 1. --- Data Preparation ---\n",
        "    returns = np.asarray(returns, dtype=float)\n",
        "    returns = returns[~np.isnan(returns)]\n",
        "    n = len(returns)\n",
        "\n",
        "    if n < 4:\n",
        "        return {\n",
        "            \"L-location (mean)\": np.nan,\n",
        "            \"L-scale (dispersion)\": np.nan,\n",
        "            \"L-skewness\": np.nan,\n",
        "            \"L-kurtosis\": np.nan,\n",
        "        }\n",
        "\n",
        "    # Sort the returns to get order statistics\n",
        "    x_sorted = np.sort(returns)\n",
        "\n",
        "    # 2. --- Calculate Probability Weighted Moments (PWMs) ---\n",
        "    # Unbiased sample PWM estimators b_r = 1/n * sum_{i=r+1 to n} [C(i-1,r)/C(n-1,r) * x_i]\n",
        "    b = np.zeros(4)\n",
        "    b[0] = np.mean(x_sorted) # b_0 is the sample mean\n",
        "\n",
        "    # More efficient vectorized calculation for b_1, b_2, b_3\n",
        "    # Denominators\n",
        "    d1 = n * (n - 1)\n",
        "    d2 = d1 * (n - 2)\n",
        "    d3 = d2 * (n - 3)\n",
        "\n",
        "    # Numerator sums\n",
        "    i = np.arange(1, n + 1)\n",
        "    s1 = np.sum((i[1:] - 1) * x_sorted[1:])\n",
        "    s2 = np.sum((i[2:] - 1) * (i[2:] - 2) * x_sorted[2:])\n",
        "    s3 = np.sum((i[3:] - 1) * (i[3:] - 2) * (i[3:] - 3) * x_sorted[3:])\n",
        "\n",
        "    if d1 > 0: b[1] = s1 / d1\n",
        "    if d2 > 0: b[2] = s2 / d2\n",
        "    if d3 > 0: b[3] = s3 / d3\n",
        "\n",
        "    # 3. --- Convert PWMs to L-moments ---\n",
        "    l1 = b[0]\n",
        "    l2 = 2 * b[1] - b[0]\n",
        "    l3 = 6 * b[2] - 6 * b[1] + b[0]\n",
        "    l4 = 20 * b[3] - 30 * b[2] + 12 * b[1] - b[0]\n",
        "\n",
        "    # 4. --- Calculate L-moment Ratios (dimensionless) ---\n",
        "    # Handle division by zero if L-scale is zero\n",
        "    if l2 == 0:\n",
        "        l_skew = np.nan\n",
        "        l_kurt = np.nan\n",
        "    else:\n",
        "        l_skew = l3 / l2\n",
        "        l_kurt = l4 / l2\n",
        "\n",
        "    return {\n",
        "        \"L-location (mean)\": l1,\n",
        "        \"L-scale (dispersion)\": l2,\n",
        "        \"L-skewness\": l_skew,\n",
        "        \"L-kurtosis\": l_kurt}"
      ],
      "metadata": {
        "id": "vLHSnoPFkWi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration for GMM"
      ],
      "metadata": {
        "id": "EA3mvUKRkbIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def gmm_objective(theta, X, y, weight_matrix):\n",
        "    \"\"\"\n",
        "    Compute the GMM objective function.\n",
        "    Moment condition: E[(y - X*theta) * X] = 0\n",
        "    \"\"\"\n",
        "    residuals = y - X @ theta\n",
        "    # Sample moment conditions: g_n(theta)\n",
        "    moments = X.T @ residuals / len(y)\n",
        "    # Quadratic form with weighting matrix\n",
        "    return moments.T @ weight_matrix @ moments\n",
        "\n",
        "# --- Example usage ---\n",
        "np.random.seed(42)\n",
        "n, k = 200, 2\n",
        "X = np.column_stack((np.ones(n), np.random.randn(n)))\n",
        "true_theta = np.array([1.0, 2.0])\n",
        "y = X @ true_theta + 0.5 * np.random.randn(n)\n",
        "\n",
        "# Identity matrix as initial weight\n",
        "W = np.eye(k)\n",
        "\n",
        "# Initial guess for theta\n",
        "theta0 = np.zeros(k)\n",
        "\n",
        "res = minimize(gmm_objective, theta0, args=(X, y, W), method=\"BFGS\")\n",
        "theta_hat = res.x\n",
        "print(\"GMM estimates:\", theta_hat)"
      ],
      "metadata": {
        "id": "AgWC1JIZkbU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration for confidence interval"
      ],
      "metadata": {
        "id": "mLUUInvIkfxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import binom\n",
        "\n",
        "def median_ci_sign_test(data, alpha=0.05, grid_points=200):\n",
        "    \"\"\"\n",
        "    Confidence interval for the median using the Sign Test.\n",
        "    \"\"\"\n",
        "    data = np.asarray(data)\n",
        "    N = len(data)\n",
        "\n",
        "    # Candidate grid between min and max\n",
        "    candidates = np.linspace(np.min(data), np.max(data), grid_points)\n",
        "\n",
        "    plausible = []\n",
        "    for mu0 in candidates:\n",
        "        greater = np.sum(data > mu0)\n",
        "        # Two-sided p-value under Binomial(N, 0.5)\n",
        "        p_val = 2 * min(\n",
        "            binom.cdf(greater, N, 0.5),\n",
        "            1 - binom.cdf(greater - 1, N, 0.5)\n",
        "        )\n",
        "        if p_val > alpha:\n",
        "            plausible.append(mu0)\n",
        "\n",
        "    if not plausible:\n",
        "        return None\n",
        "\n",
        "    return min(plausible), max(plausible)"
      ],
      "metadata": {
        "id": "nnettmfykgAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration of intervals for risk-adjusted performance metrics"
      ],
      "metadata": {
        "id": "Ow7L0nqykkpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_sortino(returns, target=0, periods=252):\n",
        "    \"\"\"Calculates the annualized Sortino ratio.\"\"\"\n",
        "    mean_return = returns.mean() * periods\n",
        "    downside_returns = returns[returns < target]\n",
        "    downside_std = downside_returns.std() * np.sqrt(periods)\n",
        "    if downside_std == 0:\n",
        "        return np.inf\n",
        "    return (mean_return - target) / downside_std\n",
        "\n",
        "def calculate_calmar(returns, periods=252):\n",
        "    \"\"\"Calculates the annualized Calmar ratio.\"\"\"\n",
        "    cagr = (1 + returns.mean()) ** periods - 1\n",
        "    equity_curve = (1 + returns).cumprod()\n",
        "    running_max = equity_curve.cummax()\n",
        "    drawdown = (equity_curve - running_max) / running_max\n",
        "    mdd = abs(drawdown.min())\n",
        "    if mdd == 0:\n",
        "        return np.inf\n",
        "    return cagr / mdd\n",
        "\n",
        "def block_bootstrap(data, block_size):\n",
        "    \"\"\"Generates one bootstrap sample using the moving block bootstrap.\"\"\"\n",
        "    n = len(data)\n",
        "    num_blocks = n // block_size\n",
        "    block_starts = np.random.randint(0, n - block_size + 1, size=num_blocks)\n",
        "    indices = [np.arange(start, start + block_size) for start in block_starts]\n",
        "    bootstrap_indices = np.concatenate(indices)\n",
        "    return data.iloc[bootstrap_indices[:n]]\n",
        "\n",
        "def bootstrap_distribution(returns, metric_func, block_size=22, n_bootstrap=5000):\n",
        "    \"\"\"Generates a bootstrap distribution for a given performance metric.\"\"\"\n",
        "    bootstrap_metrics = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        bootstrap_sample = block_bootstrap(returns, block_size)\n",
        "        metric = metric_func(bootstrap_sample)\n",
        "        bootstrap_metrics.append(metric)\n",
        "    return bootstrap_metrics"
      ],
      "metadata": {
        "id": "RWqTt6uwkk9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python illustration of potential drawdowns"
      ],
      "metadata": {
        "id": "py0YpAfZkwFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Dict\n",
        "from math import ceil\n",
        "\n",
        "def stationary_bootstrap(returns: np.ndarray, L: int, n_paths: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Politis & Romano stationary bootstrap.\n",
        "    L: mean block length.\n",
        "    \"\"\"\n",
        "    n = len(returns)\n",
        "    p = 1.0 / max(1, L)\n",
        "    out = np.empty((n_paths, n), dtype=float)\n",
        "    for k in range(n_paths):\n",
        "        path = np.empty(n, dtype=float)\n",
        "        i = rng.integers(0, n)\n",
        "        for t in range(n):\n",
        "            if t == 0:\n",
        "                path[t] = returns[i]\n",
        "            else:\n",
        "                if rng.random() < p:\n",
        "                    i = rng.integers(0, n)  # start a new block\n",
        "                else:\n",
        "                    i = (i + 1) % n        # continue current block (wrap-around)\n",
        "                path[t] = returns[i]\n",
        "        out[k] = path\n",
        "    return out\n",
        "\n",
        "def moving_block_bootstrap(returns: np.ndarray, L: int, n_paths: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Overlapping moving-block bootstrap (MBB).\n",
        "    L: block length.\n",
        "    \"\"\"\n",
        "    n = len(returns)\n",
        "    # Create overlapping blocks\n",
        "    blocks = np.array([returns[i:i+L] for i in range(n - L + 1)], dtype=float)\n",
        "    n_blocks_needed = ceil(n / L)\n",
        "    out = np.empty((n_paths, n), dtype=float)\n",
        "    for k in range(n_paths):\n",
        "        idx = rng.integers(0, len(blocks), size=n_blocks_needed)\n",
        "        sampled = blocks[idx].ravel()[:n]\n",
        "        out[k] = sampled\n",
        "    return out\n",
        "\n",
        "def equity_from_returns(ret: np.ndarray, initial: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"Equity curve from returns.\"\"\"\n",
        "    return initial * np.cumprod(1.0 + ret)\n",
        "\n",
        "def max_drawdown_and_duration(equity: np.ndarray) -> Tuple[float, int, bool]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mdd_pct (in percent),\n",
        "      mdd_duration (peak to recovery, in periods),\n",
        "      censored (True if recovery not achieved within sample).\n",
        "    \"\"\"\n",
        "    n = len(equity)\n",
        "    run_max = np.maximum.accumulate(equity)\n",
        "    drawdown = (run_max - equity) / np.where(run_max == 0, 1, run_max)  # fraction\n",
        "    i_trough = int(np.argmax(drawdown))\n",
        "    mdd = float(drawdown[i_trough])\n",
        "    # peak index prior to trough\n",
        "    i_peak = int(np.argmax(equity[:i_trough+1]))\n",
        "    # find recovery (first t >= i_trough with equity[t] >= equity[i_peak])\n",
        "    recovery_idx = None\n",
        "    for t in range(i_trough, n):\n",
        "        if equity[t] >= equity[i_peak] - 1e-12:\n",
        "            recovery_idx = t\n",
        "            break\n",
        "    if recovery_idx is None:\n",
        "        duration = (n - 1) - i_peak\n",
        "        censored = True\n",
        "    else:\n",
        "        duration = recovery_idx - i_peak\n",
        "        censored = False\n",
        "    return mdd * 100.0, int(duration), censored\n",
        "\n",
        "def simulate_drawdown_distribution(\n",
        "    returns: np.ndarray,\n",
        "    n_paths: int = 5000,\n",
        "    L: int = None,\n",
        "    method: str = \"stationary\",\n",
        "    seed: int = 42\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Bootstrap synthetic paths and compute MDD/duration per path.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(returns)\n",
        "    if L is None:\n",
        "        # rule of thumb for time-series bootstrap\n",
        "        L = max(2, int(round((3 * n) ** (1 / 3))))\n",
        "    if method == \"stationary\":\n",
        "        paths = stationary_bootstrap(returns, L, n_paths, rng)\n",
        "    elif method == \"mbb\":\n",
        "        paths = moving_block_bootstrap(returns, L, n_paths, rng)\n",
        "    else:\n",
        "        raise ValueError(\"method must be 'stationary' or 'mbb'\")\n",
        "\n",
        "    mdds = np.empty(n_paths, dtype=float)\n",
        "    durs = np.empty(n_paths, dtype=int)\n",
        "    cens = np.zeros(n_paths, dtype=bool)\n",
        "    for i in range(n_paths):\n",
        "        eq = equity_from_returns(paths[i])\n",
        "        mdd_pct, dur, cflag = max_drawdown_and_duration(eq)\n",
        "        mdds[i] = mdd_pct\n",
        "        durs[i] = dur\n",
        "        cens[i] = cflag\n",
        "    return {\"mdd_pct\": mdds, \"duration\": durs, \"censored\": cens, \"L\": L, \"method\": method, \"n_paths\": n_paths}\n",
        "\n",
        "def tail_expected_shortfall(x: np.ndarray, alpha: float = 0.95) -> float:\n",
        "    \"\"\"Mean of the worst (1-alpha) tail.\"\"\"\n",
        "    q = np.quantile(x, alpha)\n",
        "    tail = x[x >= q]\n",
        "    return float(tail.mean()) if len(tail) else float('nan')"
      ],
      "metadata": {
        "id": "XjLfPUCikwbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}