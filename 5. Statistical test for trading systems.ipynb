{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Wilcoxon Signed-Rank test"
      ],
      "metadata": {
        "id": "BtBoPNH52lwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86S8DmujvNZI",
        "outputId": "1a7d7391-d1ee-4ea3-b30c-66944c8e24ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 248, W+ = 16000.000, method = normal-approx\n",
            "mu = 15438.000, sigma = 1130.83199, z = 0.49653706534986475\n",
            "p-value = 0.309758  (H1: median > 0)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from typing import Iterable, List, Tuple, Union\n",
        "\n",
        "def _average_ranks_and_ties(abs_vals: List[float], tol: float = 0.0) -> Tuple[List[float], List[int]]:\n",
        "    \"\"\"\n",
        "    Rank |x_i| from smallest to largest starting at 1, averaging ranks within ties.\n",
        "    A tolerance 'tol' groups values whose differences are <= tol into the same tie block.\n",
        "    Returns:\n",
        "        ranks: list of average ranks in the original order\n",
        "        tie_sizes: sizes (>1) of tie groups among |x_i|\n",
        "    \"\"\"\n",
        "    n = len(abs_vals)\n",
        "    pairs = sorted([(abs_vals[i], i) for i in range(n)], key=lambda z: z[0])\n",
        "\n",
        "    ranks = [0.0] * n\n",
        "    tie_sizes = []\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        j = i + 1\n",
        "        v = pairs[i][0]\n",
        "        while j < n and abs(pairs[j][0] - v) <= tol:\n",
        "            j += 1\n",
        "        m = j - i  # size of tie block\n",
        "        # average of 1-based ranks: i+1, i+2, ..., j\n",
        "        avg_rank = (i + 1 + j) * 0.5\n",
        "        for k in range(i, j):\n",
        "            ranks[pairs[k][1]] = avg_rank\n",
        "        if m > 1:\n",
        "            tie_sizes.append(m)\n",
        "        i = j\n",
        "    return ranks, tie_sizes\n",
        "\n",
        "def _normal_cdf(z: float) -> float:\n",
        "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
        "\n",
        "def _exact_wilcoxon_pvalue(W_obs: int, n: int, alternative: str) -> float:\n",
        "    \"\"\"\n",
        "    Exact p-value for W+ when there are NO ties (ranks are exactly 1..n).\n",
        "    Dynamic programming over subset sums of {1,2,...,n}.\n",
        "    \"\"\"\n",
        "    S = n * (n + 1) // 2\n",
        "    dp = [0] * (S + 1)\n",
        "    dp[0] = 1\n",
        "    for r in range(1, n + 1):\n",
        "        for s in range(S, r - 1, -1):\n",
        "            dp[s] += dp[s - r]\n",
        "\n",
        "    total = 2 ** n  # each sign pattern equally likely under H0\n",
        "    if alternative == \"greater\":\n",
        "        return sum(dp[W_obs:]) / total\n",
        "    elif alternative == \"less\":\n",
        "        return sum(dp[: W_obs + 1]) / total\n",
        "    else:  # two-sided (symmetric about mu)\n",
        "        p_ge = sum(dp[W_obs:]) / total\n",
        "        p_le = sum(dp[: W_obs + 1]) / total\n",
        "        return min(1.0, 2.0 * min(p_ge, p_le))\n",
        "\n",
        "def wilcoxon_signed_rank(\n",
        "    returns: Iterable[float],\n",
        "    alternative: str = \"two-sided\",\n",
        "    exact: Union[str, bool] = \"auto\",\n",
        "    zero_method: str = \"wilcox\",\n",
        "    continuity: bool = True,\n",
        "    exact_n_max: int = 50,\n",
        "    tie_tol: float = 0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    From-scratch Wilcoxon Signed-Rank test (no SciPy).\n",
        "\n",
        "    - Drops zeros ('wilcox')\n",
        "    - Exact p-value via DP when there are no ties and n <= exact_n_max\n",
        "    - Otherwise: normal approximation with tie-corrected variance\n",
        "      and optional continuity correction.\n",
        "\n",
        "    Returns a dict with keys: 'N','W_plus','mu','sigma','z','p_value','method','tie_sizes'.\n",
        "    \"\"\"\n",
        "    if alternative not in {\"two-sided\", \"greater\", \"less\"}:\n",
        "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'.\")\n",
        "    if zero_method != \"wilcox\":\n",
        "        raise NotImplementedError(\"Only zero_method='wilcox' (drop zeros) is supported.\")\n",
        "\n",
        "    # 1) Drop exact zeros\n",
        "    x = [float(v) for v in returns if float(v) != 0.0]\n",
        "    n = len(x)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"All observations are zero after filtering; test undefined.\")\n",
        "\n",
        "    # 2) Handle n == 1 exactly\n",
        "    if n == 1:\n",
        "        W_plus = 1.0 if x[0] > 0 else 0.0\n",
        "        mu = 0.5\n",
        "        sigma = math.sqrt(0.25)\n",
        "        # With one nonzero, one-sided exact p = 0.5; two-sided exact p = 1.0\n",
        "        if alternative == \"two-sided\":\n",
        "            p = 1.0\n",
        "        else:\n",
        "            p = 0.5\n",
        "        return {\n",
        "            \"N\": n, \"W_plus\": W_plus, \"mu\": mu, \"sigma\": sigma,\n",
        "            \"z\": float(\"nan\"), \"p_value\": p, \"method\": \"exact\", \"tie_sizes\": []\n",
        "        }\n",
        "\n",
        "    # 3) Rank |x| with average ranks and tie detection (with tolerance)\n",
        "    abs_x = [abs(v) for v in x]\n",
        "    ranks, tie_sizes = _average_ranks_and_ties(abs_x, tol=tie_tol)\n",
        "\n",
        "    # 4) Compute W+ (sum of ranks for positive x)\n",
        "    W_plus = sum(r for r, v in zip(ranks, x) if v > 0)\n",
        "\n",
        "    # 5) Mean and (tie-corrected) variance under H0\n",
        "    mu = n * (n + 1) / 4.0\n",
        "    var = n * (n + 1) * (2 * n + 1) / 24.0  # no-tie variance\n",
        "    if tie_sizes:\n",
        "        var -= sum(t * (t + 1) * (2 * t + 1) for t in tie_sizes) / 48.0\n",
        "    sigma = math.sqrt(var)\n",
        "\n",
        "    # 6) Exact vs normal route\n",
        "    no_ties = (len(tie_sizes) == 0)\n",
        "    use_exact = (exact is True) or (exact == \"auto\" and no_ties and n <= exact_n_max)\n",
        "    method = \"exact\" if use_exact else \"normal-approx\"\n",
        "\n",
        "    if use_exact:\n",
        "        # When no ties, ranks are exactly 1..n; W+ is integer-valued\n",
        "        W_obs_int = int(round(W_plus))\n",
        "        p = _exact_wilcoxon_pvalue(W_obs_int, n, alternative)\n",
        "        z = float(\"nan\")\n",
        "    else:\n",
        "        # Degenerate variance corner case\n",
        "        if sigma == 0.0:\n",
        "            if no_ties and n <= exact_n_max:\n",
        "                p = _exact_wilcoxon_pvalue(int(round(W_plus)), n, alternative)\n",
        "                return {\"N\": n, \"W_plus\": W_plus, \"mu\": mu, \"sigma\": sigma,\n",
        "                        \"z\": float(\"nan\"), \"p_value\": p, \"method\": \"exact\",\n",
        "                        \"tie_sizes\": tie_sizes}\n",
        "            return {\"N\": n, \"W_plus\": W_plus, \"mu\": mu, \"sigma\": sigma,\n",
        "                    \"z\": float(\"nan\"), \"p_value\": float(\"nan\"),\n",
        "                    \"method\": method, \"tie_sizes\": tie_sizes}\n",
        "\n",
        "        # Normal approximation with optional continuity correction\n",
        "        cc = 0.5 if continuity else 0.0\n",
        "        if alternative == \"greater\":\n",
        "            z = (W_plus - mu - cc) / sigma\n",
        "            p = 1.0 - _normal_cdf(z)\n",
        "        elif alternative == \"less\":\n",
        "            z = (W_plus - mu + cc) / sigma\n",
        "            p = _normal_cdf(z)\n",
        "        else:  # two-sided\n",
        "            z = (abs(W_plus - mu) - cc) / sigma\n",
        "            p = 2.0 * (1.0 - _normal_cdf(z))\n",
        "            p = min(1.0, max(0.0, p))\n",
        "\n",
        "    return {\n",
        "        \"N\": n,\n",
        "        \"W_plus\": W_plus,\n",
        "        \"mu\": mu,\n",
        "        \"sigma\": sigma,\n",
        "        \"z\": z,\n",
        "        \"p_value\": p,\n",
        "        \"method\": method,\n",
        "        \"tie_sizes\": tie_sizes,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = 250\n",
        "    # Student-t with df=3 (fat tails), ~1% daily vol, slight positive drift\n",
        "    returns = 0.0005 + 0.01 * rng.standard_t(df=3, size=n)\n",
        "\n",
        "    # Add illustrative outliers (robustness to extremes) and zeros (which are dropped)\n",
        "    returns[5]  += 0.12\n",
        "    returns[101] -= 0.15\n",
        "    returns[10] = 0.0\n",
        "    returns[80] = 0.0\n",
        "\n",
        "    res = wilcoxon_signed_rank(returns, alternative=\"greater\", exact=\"auto\",\n",
        "                               continuity=True, exact_n_max=50, tie_tol=0.0)\n",
        "    print(f\"N = {res['N']}, W+ = {res['W_plus']:.3f}, method = {res['method']}\")\n",
        "    print(f\"mu = {res['mu']:.3f}, sigma = {res['sigma']:.5f}, z = {res['z']}\")\n",
        "    print(f\"p-value = {res['p_value']:.6f}  (H1: median > 0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi test of a signle variance"
      ],
      "metadata": {
        "id": "g2cxbSlt8ebW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Iterable, Union, Dict\n",
        "\n",
        "# Chi-square CDF via regularized incomplete gamma\n",
        "def _gammainc_lower_reg(s: float, x: float, eps: float = 1e-12, itmax: int = 200) -> float:\n",
        "    \"\"\"\n",
        "    Regularized lower incomplete gamma P(s, x) = gamma(s, x) / Gamma(s).\n",
        "    Uses series expansion when x < s+1, else Lentz's continued fraction for Q(s, x).\n",
        "    \"\"\"\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    if s <= 0.0:\n",
        "        raise ValueError(\"s must be > 0\")\n",
        "\n",
        "    # Helper: exp(s*log(x) - x - lgamma(s))\n",
        "    def _prefactor(ss: float, xx: float) -> float:\n",
        "        return math.exp(ss * math.log(xx) - xx - math.lgamma(ss))\n",
        "\n",
        "    if x < s + 1.0:\n",
        "        # Series expansion for P(s, x)\n",
        "        term = 1.0 / s\n",
        "        summ = term\n",
        "        k = 1\n",
        "        while k < itmax:\n",
        "            term *= x / (s + k)\n",
        "            summ += term\n",
        "            if abs(term) < abs(summ) * eps:\n",
        "                break\n",
        "            k += 1\n",
        "        return _prefactor(s, x) * summ\n",
        "    else:\n",
        "        # Continued fraction for Q(s, x) = 1 - P(s, x)\n",
        "        # Lentz's algorithm for the CF of the complemented gamma\n",
        "        tiny = 1e-300\n",
        "        f = 1.0\n",
        "        C = 1.0 / tiny\n",
        "        D = 0.0\n",
        "        a_k = 0.0  # will be set within loop\n",
        "        b_k = x - s + 1.0\n",
        "        D = b_k if abs(b_k) > tiny else tiny\n",
        "        C = b_k if abs(b_k) > tiny else tiny\n",
        "        D = 1.0 / D\n",
        "        f = C * D\n",
        "        for k in range(1, itmax + 1):\n",
        "            a_k = -k * (k - s)\n",
        "            b_k = b_k + 2.0\n",
        "            # First part\n",
        "            D = a_k * D + b_k\n",
        "            if abs(D) < tiny:\n",
        "                D = tiny\n",
        "            C = b_k + a_k / C\n",
        "            if abs(C) < tiny:\n",
        "                C = tiny\n",
        "            D = 1.0 / D\n",
        "            delta = C * D\n",
        "            f *= delta\n",
        "            if abs(delta - 1.0) < eps:\n",
        "                break\n",
        "        Q = math.exp(s * math.log(x) - x - math.lgamma(s)) * f\n",
        "        P = 1.0 - Q\n",
        "        # Guard against tiny numerical drift\n",
        "        if P < 0.0:\n",
        "            P = 0.0\n",
        "        if P > 1.0:\n",
        "            P = 1.0\n",
        "        return P\n",
        "\n",
        "def chi2_cdf(x: float, df: int) -> float:\n",
        "    \"\"\"\n",
        "    CDF of Chi-square(df) using P(s=df/2, x/2).\n",
        "    \"\"\"\n",
        "    if df <= 0 or int(df) != df:\n",
        "        raise ValueError(\"df must be a positive integer.\")\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    return _gammainc_lower_reg(0.5 * df, 0.5 * x)\n",
        "\n",
        "# Single-variance chi-square test (one/two-sided)\n",
        "def chi2_variance_test(\n",
        "    returns: Iterable[float],\n",
        "    sigma0_sq: float,\n",
        "    alternative: str = \"two-sided\",\n",
        "    ddof: int = 1\n",
        ") -> Dict[str, Union[int, float, str]]:\n",
        "    \"\"\"\n",
        "    Test H0: sigma^2 == sigma0_sq using Chi-square with df = n - ddof.\n",
        "    Assumes i.i.d. normal returns under H0.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : iterable of float\n",
        "        Return series (NaNs are ignored).\n",
        "    sigma0_sq : float\n",
        "        Hypothesized variance (e.g., (0.15/sqrt(252))**2 for 15% annual vol).\n",
        "    alternative : {'two-sided','greater','less'}\n",
        "        'greater' tests sigma^2 > sigma0_sq; 'less' tests sigma^2 < sigma0_sq.\n",
        "    ddof : int\n",
        "        Degrees-of-freedom used by the variance estimator (default 1 for unbiased).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys: n, df, s2, chi2, p_value, alternative\n",
        "    \"\"\"\n",
        "    import math\n",
        "\n",
        "    x = [float(v) for v in returns if v is not None and not math.isnan(float(v))]\n",
        "    n = len(x)\n",
        "    if n - ddof <= 0:\n",
        "        raise ValueError(\"Not enough observations given ddof.\")\n",
        "\n",
        "    # Sample variance with given ddof\n",
        "    mean = sum(x) / n\n",
        "    s2 = sum((v - mean) ** 2 for v in x) / (n - ddof)\n",
        "\n",
        "    df = n - ddof\n",
        "    chi2 = df * s2 / sigma0_sq\n",
        "\n",
        "    # Tail probabilities via chi-square CDF\n",
        "    F = chi2_cdf(chi2, df)\n",
        "    if alternative == \"greater\":\n",
        "        p = 1.0 - F  # right tail\n",
        "    elif alternative == \"less\":\n",
        "        p = F        # left tail\n",
        "    elif alternative == \"two-sided\":\n",
        "        # Two-sided p as doubling the smaller tail probability\n",
        "        p = 2.0 * min(F, 1.0 - F)\n",
        "        p = min(1.0, max(0.0, p))\n",
        "    else:\n",
        "        raise ValueError(\"alternative must be 'two-sided','greater', or 'less'.\")\n",
        "\n",
        "    return {\"n\": n, \"df\": df, \"s2\": s2, \"chi2\": chi2, \"p_value\": p, \"alternative\": alternative}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "\n",
        "    # Synthetic heavy-tailed daily returns (Student-t df=3) with small drift\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = 250\n",
        "    r = 0.0005 + 0.01 * rng.standard_t(df=3, size=n)\n",
        "\n",
        "    # Hypothesized annualized volatility cap 15% -> daily variance\n",
        "    sigma0_sq = (0.15 / np.sqrt(252.0)) ** 2\n",
        "\n",
        "    # Run two-sided chi-square test of sigma^2 == sigma0_sq\n",
        "    out = chi2_variance_test(r, sigma0_sq, alternative=\"two-sided\", ddof=1)\n",
        "    print(f\"n={out['n']}, df={out['df']}\")\n",
        "    print(f\"sample variance s^2 = {out['s2']:.8f}\")\n",
        "    print(f\"chi^2 stat = {out['chi2']:.4f}, p-value = {out['p_value']:.6f} ({out['alternative']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzT2XWpC8eit",
        "outputId": "1fb77868-3411-44da-f984-9ee6c16ce3f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n=250, df=249\n",
            "sample variance s^2 = 0.00035838\n",
            "chi^2 stat = 999.4510, p-value = 0.000000 (two-sided)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levene's test"
      ],
      "metadata": {
        "id": "KL7Xv-V2-dl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import List, Sequence, Dict, Union\n",
        "\n",
        "# Regularized incomplete beta via continued fraction\n",
        "def _betacf(a: float, b: float, x: float, itmax: int = 200, eps: float = 1e-12) -> float:\n",
        "    am, bm = 1.0, 1.0\n",
        "    az = 1.0\n",
        "    qab = a + b\n",
        "    qap = a + 1.0\n",
        "    qam = a - 1.0\n",
        "    bz = 1.0 - qab * x / qap\n",
        "    em = 0.0\n",
        "    tem = 0.0\n",
        "    d = 0.0\n",
        "    ap = 0.0\n",
        "    bp = 0.0\n",
        "    app = 0.0\n",
        "    bpp = 0.0\n",
        "    aold = 0.0\n",
        "    for m in range(1, itmax + 1):\n",
        "        em = float(m)\n",
        "        tem = em + em\n",
        "        d = em * (b - em) * x / ((qam + tem) * (a + tem))\n",
        "        ap = az + d * am\n",
        "        bp = bz + d * bm\n",
        "        d = -(a + em) * (qab + em) * x / ((a + tem) * (qap + tem))\n",
        "        app = ap + d * az\n",
        "        bpp = bp + d * bz\n",
        "        aold = az\n",
        "        am = ap / bpp\n",
        "        bm = bp / bpp\n",
        "        az = app / bpp\n",
        "        bz = 1.0\n",
        "        if abs(az - aold) < eps * abs(az):\n",
        "            return az\n",
        "    return az\n",
        "\n",
        "def _betainc_reg(a: float, b: float, x: float) -> float:\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    if x >= 1.0:\n",
        "        return 1.0\n",
        "    bt = math.exp(math.lgamma(a + b) - math.lgamma(a) - math.lgamma(b)\n",
        "                  + a * math.log(x) + b * math.log(1.0 - x))\n",
        "    if x < (a + 1.0) / (a + b + 2.0):\n",
        "        return bt * _betacf(a, b, x) / a\n",
        "    else:\n",
        "        return 1.0 - bt * _betacf(b, a, 1.0 - x) / b\n",
        "\n",
        "def f_cdf(x: float, d1: int, d2: int) -> float:\n",
        "    \"\"\"CDF of F(d1,d2) via I_{d1 x / (d1 x + d2)}(d1/2, d2/2).\"\"\"\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    z = (d1 * x) / (d1 * x + d2)\n",
        "    return _betainc_reg(0.5 * d1, 0.5 * d2, z)\n",
        "\n",
        "# Levene's test\n",
        "def _mean(a: Sequence[float]) -> float:\n",
        "    return sum(a) / len(a)\n",
        "\n",
        "def levene_test(groups: List[Sequence[float]]) -> Dict[str, Union[int, float]]:\n",
        "    \"\"\"\n",
        "    Levene's test (center = mean). Tests equality of scale across k groups.\n",
        "    Returns: F, df1, df2, p_value, N, k\n",
        "    \"\"\"\n",
        "    # Flatten counts\n",
        "    k = len(groups)\n",
        "    ns = [len(g) for g in groups]\n",
        "    if any(n < 2 for n in ns):\n",
        "        raise ValueError(\"Each group must have at least 2 observations.\")\n",
        "    N = sum(ns)\n",
        "\n",
        "    # Absolute deviations from group means\n",
        "    z_groups = []\n",
        "    for g in groups:\n",
        "        mu = _mean(g)\n",
        "        z_groups.append([abs(x - mu) for x in g])\n",
        "\n",
        "    # One-way ANOVA on z\n",
        "    z_means = [_mean(zg) for zg in z_groups]\n",
        "    z_all = [z for zg in z_groups for z in zg]\n",
        "    z_grand = _mean(z_all)\n",
        "\n",
        "    # Between- and within-group sums of squares\n",
        "    ss_between = sum(n * (m - z_grand) ** 2 for n, m in zip(ns, z_means))\n",
        "    ss_within = sum(sum((z - m) ** 2 for z in zg) for zg, m in zip(z_groups, z_means))\n",
        "\n",
        "    df1 = k - 1\n",
        "    df2 = N - k\n",
        "    ms_between = ss_between / df1\n",
        "    ms_within = ss_within / df2\n",
        "    F = ms_between / ms_within\n",
        "\n",
        "    # Upper-tail p-value\n",
        "    p = 1.0 - f_cdf(F, df1, df2)\n",
        "    return {\"F\": F, \"df1\": df1, \"df2\": df2, \"p_value\": p, \"N\": N, \"k\": k}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    # Example: split a return series into 5 consecutive subperiods (groups)\n",
        "    n = 250\n",
        "    r = 0.0005 + 0.01 * rng.standard_t(df=3, size=n)  # heavy-tailed strategy returns\n",
        "    G = 5\n",
        "    groups = [r[i*(n//G):(i+1)*(n//G)] for i in range(G)]\n",
        "\n",
        "    out = levene_test(groups)\n",
        "    print(f\"Levene: F={out['F']:.4f}, df=({out['df1']},{out['df2']}), p={out['p_value']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8zWxg3h-hmN",
        "outputId": "a89a3d0b-429b-4c96-c46f-67dc1cd6b528"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levene: F=1.5778, df=(4,245), p=0.180795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brown-Forsythe test"
      ],
      "metadata": {
        "id": "g03BXM06-j9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import List, Sequence, Dict, Union\n",
        "\n",
        "# Regularized incomplete beta and F CDF\n",
        "def _betacf(a: float, b: float, x: float, itmax: int = 200, eps: float = 1e-12) -> float:\n",
        "    am, bm = 1.0, 1.0\n",
        "    az = 1.0\n",
        "    qab = a + b\n",
        "    qap = a + 1.0\n",
        "    qam = a - 1.0\n",
        "    bz = 1.0 - qab * x / qap\n",
        "    for m in range(1, itmax + 1):\n",
        "        em = float(m)\n",
        "        d = em * (b - em) * x / ((qam + 2*em) * (a + 2*em))\n",
        "        ap = az + d * am\n",
        "        bp = bz + d * bm\n",
        "        d = -(a + em) * (qab + em) * x / ((a + 2*em) * (qap + 2*em))\n",
        "        app = ap + d * az\n",
        "        bpp = bp + d * bz\n",
        "        aold = az\n",
        "        am = ap / bpp\n",
        "        bm = bp / bpp\n",
        "        az = app / bpp\n",
        "        bz = 1.0\n",
        "        if abs(az - aold) < eps * abs(az):\n",
        "            return az\n",
        "    return az\n",
        "\n",
        "def _betainc_reg(a: float, b: float, x: float) -> float:\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    if x >= 1.0:\n",
        "        return 1.0\n",
        "    bt = math.exp(math.lgamma(a + b) - math.lgamma(a) - math.lgamma(b)\n",
        "                  + a * math.log(x) + b * math.log(1.0 - x))\n",
        "    if x < (a + 1.0) / (a + b + 2.0):\n",
        "        return bt * _betacf(a, b, x) / a\n",
        "    else:\n",
        "        return 1.0 - bt * _betacf(b, a, 1.0 - x) / b\n",
        "\n",
        "def f_cdf(x: float, d1: int, d2: int) -> float:\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    z = (d1 * x) / (d1 * x + d2)\n",
        "    return _betainc_reg(0.5 * d1, 0.5 * d2, z)\n",
        "\n",
        "# Brown–Forsythe\n",
        "def _median(a: Sequence[float]) -> float:\n",
        "    s = sorted(a)\n",
        "    n = len(s)\n",
        "    mid = n // 2\n",
        "    if n % 2 == 1:\n",
        "        return s[mid]\n",
        "    return 0.5 * (s[mid - 1] + s[mid])\n",
        "\n",
        "def brown_forsythe_test(groups: List[Sequence[float]]) -> Dict[str, Union[int, float]]:\n",
        "    \"\"\"\n",
        "    Brown–Forsythe test (center = median). Tests equality of scale across k groups.\n",
        "    Returns: F, df1, df2, p_value, N, k\n",
        "    \"\"\"\n",
        "    k = len(groups)\n",
        "    ns = [len(g) for g in groups]\n",
        "    if any(n < 2 for n in ns):\n",
        "        raise ValueError(\"Each group must have at least 2 observations.\")\n",
        "    N = sum(ns)\n",
        "\n",
        "    # Absolute deviations from group medians\n",
        "    z_groups = []\n",
        "    for g in groups:\n",
        "        med = _median(g)\n",
        "        z_groups.append([abs(x - med) for x in g])\n",
        "\n",
        "    # One-way ANOVA on z\n",
        "    def _mean(a): return sum(a) / len(a)\n",
        "    z_means = [_mean(zg) for zg in z_groups]\n",
        "    z_all = [z for zg in z_groups for z in zg]\n",
        "    z_grand = _mean(z_all)\n",
        "\n",
        "    ss_between = sum(n * (m - z_grand) ** 2 for n, m in zip(ns, z_means))\n",
        "    ss_within = sum(sum((z - m) ** 2 for z in zg) for zg, m in zip(z_groups, z_means))\n",
        "\n",
        "    df1 = k - 1\n",
        "    df2 = N - k\n",
        "    F = (ss_between / df1) / (ss_within / df2)\n",
        "\n",
        "    p = 1.0 - f_cdf(F, df1, df2)\n",
        "    return {\"F\": F, \"df1\": df1, \"df2\": df2, \"p_value\": p, \"N\": N, \"k\": k}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import numpy as np\n",
        "    rng = np.random.default_rng(7)\n",
        "\n",
        "    # Same grouping idea as in Levene: 4 consecutive subperiods\n",
        "    n = 240\n",
        "    r = 0.0003 + 0.012 * rng.standard_t(df=3, size=n)\n",
        "\n",
        "    G = 4\n",
        "    groups = [r[i*(n//G):(i+1)*(n//G)] for i in range(G)]\n",
        "\n",
        "    out = brown_forsythe_test(groups)\n",
        "    print(f\"Brown-Forsythe: F={out['F']:.4f}, df=({out['df1']},{out['df2']}), p={out['p_value']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcNRQW9z-oWl",
        "outputId": "e91c6131-d858-4d5a-fdd1-7be51b4f34f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown-Forsythe: F=0.8120, df=(3,236), p=0.488324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kolmogorov–Smirnov (K–S) test"
      ],
      "metadata": {
        "id": "GoqOU9znBU5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Callable, Dict, Iterable, Tuple, Union\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "SQRT2 = math.sqrt(2.0)\n",
        "\n",
        "def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"Vectorized math.erf for arrays/scalars without SciPy.\"\"\"\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    # np.vectorize is fine here (sizes are small/moderate); replace with a fast approx if needed.\n",
        "    return np.vectorize(math.erf)(z)\n",
        "\n",
        "def normal_cdf(x: Union[float, np.ndarray], mu: float = 0.0, sigma: float = 1.0):\n",
        "    z = (np.asarray(x, dtype=float) - mu) / sigma\n",
        "    return 0.5 * (1.0 + _erf_array(z / SQRT2))\n",
        "\n",
        "def _ks_asymp_pvalue(D: float, n_eff: float, tol: float = 1e-12) -> float:\n",
        "    \"\"\"\n",
        "    Asymptotic K–S tail probability: P(D_n >= D) ≈ 2 * sum_{k>=1} (-1)^{k-1} exp(-2 k^2 n_eff D^2).\n",
        "    For 1-sample, n_eff = n. For 2-sample, n_eff = n*m/(n+m).\n",
        "    \"\"\"\n",
        "    if D <= 0.0:\n",
        "        return 1.0\n",
        "    x = -2.0 * (D * D) * n_eff\n",
        "    s = 0.0\n",
        "    k = 1\n",
        "    while True:\n",
        "        term = 2.0 * ((-1.0) ** (k - 1)) * math.exp(x * (k * k))\n",
        "        s += term\n",
        "        if abs(term) < tol:\n",
        "            break\n",
        "        k += 1\n",
        "        if k > 5000:  # ultra-conservative guard\n",
        "            break\n",
        "    return max(0.0, min(1.0, s))\n",
        "\n",
        "# ---------- One-sample K–S (known F) ----------\n",
        "def ks_1samp(x: Iterable[float], cdf: Callable[[np.ndarray], np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    One-sample K–S test against a fully specified continuous CDF F (no parameters fitted on x).\n",
        "    Returns: D, D_plus, D_minus, n, p_value (asymptotic)\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n == 0:\n",
        "        raise ValueError(\"Empty sample.\")\n",
        "\n",
        "    xs = np.sort(x)\n",
        "    Fi = cdf(xs)  # theoretical CDF at data points\n",
        "    i = np.arange(1, n + 1)\n",
        "    D_plus = np.max(i / n - Fi)\n",
        "    D_minus = np.max(Fi - (i - 1) / n)\n",
        "    D = max(D_plus, D_minus)\n",
        "\n",
        "    p = _ks_asymp_pvalue(D, n_eff=n)\n",
        "    return {\"D\": float(D), \"D_plus\": float(D_plus), \"D_minus\": float(D_minus),\n",
        "            \"n\": int(n), \"p_value\": float(p)}\n",
        "\n",
        "# ---------- Two-sample K–S ----------\n",
        "def ks_2samp(x: Iterable[float], y: Iterable[float]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Two-sample K–S test comparing ECDFs of x and y.\n",
        "    Returns: D, n, m, p_value (asymptotic with n_eff = n*m/(n+m))\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    y = np.asarray(list(y), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    y = y[~np.isnan(y)]\n",
        "    n, m = x.size, y.size\n",
        "    if n == 0 or m == 0:\n",
        "        raise ValueError(\"Both samples must be non-empty.\")\n",
        "\n",
        "    xs = np.sort(x)\n",
        "    ys = np.sort(y)\n",
        "    z = np.sort(np.unique(np.concatenate([xs, ys])))   # merged grid\n",
        "    Fx = np.searchsorted(xs, z, side=\"right\") / n\n",
        "    Fy = np.searchsorted(ys, z, side=\"right\") / m\n",
        "    D = float(np.max(np.abs(Fx - Fy)))\n",
        "    n_eff = (n * m) / (n + m)\n",
        "\n",
        "    p = _ks_asymp_pvalue(D, n_eff=n_eff)\n",
        "    return {\"D\": D, \"n\": int(n), \"m\": int(m), \"p_value\": float(p)}\n",
        "\n",
        "# ---------- Lilliefors (Monte Carlo) ----------\n",
        "def lilliefors_normal(\n",
        "    x: Iterable[float],\n",
        "    nsim: int = 2000,\n",
        "    rng: np.random.Generator = None,\n",
        "    ddof_sigma_mle: int = 0,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Lilliefors test for normality: K–S vs N(mu_hat, sigma_hat), with mu_hat, sigma_hat estimated from x.\n",
        "    p-value obtained by Monte Carlo under H0.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng(12345)\n",
        "\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n < 3:\n",
        "        raise ValueError(\"Sample too small for Lilliefors.\")\n",
        "\n",
        "    # Fit mu, sigma on the sample\n",
        "    mu_hat = float(np.mean(x))\n",
        "    sigma_hat = float(np.std(x, ddof=ddof_sigma_mle))\n",
        "    if sigma_hat <= 0.0:\n",
        "        return {\"D\": 1.0, \"n\": int(n), \"p_value\": 0.0, \"nsim\": int(nsim)}\n",
        "\n",
        "    # Observed D against fitted normal\n",
        "    xs = np.sort(x)\n",
        "    Fi = 0.5 * (1.0 + _erf_array((xs - mu_hat) / (sigma_hat * SQRT2)))\n",
        "    i = np.arange(1, n + 1)\n",
        "    D_plus = np.max(i / n - Fi)\n",
        "    D_minus = np.max(Fi - (i - 1) / n)\n",
        "    D_obs = float(max(D_plus, D_minus))\n",
        "\n",
        "    # Simulate null with parameter re-fitting each time\n",
        "    geq = 0\n",
        "    for _ in range(nsim):\n",
        "        z = rng.standard_normal(n)\n",
        "        mu_s = float(np.mean(z))\n",
        "        sig_s = float(np.std(z, ddof=ddof_sigma_mle))\n",
        "        if sig_s <= 0.0:\n",
        "            continue\n",
        "        zs = np.sort(z)\n",
        "        Fsim = 0.5 * (1.0 + _erf_array((zs - mu_s) / (sig_s * SQRT2)))\n",
        "        i = np.arange(1, n + 1)\n",
        "        Dp = np.max(i / n - Fsim)\n",
        "        Dm = np.max(Fsim - (i - 1) / n)\n",
        "        D_sim = max(Dp, Dm)\n",
        "        if D_sim >= D_obs:\n",
        "            geq += 1\n",
        "\n",
        "    p_mc = (geq + 1.0) / (nsim + 1.0)  # smoothed MC p-value\n",
        "    return {\"D\": D_obs, \"n\": int(n), \"p_value\": float(p_mc), \"nsim\": int(nsim)}\n",
        "\n",

        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    # Synthetic returns (fat-tailed) to show normal misspecification\n",
        "    n = 300\n",
        "    returns = 0.0003 + 0.01 * rng.standard_t(df=3, size=n)\n",
        "\n",
        "    # 1) One-sample K–S against *fully specified* Normal(0, 1% daily vol)\n",
        "    mu0, sig0 = 0.0, 0.01\n",
        "    def F0(x):  # theoretical CDF known a priori (no parameter fitting!)\n",
        "        return 0.5 * (1.0 + _erf_array((np.asarray(x) - mu0) / (sig0 * SQRT2)))\n",
        "\n",
        "    ks1 = ks_1samp(returns, cdf=F0)\n",
        "    print(f\"[KS 1-sample vs N({mu0:.4f},{sig0:.4f}):] D={ks1['D']:.4f}, p={ks1['p_value']:.6f}, n={ks1['n']}\")\n",
        "\n",
        "    # 2) Lilliefors\n",
        "    lillie = lilliefors_normal(returns, nsim=2000, rng=rng, ddof_sigma_mle=0)\n",
        "    print(f\"[Lilliefors normality:] D={lillie['D']:.4f}, p≈{lillie['p_value']:.6f} (nsim={lillie['nsim']})\")\n",
        "\n",
        "    # 3) Two-sample K–S: compare two subperiods (high-vol vs low-vol regime proxy)\n",
        "    rA = returns[: n//2]\n",
        "    rB = returns[n//2 :]\n",
        "    ks2 = ks_2samp(rA, rB)\n",
        "    print(f\"[KS 2-sample A vs B:] D={ks2['D']:.4f}, p={ks2['p_value']:.6f}, n={ks2['n']}, m={ks2['m']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDRov_K4BV8n",
        "outputId": "77a10e14-51f3-4995-8981-cc3e14d61712"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KS 1-sample vs N(0.0000,0.0100):] D=0.0713, p=0.094391, n=300\n",
            "[Lilliefors normality:] D=0.1215, p≈0.000500 (nsim=2000)\n",
            "[KS 2-sample A vs B:] D=0.0533, p=0.983287, n=150, m=150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi Goodness-of-Fit test"
      ],
      "metadata": {
        "id": "w-eiVNOuDw2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Callable, Dict, Iterable, List, Tuple, Union\n",
        "import numpy as np\n",
        "\n",
        "# Chi-square CDF via regularized incomplete gamma\n",
        "def _gammainc_lower_reg(s: float, x: float, eps: float = 1e-12, itmax: int = 200) -> float:\n",
        "    \"\"\"Regularized lower incomplete gamma P(s,x) = gamma(s,x)/Gamma(s).\"\"\"\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    if s <= 0.0:\n",
        "        raise ValueError(\"s must be > 0\")\n",
        "\n",
        "    def _pref(ss, xx):\n",
        "        return math.exp(ss * math.log(xx) - xx - math.lgamma(ss))\n",
        "\n",
        "    if x < s + 1.0:\n",
        "        # Series for P(s, x)\n",
        "        term = 1.0 / s\n",
        "        summ = term\n",
        "        k = 1\n",
        "        while k < itmax:\n",
        "            term *= x / (s + k)\n",
        "            summ += term\n",
        "            if abs(term) < abs(summ) * eps:\n",
        "                break\n",
        "            k += 1\n",
        "        return _pref(s, x) * summ\n",
        "    else:\n",
        "        # Continued fraction for Q(s, x) = 1 - P(s, x)\n",
        "        tiny = 1e-300\n",
        "        b = x - s + 1.0\n",
        "        C = b if abs(b) > tiny else tiny\n",
        "        D = 1.0 / (b if abs(b) > tiny else tiny)\n",
        "        f = C * D\n",
        "        for k in range(1, itmax + 1):\n",
        "            a = -k * (k - s)\n",
        "            b += 2.0\n",
        "            D = a * D + b\n",
        "            if abs(D) < tiny: D = tiny\n",
        "            C = b + a / C\n",
        "            if abs(C) < tiny: C = tiny\n",
        "            D = 1.0 / D\n",
        "            delta = C * D\n",
        "            f *= delta\n",
        "            if abs(delta - 1.0) < eps:\n",
        "                break\n",
        "        Q = math.exp(s * math.log(x) - x - math.lgamma(s)) * f\n",
        "        P = 1.0 - Q\n",
        "        return min(1.0, max(0.0, P))\n",
        "\n",
        "def chi2_cdf(x: float, df: int) -> float:\n",
        "    \"\"\"CDF of Chi-square(df) using P(df/2, x/2).\"\"\"\n",
        "    if df <= 0 or int(df) != df:\n",
        "        raise ValueError(\"df must be a positive integer.\")\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    return _gammainc_lower_reg(0.5 * df, 0.5 * x)\n",
        "\n",
        "# Binning rules & helpers\n",
        "def _sturges_bins(x: np.ndarray) -> np.ndarray:\n",
        "    n = x.size\n",
        "    k = int(np.ceil(np.log2(n))) + 1 if n > 1 else 1\n",
        "    lo, hi = np.min(x), np.max(x)\n",
        "    if hi == lo:\n",
        "        hi = lo + 1e-12\n",
        "    return np.linspace(lo, hi, k + 1)\n",
        "\n",
        "def _scott_bins(x: np.ndarray, max_bins: int = 500) -> np.ndarray:\n",
        "    n = x.size\n",
        "    sigma = np.std(x, ddof=1) if n > 1 else 0.0\n",
        "    if sigma <= 0.0:\n",
        "        return _sturges_bins(x)\n",
        "    h = 3.49 * sigma * n ** (-1/3)\n",
        "    lo, hi = np.min(x), np.max(x)\n",
        "    if h <= 0 or hi == lo:\n",
        "        return _sturges_bins(x)\n",
        "    k = int(np.ceil((hi - lo) / h))\n",
        "    k = max(1, min(k, max_bins))\n",
        "    return np.linspace(lo, hi, k + 1)\n",
        "\n",
        "def _fd_bins(x: np.ndarray, max_bins: int = 500) -> np.ndarray:\n",
        "    n = x.size\n",
        "    q75, q25 = np.percentile(x, [75, 25])\n",
        "    iqr = q75 - q25\n",
        "    if iqr <= 0:\n",
        "        return _scott_bins(x, max_bins=max_bins)\n",
        "    h = 2.0 * iqr * n ** (-1/3)\n",
        "    lo, hi = np.min(x), np.max(x)\n",
        "    if h <= 0 or hi == lo:\n",
        "        return _sturges_bins(x)\n",
        "    k = int(np.ceil((hi - lo) / h))\n",
        "    k = max(1, min(k, max_bins))\n",
        "    return np.linspace(lo, hi, k + 1)\n",
        "\n",
        "def make_bin_edges(x: np.ndarray, rule: str = \"fd\", max_bins: int = 500) -> np.ndarray:\n",
        "    rule = rule.lower()\n",
        "    if rule in (\"fd\", \"freedman-diaconis\", \"freedman_diaconis\"):\n",
        "        return _fd_bins(x, max_bins=max_bins)\n",
        "    elif rule in (\"scott\",):\n",
        "        return _scott_bins(x, max_bins=max_bins)\n",
        "    elif rule in (\"sturges\",):\n",
        "        return _sturges_bins(x)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown rule. Use 'fd', 'scott', or 'sturges'.\")\n",
        "\n",
        "def _merge_low_expected(O: np.ndarray, E: np.ndarray, edges: np.ndarray, min_exp: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Greedily merge adjacent bins with E_i < min_exp. Prefer merging with the neighbor\n",
        "    that yields the smallest combined expected count increase (left if i>0 else right).\n",
        "    \"\"\"\n",
        "    O = O.astype(float).copy()\n",
        "    E = E.astype(float).copy()\n",
        "    edges = edges.astype(float).copy()\n",
        "\n",
        "    i = 0\n",
        "    while i < len(E):\n",
        "        if E[i] >= min_exp:\n",
        "            i += 1\n",
        "            continue\n",
        "        # choose neighbor\n",
        "        if len(E) == 1:\n",
        "            break\n",
        "        if i == 0:\n",
        "            j = 1\n",
        "            left = False\n",
        "        elif i == len(E) - 1:\n",
        "            j = i - 1\n",
        "            left = True\n",
        "        else:\n",
        "            # choose neighbor with smaller E\n",
        "            left = E[i-1] <= E[i+1]\n",
        "            j = i - 1 if left else i + 1\n",
        "        # merge bin i into bin j (ensure j < i for edge deletion)\n",
        "        if j > i:\n",
        "            # merge into right neighbor\n",
        "            O[j] += O[i]; E[j] += E[i]\n",
        "            O = np.delete(O, i); E = np.delete(E, i)\n",
        "            edges = np.delete(edges, i + 1)  # remove boundary between i and j\n",
        "        else:\n",
        "            # merge into left neighbor\n",
        "            O[j] += O[i]; E[j] += E[i]\n",
        "            O = np.delete(O, i); E = np.delete(E, i)\n",
        "            edges = np.delete(edges, j + 1)  # remove boundary between j and i\n",
        "            i = j  # current index moved left\n",
        "    return O, E, edges\n",
        "\n",
        "# Chi-squared GoF core\n",
        "def chi2_gof_binned(\n",
        "    x: Iterable[float],\n",
        "    cdf: Callable[[np.ndarray], np.ndarray],\n",
        "    bins: str = \"fd\",\n",
        "    min_exp: float = 5.0,\n",
        "    params_estimated: int = 0,\n",
        "    max_bins: int = 500\n",
        ") -> Dict[str, Union[int, float, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Chi-squared GoF test against a fully specified CDF 'cdf'.\n",
        "      - 'bins': 'fd' (Freedman–Diaconis), 'scott', or 'sturges'\n",
        "      - 'min_exp': minimum expected count per bin (merges until satisfied)\n",
        "      - 'params_estimated': number of parameters fitted on the data (df adjustment)\n",
        "    Returns: chi2, df, p_value, k_final, O, E, edges\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Not enough observations.\")\n",
        "\n",
        "    edges = make_bin_edges(x, rule=bins, max_bins=max_bins)\n",
        "    # Observed counts\n",
        "    O, _ = np.histogram(x, bins=edges)\n",
        "\n",
        "    # Expected counts from theoretical CDF\n",
        "    F_lo = cdf(edges[:-1])\n",
        "    F_hi = cdf(edges[1:])\n",
        "    probs = np.clip(F_hi - F_lo, 0.0, 1.0)\n",
        "    E = n * probs\n",
        "\n",
        "    # Merge bins until all E_i >= min_exp\n",
        "    O2, E2, edges2 = _merge_low_expected(O, E, edges, min_exp=min_exp)\n",
        "\n",
        "    # Recompute statistic\n",
        "    if np.any(E2 <= 0):\n",
        "        raise ValueError(\"Some expected counts are non-positive after merging.\")\n",
        "    chi2 = float(np.sum((O2 - E2) ** 2 / E2))\n",
        "\n",
        "    k_final = int(len(E2))\n",
        "    df = k_final - 1 - int(params_estimated)\n",
        "    if df <= 0:\n",
        "        raise ValueError(f\"Non-positive degrees of freedom (df={df}). \"\n",
        "                         f\"Reduce parameters_estimated or adjust binning.\")\n",
        "\n",
        "    p_value = 1.0 - chi2_cdf(chi2, df)\n",
        "    return {\n",
        "        \"chi2\": chi2,\n",
        "        \"df\": df,\n",
        "        \"p_value\": float(p_value),\n",
        "        \"k_final\": k_final,\n",
        "        \"O\": O2,\n",
        "        \"E\": E2,\n",
        "        \"edges\": edges2\n",
        "    }\n",
        "\n",
        "# Convenience: Normal CDF\n",
        "SQRT2 = math.sqrt(2.0)\n",
        "def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return np.vectorize(math.erf)(z)\n",
        "\n",
        "def normal_cdf(x: Union[float, np.ndarray], mu: float = 0.0, sigma: float = 1.0):\n",
        "    z = (np.asarray(x, dtype=float) - mu) / sigma\n",
        "    return 0.5 * (1.0 + _erf_array(z / SQRT2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(7)\n",
        "\n",
        "    # Synthetic returns (fat-tailed) to illustrate misspecification\n",
        "    n = 300\n",
        "    returns = 0.0003 + 0.01 * rng.standard_t(df=3, size=n)\n",
        "\n",
        "    # Case A: Test against pre-specified Normal(0, 1% daily vol): p=0 parameters estimated\n",
        "    mu0, sig0 = 0.0, 0.01\n",
        "    F0 = lambda x: normal_cdf(x, mu=mu0, sigma=sig0)\n",
        "    res_A = chi2_gof_binned(returns, cdf=F0, bins=\"fd\", min_exp=5, params_estimated=0)\n",
        "    print(\"[Chi2 GoF vs N(0, 0.01)]\",\n",
        "          f\"chi2={res_A['chi2']:.3f}, df={res_A['df']}, p={res_A['p_value']:.6f}, k'={res_A['k_final']}\")\n",
        "\n",
        "    # Case B: Test against Normal with mu,sigma estimated from the sample: p=2\n",
        "    mu_hat = float(np.mean(returns))\n",
        "    sig_hat = float(np.std(returns, ddof=1))\n",
        "    Fhat = lambda x: normal_cdf(x, mu=mu_hat, sigma=sig_hat)\n",
        "    res_B = chi2_gof_binned(returns, cdf=Fhat, bins=\"fd\", min_exp=5, params_estimated=2)\n",
        "    print(\"[Chi2 GoF vs N(mu_hat, sigma_hat)]\",\n",
        "          f\"chi2={res_B['chi2']:.3f}, df={res_B['df']}, p={res_B['p_value']:.6f}, k'={res_B['k_final']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNichlz7DxBM",
        "outputId": "a01fbb67-6f09-4b33-f700-cdd02f02bfd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chi2 GoF vs N(0, 0.01)] chi2=27.935, df=10, p=0.019028, k'=11\n",
            "[Chi2 GoF vs N(mu_hat, sigma_hat)] chi2=34.673, df=12, p=0.006691, k'=15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jarque-Bera (J-B) test"
      ],
      "metadata": {
        "id": "XRv5x_I4UaSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable, Union\n",
        "import numpy as np\n",
        "\n",
        "# Chi-square CDF via regularized incomplete gamma\n",
        "def _gammainc_lower_reg(s: float, x: float, eps: float = 1e-12, itmax: int = 200) -> float:\n",
        "    \"\"\"Regularized lower incomplete gamma P(s, x) = gamma(s, x) / Gamma(s).\"\"\"\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    if s <= 0.0:\n",
        "        raise ValueError(\"s must be > 0\")\n",
        "\n",
        "    def _pref(ss, xx):\n",
        "        return math.exp(ss * math.log(xx) - xx - math.lgamma(ss))\n",
        "\n",
        "    if x < s + 1.0:\n",
        "        # Series for P(s, x)\n",
        "        term = 1.0 / s\n",
        "        summ = term\n",
        "        for k in range(1, itmax + 1):\n",
        "            term *= x / (s + k)\n",
        "            summ += term\n",
        "            if abs(term) < abs(summ) * eps:\n",
        "                break\n",
        "        return _pref(s, x) * summ\n",
        "    else:\n",
        "        # Continued fraction for Q(s, x) = 1 - P(s, x) (Lentz)\n",
        "        tiny = 1e-300\n",
        "        b = x - s + 1.0\n",
        "        C = b if abs(b) > tiny else tiny\n",
        "        D = 1.0 / (b if abs(b) > tiny else tiny)\n",
        "        f = C * D\n",
        "        for k in range(1, itmax + 1):\n",
        "            a = -k * (k - s)\n",
        "            b += 2.0\n",
        "            D = a * D + b\n",
        "            if abs(D) < tiny: D = tiny\n",
        "            C = b + a / C\n",
        "            if abs(C) < tiny: C = tiny\n",
        "            D = 1.0 / D\n",
        "            delta = C * D\n",
        "            f *= delta\n",
        "            if abs(delta - 1.0) < eps:\n",
        "                break\n",
        "        Q = math.exp(s * math.log(x) - x - math.lgamma(s)) * f\n",
        "        P = 1.0 - Q\n",
        "        return min(1.0, max(0.0, P))\n",
        "\n",
        "def chi2_cdf(x: float, df: int) -> float:\n",
        "    \"\"\"CDF of Chi-square(df) using P(df/2, x/2).\"\"\"\n",
        "    if df <= 0 or int(df) != df:\n",
        "        raise ValueError(\"df must be a positive integer.\")\n",
        "    if x <= 0.0:\n",
        "        return 0.0\n",
        "    return _gammainc_lower_reg(0.5 * df, 0.5 * x)\n",
        "\n",
        "# Jarque–Bera core\n",
        "def _central_moments(x: np.ndarray) -> dict:\n",
        "    \"\"\"Return mean and central moments m2,m3,m4 using division by n (MLE-style).\"\"\"\n",
        "    mu = float(np.mean(x))\n",
        "    d = x - mu\n",
        "    n = x.size\n",
        "    m2 = float(np.mean(d**2))\n",
        "    m3 = float(np.mean(d**3))\n",
        "    m4 = float(np.mean(d**4))\n",
        "    return {\"mu\": mu, \"m2\": m2, \"m3\": m3, \"m4\": m4}\n",
        "\n",
        "def jarque_bera(\n",
        "    x: Iterable[float],\n",
        "    pvalue: str = \"chi2\",   # 'chi2' (asymptotic) or 'mc' (Monte Carlo)\n",
        "    nsim: int = 5000,\n",
        "    rng: np.random.Generator = None\n",
        ") -> Dict[str, Union[int, float, str]]:\n",
        "    \"\"\"\n",
        "    Jarque–Bera normality test (from scratch, no SciPy).\n",
        "    - Moments use division by n (classical JB definition).\n",
        "    - p-value via chi-square with 2 df ('chi2'), or Monte Carlo under Normal(0,1) ('mc').\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n < 3:\n",
        "        raise ValueError(\"Need at least 3 observations for JB.\")\n",
        "\n",
        "    cm = _central_moments(x)\n",
        "    if cm[\"m2\"] <= 0.0:\n",
        "        # All values equal -> undefined skew/kurt; treat as extreme non-normal\n",
        "        return {\"n\": n, \"S\": float(\"nan\"), \"K\": float(\"nan\"),\n",
        "                \"JB\": float(\"inf\"), \"p_value\": 0.0, \"method\": pvalue}\n",
        "\n",
        "    S = cm[\"m3\"] / (cm[\"m2\"] ** 1.5)                # sample skewness (division by n)\n",
        "    K = cm[\"m4\"] / (cm[\"m2\"] ** 2) - 3.0            # sample excess kurtosis (division by n)\n",
        "    JB = n / 6.0 * (S**2 + 0.25 * K**2)\n",
        "\n",
        "    if pvalue == \"chi2\":\n",
        "        p = 1.0 - chi2_cdf(JB, df=2)\n",
        "        method = \"asymptotic-chi2(df=2)\"\n",
        "    elif pvalue == \"mc\":\n",
        "        if rng is None:\n",
        "            rng = np.random.default_rng(12345)\n",
        "        # Monte Carlo null: standard normal samples of size n\n",
        "        geq = 0\n",
        "        for _ in range(nsim):\n",
        "            z = rng.standard_normal(n)\n",
        "            cmz = _central_moments(z)\n",
        "            Sz = cmz[\"m3\"] / (cmz[\"m2\"] ** 1.5) if cmz[\"m2\"] > 0 else 0.0\n",
        "            Kz = cmz[\"m4\"] / (cmz[\"m2\"] ** 2) - 3.0 if cmz[\"m2\"] > 0 else 0.0\n",
        "            JBz = n / 6.0 * (Sz**2 + 0.25 * Kz**2)\n",
        "            if JBz >= JB:\n",
        "                geq += 1\n",
        "        p = (geq + 1.0) / (nsim + 1.0)  # smoothed MC p-value\n",
        "        method = f\"monte-carlo (nsim={nsim})\"\n",
        "    else:\n",
        "        raise ValueError(\"pvalue must be 'chi2' or 'mc'.\")\n",
        "\n",
        "    return {\"n\": n, \"S\": S, \"K\": K, \"JB\": JB, \"p_value\": float(p), \"method\": method}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    # (A) Nearly normal returns (tiny drift + ~1% vol)\n",
        "    n = 300\n",
        "    r_norm = 0.0003 + 0.01 * rng.standard_normal(n)\n",
        "    outA = jarque_bera(r_norm, pvalue=\"chi2\")\n",
        "    print(f\"[JB normal-ish] n={outA['n']}, S={outA['S']:.4f}, K={outA['K']:.4f}, \"\n",
        "          f\"JB={outA['JB']:.3f}, p={outA['p_value']:.6f} ({outA['method']})\")\n",
        "\n",
        "    # (B) Heavy-tailed returns (Student-t df=3) to show rejection\n",
        "    r_t = 0.0003 + 0.01 * rng.standard_t(df=3, size=n)\n",
        "    outB = jarque_bera(r_t, pvalue=\"chi2\")\n",
        "    print(f\"[JB t(3)]      n={outB['n']}, S={outB['S']:.4f}, K={outB['K']:.4f}, \"\n",
        "          f\"JB={outB['JB']:.3f}, p={outB['p_value']:.6f} ({outB['method']})\")\n",
        "\n",
        "    # (C) Same sample as (B) but small-sample Monte Carlo p-value (optional)\n",
        "    outB_mc = jarque_bera(r_t, pvalue=\"mc\", nsim=3000, rng=rng)\n",
        "    print(f\"[JB t(3) MC]   n={outB_mc['n']}, JB={outB_mc['JB']:.3f}, \"\n",
        "          f\"p≈{outB_mc['p_value']:.6f} ({outB_mc['method']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "271CCbP2UebH",
        "outputId": "2e9605f9-4c6d-430c-9168-cf4813d85ebf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[JB normal-ish] n=300, S=0.2918, K=0.2264, JB=4.899, p=0.211511 (asymptotic-chi2(df=2))\n",
            "[JB t(3)]      n=300, S=0.4962, K=12.0276, JB=1820.614, p=0.000000 (asymptotic-chi2(df=2))\n",
            "[JB t(3) MC]   n=300, JB=1820.614, p≈0.000333 (monte-carlo (nsim=3000))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-sample mean test with autocorrelation (HAC-adjusted)"
      ],
      "metadata": {
        "id": "VC545DrAVFqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Dict, Iterable, Tuple, Union\n",
        "import numpy as np\n",
        "\n",
        "# Utilities\n",
        "SQRT2 = math.sqrt(2.0)\n",
        "def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return np.vectorize(math.erf)(z)\n",
        "def normal_cdf(z: float) -> float:\n",
        "    return 0.5 * (1.0 + _erf_array(z / SQRT2))\n",
        "\n",
        "# HAC (Newey–West with Bartlett kernel)\n",
        "def _autocovariances(x: np.ndarray, max_lag: int) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Return (gamma_0, gamma[1:max_lag+1]) with division by n (not n-1).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    mu = np.mean(x)\n",
        "    d = x - mu\n",
        "    g0 = float(np.mean(d * d))\n",
        "    if max_lag <= 0 or n <= 1:\n",
        "        return g0, np.zeros(0)\n",
        "    gammas = np.empty(max_lag, dtype=float)\n",
        "    for k in range(1, max_lag + 1):\n",
        "        gammas[k-1] = np.mean(d[k:] * d[:-k])\n",
        "    return g0, gammas\n",
        "\n",
        "def _nw_bandwidth(n: int) -> int:\n",
        "    # Common rule-of-thumb: floor(4 * (n/100)^(2/9)), at least 1\n",
        "    L = int(np.floor(4.0 * (n / 100.0) ** (2.0 / 9.0)))\n",
        "    return max(1, L)\n",
        "\n",
        "def hac_lrv(x: Iterable[float], max_lag: int = None) -> Tuple[float, float, int]:\n",
        "    \"\"\"\n",
        "    Newey–West LRV with Bartlett weights: LRV = gamma0 + 2 * sum_{k=1}^L w_k * gamma_k,\n",
        "    w_k = 1 - k/(L+1). Returns (LRV, sigma2_hat, L).\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Need at least 2 observations.\")\n",
        "    if max_lag is None:\n",
        "        max_lag = _nw_bandwidth(n)\n",
        "    g0, g = _autocovariances(x, max_lag)\n",
        "    L = max_lag\n",
        "    if L == 0:\n",
        "        lrv = g0\n",
        "    else:\n",
        "        k = np.arange(1, L + 1, dtype=float)\n",
        "        w = 1.0 - k / (L + 1.0)  # Bartlett\n",
        "        lrv = g0 + 2.0 * float(np.sum(w * g))\n",
        "    # Guard against tiny negative due to numerical noise\n",
        "    lrv = max(lrv, 1e-12)\n",
        "    return lrv, g0, L\n",
        "\n",
        "def mean_se_hac(x: Iterable[float], max_lag: int = None) -> Tuple[float, float, float, int]:\n",
        "    \"\"\"\n",
        "    Returns (mean, SE_mean, sigma2_hat, L) where SE_mean = sqrt(LRV / n).\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    mu = float(np.mean(x))\n",
        "    lrv, sig2, L = hac_lrv(x, max_lag=max_lag)\n",
        "    se = math.sqrt(lrv / n)\n",
        "    return mu, se, sig2, L\n",
        "\n",
        "def effective_sample_size(sig2: float, lrv: float, n: int) -> float:\n",
        "    \"\"\"\n",
        "    n_eff = n * sig2 / lrv  (so that sig2 / n_eff = lrv / n).\n",
        "    \"\"\"\n",
        "    return n * sig2 / max(lrv, 1e-12)\n",
        "\n",
        "# Two-sample mean test with HAC\n",
        "def two_sample_mean_test_hac(\n",
        "    A: Iterable[float],\n",
        "    B: Iterable[float],\n",
        "    delta0: float = 0.0,           # hypothesized difference (mu_A - mu_B)\n",
        "    alternative: str = \"two-sided\",\n",
        "    lag_A: int = None,\n",
        "    lag_B: int = None\n",
        ") -> Dict[str, Union[float, int, str]]:\n",
        "    \"\"\"\n",
        "    Test H0: mu_A - mu_B = delta0 using HAC (Newey–West) standard errors for means.\n",
        "    Assumes independence across A and B. Returns z-stat (asymptotic) and p-value.\n",
        "    \"\"\"\n",
        "    A = np.asarray(list(A), dtype=float); A = A[~np.isnan(A)]\n",
        "    B = np.asarray(list(B), dtype=float); B = B[~np.isnan(B)]\n",
        "    nA, nB = A.size, B.size\n",
        "    if nA < 2 or nB < 2:\n",
        "        raise ValueError(\"Both samples need at least 2 observations.\")\n",
        "\n",
        "    muA, seA, sig2A, LA = mean_se_hac(A, max_lag=lag_A)\n",
        "    muB, seB, sig2B, LB = mean_se_hac(B, max_lag=lag_B)\n",
        "    lrvA = (seA**2) * nA\n",
        "    lrvB = (seB**2) * nB\n",
        "\n",
        "    # Denominator via LRV (equivalently via n_eff)\n",
        "    se_diff = math.sqrt(lrvA / nA + lrvB / nB)\n",
        "\n",
        "    z = ((muA - muB) - delta0) / se_diff\n",
        "\n",
        "    if alternative == \"greater\":         # H1: mu_A - mu_B > delta0\n",
        "        p = 1.0 - float(normal_cdf(z))\n",
        "    elif alternative == \"less\":          # H1: mu_A - mu_B < delta0\n",
        "        p = float(normal_cdf(z))\n",
        "    elif alternative == \"two-sided\":\n",
        "        p = 2.0 * (1.0 - float(normal_cdf(abs(z))))\n",
        "        p = min(1.0, max(0.0, p))\n",
        "    else:\n",
        "        raise ValueError(\"alternative must be 'two-sided','greater','less'.\")\n",
        "\n",
        "    n_eff_A = effective_sample_size(sig2A, lrvA, nA)\n",
        "    n_eff_B = effective_sample_size(sig2B, lrvB, nB)\n",
        "\n",
        "    return {\n",
        "        \"muA\": muA, \"muB\": muB,\n",
        "        \"diff_hat\": muA - muB,\n",
        "        \"z_stat\": z, \"p_value\": p,\n",
        "        \"se_diff\": se_diff,\n",
        "        \"nA\": nA, \"nB\": nB,\n",
        "        \"L_A\": LA, \"L_B\": LB,\n",
        "        \"sig2A\": sig2A, \"sig2B\": sig2B,\n",
        "        \"lrvA\": lrvA, \"lrvB\": lrvB,\n",
        "        \"n_eff_A\": n_eff_A, \"n_eff_B\": n_eff_B,\n",
        "        \"alternative\": alternative\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = 600\n",
        "\n",
        "    # Construct two heavy-tailed, mildly autocorrelated return series\n",
        "    # Strategy A: lower mean, phi=0.20\n",
        "    eA = 0.01 * rng.standard_t(df=3, size=n)\n",
        "    muA_true, phiA = 0.0004, 0.20\n",
        "    A = np.empty(n); A[0] = muA_true + eA[0]\n",
        "    for t in range(1, n):\n",
        "        A[t] = muA_true + phiA * (A[t-1] - muA_true) + eA[t]\n",
        "\n",
        "    # Strategy B: slightly higher mean, phi=0.10\n",
        "    eB = 0.01 * rng.standard_t(df=3, size=n)\n",
        "    muB_true, phiB = 0.0006, 0.10\n",
        "    B = np.empty(n); B[0] = muB_true + eB[0]\n",
        "    for t in range(1, n):\n",
        "        B[t] = muB_true + phiB * (B[t-1] - muB_true) + eB[t]\n",
        "\n",
        "    # Test H0: muA - muB = 0 (two-sided)\n",
        "    out = two_sample_mean_test_hac(A, B, delta0=0.0, alternative=\"two-sided\")\n",
        "    print(f\"muA={out['muA']:.6f}, muB={out['muB']:.6f}, diff={out['diff_hat']:.6f}\")\n",
        "    print(f\"z={out['z_stat']:.3f}, p={out['p_value']:.6f}, SE(diff)={out['se_diff']:.6f}\")\n",
        "    print(f\"n_eff_A≈{out['n_eff_A']:.1f}, n_eff_B≈{out['n_eff_B']:.1f} (L_A={out['L_A']}, L_B={out['L_B']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahufPvgJVFBK",
        "outputId": "c59ea93e-fcce-46e8-8c11-11badb16fc08"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "muA=0.000458, muB=0.000226, diff=0.000232\n",
            "z=0.223, p=0.823782, SE(diff)=0.001042\n",
            "n_eff_A≈510.6, n_eff_B≈519.6 (L_A=5, L_B=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paired t-test with autocorrelation"
      ],
      "metadata": {
        "id": "92ycoCiNbSRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable, Tuple, Union\n",
        "import numpy as np\n",
        "\n",
        "# Utilities\n",
        "SQRT2 = math.sqrt(2.0)\n",
        "def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return np.vectorize(math.erf)(z)\n",
        "\n",
        "def normal_cdf(z: float) -> float:\n",
        "    return 0.5 * (1.0 + _erf_array(z / SQRT2))\n",
        "\n",
        "# HAC\n",
        "def _autocovariances(x: np.ndarray, max_lag: int) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    gamma_0 and gamma_k (k=1..L), each divided by n (not n-1).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    mu = np.mean(x)\n",
        "    d = x - mu\n",
        "    g0 = float(np.mean(d * d))\n",
        "    if max_lag <= 0 or n <= 1:\n",
        "        return g0, np.zeros(0)\n",
        "    gammas = np.empty(max_lag, dtype=float)\n",
        "    for k in range(1, max_lag + 1):\n",
        "        gammas[k-1] = np.mean(d[k:] * d[:-k])\n",
        "    return g0, gammas\n",
        "\n",
        "def _nw_bandwidth(n: int) -> int:\n",
        "    # Rule-of-thumb bandwidth: floor(4 * (n/100)^(2/9)), at least 1\n",
        "    L = int(np.floor(4.0 * (n / 100.0) ** (2.0 / 9.0)))\n",
        "    return max(1, L)\n",
        "\n",
        "def hac_lrv(x: Iterable[float], max_lag: int = None) -> Tuple[float, float, int]:\n",
        "    \"\"\"\n",
        "    LRV = gamma0 + 2 * sum_{k=1}^L w_k * gamma_k, with Bartlett weights w_k = 1 - k/(L+1).\n",
        "    Returns (LRV, sigma2_hat, L), where sigma2_hat = gamma0.\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    if n < 2:\n",
        "        raise ValueError(\"Need at least 2 observations.\")\n",
        "    if max_lag is None:\n",
        "        max_lag = _nw_bandwidth(n)\n",
        "    g0, g = _autocovariances(x, max_lag)\n",
        "    if max_lag == 0:\n",
        "        lrv = g0\n",
        "    else:\n",
        "        k = np.arange(1, max_lag + 1, dtype=float)\n",
        "        w = 1.0 - k / (max_lag + 1.0)\n",
        "        lrv = g0 + 2.0 * float(np.sum(w * g))\n",
        "    return max(lrv, 1e-12), g0, max_lag  # guard tiny negatives\n",
        "\n",
        "def mean_se_hac(x: Iterable[float], max_lag: int = None) -> Tuple[float, float, float, int]:\n",
        "    \"\"\"\n",
        "    Mean and HAC standard error of the mean: SE = sqrt(LRV / n).\n",
        "    Returns (mean, SE, sigma2_hat, L).\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[~np.isnan(x)]\n",
        "    n = x.size\n",
        "    mu = float(np.mean(x))\n",
        "    lrv, sig2, L = hac_lrv(x, max_lag=max_lag)\n",
        "    se = math.sqrt(lrv / n)\n",
        "    return mu, se, sig2, L\n",
        "\n",
        "def effective_sample_size(sig2: float, lrv: float, n: int) -> float:\n",
        "    \"\"\"\n",
        "    n_eff = n * sig2 / lrv (so that sig2 / n_eff = lrv / n).\n",
        "    \"\"\"\n",
        "    return n * sig2 / max(lrv, 1e-12)\n",
        "\n",
        "# Paired HAC mean test\n",
        "def paired_mean_test_hac(\n",
        "    A: Iterable[float],\n",
        "    B: Iterable[float],\n",
        "    delta0: float = 0.0,          # hypothesized mean difference mu_B - mu_A\n",
        "    alternative: str = \"two-sided\",\n",
        "    lag: int = None\n",
        ") -> Dict[str, Union[float, int, str]]:\n",
        "    \"\"\"\n",
        "    Correct paired test using d_t = B_t - A_t (aligned in time) with HAC SE.\n",
        "    Returns z-stat and p-value (asymptotic normal).\n",
        "    \"\"\"\n",
        "    A = np.asarray(list(A), dtype=float)\n",
        "    B = np.asarray(list(B), dtype=float)\n",
        "\n",
        "    # Drop any pairs with NaN in either series (alignment in practice uses timestamps)\n",
        "    mask = np.isfinite(A) & np.isfinite(B)\n",
        "    A, B = A[mask], B[mask]\n",
        "    if A.size != B.size or A.size < 2:\n",
        "        raise ValueError(\"A and B must be aligned and have at least 2 paired observations.\")\n",
        "\n",
        "    d = B - A\n",
        "    n = d.size\n",
        "    mu_d, se_d, sig2_d, L = mean_se_hac(d, max_lag=lag)\n",
        "    z = (mu_d - delta0) / se_d\n",
        "\n",
        "    if alternative == \"greater\":      # H1: mu_B - mu_A > delta0\n",
        "        p = 1.0 - float(normal_cdf(z))\n",
        "    elif alternative == \"less\":       # H1: mu_B - mu_A < delta0\n",
        "        p = float(normal_cdf(z))\n",
        "    elif alternative == \"two-sided\":\n",
        "        p = 2.0 * (1.0 - float(normal_cdf(abs(z))))\n",
        "        p = min(1.0, max(0.0, p))\n",
        "    else:\n",
        "        raise ValueError(\"alternative must be 'two-sided','greater','less'.\")\n",
        "\n",
        "    n_eff = effective_sample_size(sig2_d, (se_d**2) * n, n)\n",
        "    return {\"mu_d\": mu_d, \"z_stat\": z, \"p_value\": p, \"se_mean\": se_d,\n",
        "            \"n\": n, \"L\": L, \"sig2_d\": sig2_d, \"n_eff\": n_eff,\n",
        "            \"alternative\": alternative}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(2025)\n",
        "    n = 500\n",
        "\n",
        "    # Two strategies on the SAME dates, sharing a common market shock (positive covariance)\n",
        "    muA, muB = 0.0004, 0.0006\n",
        "    market = 0.008 * rng.standard_t(df=4, size=n)   # common factor, fat-tailed\n",
        "    eA = 0.006 * rng.standard_normal(n)             # idiosyncratic noises\n",
        "    eB = 0.006 * rng.standard_normal(n)\n",
        "\n",
        "    A = muA + market + eA\n",
        "    B = muB + market + eB\n",
        "\n",
        "    # Correct paired HAC test (H1: B better than A)\n",
        "    out = paired_mean_test_hac(A, B, delta0=0.0, alternative=\"greater\")\n",
        "    print(f\"mu_d={out['mu_d']:.6f}, z={out['z_stat']:.3f}, p={out['p_value']:.6f}, \"\n",
        "          f\"SE_mean={out['se_mean']:.6f}, n_eff≈{out['n_eff']:.1f} (L={out['L']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqtBj1VgbT0g",
        "outputId": "74cb684c-9f18-429c-cdcb-633065b32686"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu_d=0.000033, z=0.076, p=0.469536, SE_mean=0.000427, n_eff≈438.3 (L=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing positive serial correlation (trend-following)"
      ],
      "metadata": {
        "id": "BunBhJdxcJsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import math\n",
        "    from typing import Callable, Dict, Iterable, Tuple, Union\n",
        "    import numpy as np\n",
        "\n",
        "    # Utilities\n",
        "    SQRT2 = math.sqrt(2.0)\n",
        "    def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "        z = np.asarray(z, dtype=float)\n",
        "        return np.vectorize(math.erf)(z)\n",
        "    def normal_cdf(z: float) -> float:\n",
        "        return float(0.5 * (1.0 + _erf_array(z / SQRT2)))\n",
        "\n",
        "    # Chi-square CDF via regularized incomplete gamma (for Ljung–Box p)\n",
        "    def _gammainc_lower_reg(s: float, x: float, eps: float = 1e-12, itmax: int = 200) -> float:\n",
        "        \"\"\"Regularized lower incomplete gamma P(s,x).\"\"\"\n",
        "        if x <= 0.0:\n",
        "            return 0.0\n",
        "        if s <= 0.0:\n",
        "            raise ValueError(\"s must be > 0\")\n",
        "        def _pref(ss, xx): return math.exp(ss*math.log(xx) - xx - math.lgamma(ss))\n",
        "        if x < s + 1.0:\n",
        "            term = 1.0 / s; summ = term\n",
        "            for k in range(1, itmax + 1):\n",
        "                term *= x / (s + k); summ += term\n",
        "                if abs(term) < abs(summ) * eps: break\n",
        "            return _pref(s, x) * summ\n",
        "        else:\n",
        "            tiny = 1e-300\n",
        "            b = x - s + 1.0\n",
        "            C = b if abs(b) > tiny else tiny\n",
        "            D = 1.0 / (b if abs(b) > tiny else tiny)\n",
        "            f = C * D\n",
        "            for k in range(1, itmax + 1):\n",
        "                a = -k * (k - s); b += 2.0\n",
        "                D = a * D + b;  D = tiny if abs(D) < tiny else D\n",
        "                C = b + a / C;  C = tiny if abs(C) < tiny else C\n",
        "                D = 1.0 / D\n",
        "                delta = C * D; f *= delta\n",
        "                if abs(delta - 1.0) < eps: break\n",
        "            Q = math.exp(s*math.log(x) - x - math.lgamma(s)) * f\n",
        "            P = 1.0 - Q\n",
        "            return min(1.0, max(0.0, P))\n",
        "\n",
        "    def chi2_cdf(x: float, df: int) -> float:\n",
        "        if df <= 0 or int(df) != df:\n",
        "            raise ValueError(\"df must be a positive integer.\")\n",
        "        if x <= 0.0:\n",
        "            return 0.0\n",
        "        return _gammainc_lower_reg(0.5 * df, 0.5 * x)\n",
        "\n",
        "    # Sample ACF and Ljung–Box Q\n",
        "    def acf(x: Iterable[float], max_lag: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample autocorrelation up to max_lag (rho_0..rho_max).\n",
        "        Uses gamma_k = mean[(x_t-mu)(x_{t-k}-mu)] with division by n.\n",
        "        \"\"\"\n",
        "        x = np.asarray(list(x), dtype=float)\n",
        "        x = x[np.isfinite(x)]\n",
        "        n = x.size\n",
        "        if n < max_lag + 1:\n",
        "            raise ValueError(\"Series too short for requested max_lag.\")\n",
        "        mu = np.mean(x)\n",
        "        d = x - mu\n",
        "        g0 = float(np.mean(d * d))\n",
        "        rhos = np.empty(max_lag + 1, dtype=float)\n",
        "        rhos[0] = 1.0\n",
        "        for k in range(1, max_lag + 1):\n",
        "            rhos[k] = float(np.mean(d[k:] * d[:-k])) / g0 if g0 > 0 else 0.0\n",
        "        return rhos\n",
        "\n",
        "    def ljung_box(x: Iterable[float], m: int = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Ljung–Box Q = n(n+2) sum_{k=1}^m rho_k^2/(n-k) ~ Chi^2_m under H0 (white noise).\n",
        "        \"\"\"\n",
        "        x = np.asarray(list(x), dtype=float)\n",
        "        x = x[np.isfinite(x)]\n",
        "        n = x.size\n",
        "        if m is None:\n",
        "            m = max(1, int(min(20, np.floor(np.sqrt(n)))))\n",
        "        rhos = acf(x, m)\n",
        "        Q = n * (n + 2.0) * float(np.sum((rhos[1:] ** 2) / (n - np.arange(1, m + 1))))\n",
        "        p = 1.0 - chi2_cdf(Q, df=m)\n",
        "        return {\"Q\": Q, \"df\": m, \"p_value\": float(p)}\n",
        "\n",
        "    # AR(1) slope > 0 with HAC SE\n",
        "    def _nw_bandwidth(n: int) -> int:\n",
        "        # Common ROT bandwidth\n",
        "        return max(1, int(np.floor(4.0 * (n / 100.0) ** (2.0 / 9.0))))\n",
        "\n",
        "    def ar1_pos_trend_test_hac(x: Iterable[float], L: int = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Test H0: phi <= 0 in r_t = alpha + phi r_{t-1} + u_t  vs H1: phi > 0.\n",
        "        HAC covariance for (alpha, phi) via Newey–West (Bartlett). Returns z and p(one-sided).\n",
        "        \"\"\"\n",
        "        x = np.asarray(list(x), dtype=float)\n",
        "        x = x[np.isfinite(x)]\n",
        "        y = x[1:]\n",
        "        X = np.column_stack([np.ones_like(x[:-1]), x[:-1]])  # T x 2\n",
        "        T = y.size\n",
        "        if T < 5:\n",
        "            raise ValueError(\"Series too short for AR(1) regression.\")\n",
        "\n",
        "        # OLS\n",
        "        XtX = X.T @ X\n",
        "        XtY = X.T @ y\n",
        "        beta = np.linalg.solve(XtX, XtY)       # [alpha, phi]\n",
        "        resid = y - X @ beta\n",
        "        phi_hat = float(beta[1])\n",
        "\n",
        "        # HAC sandwich: Var(beta) ≈ (1/T) Q^{-1} S Q^{-1}, Q = X'X/T\n",
        "        if L is None:\n",
        "            L = _nw_bandwidth(T)\n",
        "        Z = X * resid[:, None]                  # T x 2, z_t = x_t u_t\n",
        "        Q = XtX / T\n",
        "        Q_inv = np.linalg.inv(Q)\n",
        "\n",
        "        # S = Γ_0 + sum_{h=1}^L w_h (Γ_h + Γ_h')\n",
        "        Gamma0 = (Z.T @ Z) / T\n",
        "        S = Gamma0.copy()\n",
        "        for h in range(1, L + 1):\n",
        "            w = 1.0 - h / (L + 1.0)            # Bartlett weight\n",
        "            Zh = Z[h:, :]\n",
        "            Zlag = Z[:-h, :]\n",
        "            Gamma_h = (Zh.T @ Zlag) / T\n",
        "            S += w * (Gamma_h + Gamma_h.T)\n",
        "\n",
        "        Var_beta = (Q_inv @ S @ Q_inv) / T\n",
        "        se_phi = float(np.sqrt(max(Var_beta[1, 1], 1e-18)))\n",
        "\n",
        "        z = (phi_hat - 0.0) / se_phi\n",
        "        p_one_sided = 1.0 - normal_cdf(z)      # H1: phi > 0\n",
        "\n",
        "        return {\"phi_hat\": phi_hat, \"se_phi\": se_phi, \"z\": z, \"p_value\": p_one_sided, \"L\": L, \"T\": T}\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        rng = np.random.default_rng(42)\n",
        "        n = 600\n",
        "\n",
        "        # Synthetic returns with trend-following (positive AR(1)) and heavy-tailed shocks\n",
        "        phi = 0.15\n",
        "        e = 0.01 * rng.standard_t(df=3, size=n)\n",
        "        r = np.empty(n, dtype=float)\n",
        "        r[0] = e[0]\n",
        "        for t in range(1, n):\n",
        "            r[t] = phi * r[t-1] + e[t]\n",
        "\n",
        "        # (i) One-sided AR(1) slope > 0, HAC SE\n",
        "        ar1 = ar1_pos_trend_test_hac(r)\n",
        "        print(f\"[AR(1) trend test] phi_hat={ar1['phi_hat']:.4f}, se={ar1['se_phi']:.4f}, \"\n",
        "              f\"z={ar1['z']:.3f}, p(one-sided)={ar1['p_value']:.6f}, L={ar1['L']}\")\n",
        "\n",
        "        # (ii) Ljung–Box up to m lags\n",
        "        lb = ljung_box(r, m=10)\n",
        "        print(f\"[Ljung–Box] Q={lb['Q']:.2f}, df={lb['df']}, p={lb['p_value']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W86BeerpcJyk",
        "outputId": "531f1bdc-a2d8-4b85-b867-e76881a428fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AR(1) trend test] phi_hat=0.1210, se=0.0387, z=3.125, p(one-sided)=0.000888, L=5\n",
            "[Ljung–Box] Q=19.67, df=10, p=0.205435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing negative serial correlation (mean reversion)"
      ],
      "metadata": {
        "id": "g9C_STnyjWkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import math\n",
        "    from typing import Dict, Iterable, Tuple, Union\n",
        "    import numpy as np\n",
        "\n",
        "    # Utilities\n",
        "    SQRT2 = math.sqrt(2.0)\n",
        "    def _erf_array(z: Union[float, np.ndarray]) -> np.ndarray:\n",
        "        z = np.asarray(z, dtype=float)\n",
        "        return np.vectorize(math.erf)(z)\n",
        "    def normal_cdf(z: float) -> float:\n",
        "        return float(0.5 * (1.0 + _erf_array(z / SQRT2)))\n",
        "\n",
        "    # HAC\n",
        "    def _nw_bandwidth(n: int) -> int:\n",
        "        # Rule-of-thumb bandwidth: floor(4 * (n/100)^(2/9)), at least 1\n",
        "        return max(1, int(np.floor(4.0 * (n / 100.0) ** (2.0 / 9.0))))\n",
        "\n",
        "    def ar1_neg_meanrev_test_hac(x: Iterable[float], L: int = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Test H0: phi >= 0 vs H1: phi < 0 in r_t = alpha + phi r_{t-1} + u_t.\n",
        "        HAC covariance for (alpha, phi) via Newey–West (Bartlett). Returns z and p(one-sided).\n",
        "        \"\"\"\n",
        "        x = np.asarray(list(x), dtype=float)\n",
        "        x = x[np.isfinite(x)]\n",
        "        y = x[1:]\n",
        "        X = np.column_stack([np.ones_like(x[:-1]), x[:-1]])  # T x 2\n",
        "        T = y.size\n",
        "        if T < 5:\n",
        "            raise ValueError(\"Series too short for AR(1) regression.\")\n",
        "\n",
        "        # OLS estimates\n",
        "        XtX = X.T @ X\n",
        "        XtY = X.T @ y\n",
        "        beta = np.linalg.solve(XtX, XtY)       # [alpha, phi]\n",
        "        resid = y - X @ beta\n",
        "        phi_hat = float(beta[1])\n",
        "\n",
        "        # HAC sandwich: Var(beta) ≈ (1/T) Q^{-1} S Q^{-1}\n",
        "        if L is None:\n",
        "            L = _nw_bandwidth(T)\n",
        "        Z = X * resid[:, None]                  # T x 2, z_t = x_t u_t\n",
        "        Q = XtX / T\n",
        "        Q_inv = np.linalg.inv(Q)\n",
        "\n",
        "        # S = Γ_0 + sum_{h=1}^L w_h (Γ_h + Γ_h')\n",
        "        Gamma0 = (Z.T @ Z) / T\n",
        "        S = Gamma0.copy()\n",
        "        for h in range(1, L + 1):\n",
        "            w = 1.0 - h / (L + 1.0)            # Bartlett weight\n",
        "            Zh = Z[h:, :]\n",
        "            Zlag = Z[:-h, :]\n",
        "            Gamma_h = (Zh.T @ Zlag) / T\n",
        "            S += w * (Gamma_h + Gamma_h.T)\n",
        "\n",
        "        Var_beta = (Q_inv @ S @ Q_inv) / T\n",
        "        se_phi = float(np.sqrt(max(Var_beta[1, 1], 1e-18)))\n",
        "\n",
        "        z = (phi_hat - 0.0) / se_phi\n",
        "        # One-sided p for mean reversion: H1: phi < 0  =>  P(Z <= z)\n",
        "        p_one_sided = float(normal_cdf(z))\n",
        "\n",
        "        return {\"phi_hat\": phi_hat, \"se_phi\": se_phi, \"z\": z, \"p_value\": p_one_sided, \"L\": L, \"T\": T}\n",
        "\n",
        "    # Heteroskedasticity-robust variance ratio\n",
        "    def variance_ratio_test(r: Iterable[float], q: int = 5) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Lo–MacKinlay variance–ratio test with heteroskedasticity-robust variance.\n",
        "        H0: random walk (VR=1). Mean reversion => VR < 1 (one-sided 'less').\n",
        "        Returns VR, Z (two-sided), and p_one_sided_less for VR<1.\n",
        "        \"\"\"\n",
        "        r = np.asarray(list(r), dtype=float)\n",
        "        r = r[np.isfinite(r)]\n",
        "        n = r.size\n",
        "        if q < 2 or n <= q:\n",
        "            raise ValueError(\"Need q>=2 and n>q.\")\n",
        "\n",
        "        mu = float(np.mean(r))\n",
        "        d = r - mu\n",
        "        # Variance of 1-step returns (division by n, consistent with asymptotics)\n",
        "        s2_1 = float(np.mean(d**2))\n",
        "        if s2_1 <= 0.0:\n",
        "            return {\"VR\": 1.0, \"Z\": 0.0, \"p_two_sided\": 1.0, \"p_one_sided_less\": 0.5, \"q\": q}\n",
        "\n",
        "        # Overlapping q-period returns via cumulative sums\n",
        "        c = np.cumsum(np.insert(r, 0, 0.0))\n",
        "        y = c[q:] - c[:-q]                      # length m = n - q + 1\n",
        "        m = y.size\n",
        "        # Center y around q*mu to remove mean\n",
        "        y_center = y - q * mu\n",
        "        s2_q = float(np.mean(y_center**2))\n",
        "\n",
        "        VR = s2_q / (q * s2_1)\n",
        "\n",
        "        # Heteroskedasticity-robust variance:\n",
        "        # theta(q) = sum_{j=1}^{q-1} [ (2(q-j)/q)^2 * delta_j ]\n",
        "        # delta_j = sum_{t=j}^{n-1} (d_t^2 * d_{t-j}^2) / (sum_{t=0}^{n-1} d_t^2)^2\n",
        "        denom = float(np.sum(d**2)) ** 2\n",
        "        theta = 0.0\n",
        "        for j in range(1, q):\n",
        "            num = float(np.sum(d[j:]**2 * d[:-j]**2))\n",
        "            delta_j = num / denom\n",
        "            wj = 2.0 * (q - j) / q\n",
        "            theta += (wj ** 2) * delta_j\n",
        "\n",
        "        # Asymptotic: sqrt(n) (VR - 1) -> N(0, theta)\n",
        "        Z = (np.sqrt(n) * (VR - 1.0)) / math.sqrt(max(theta, 1e-18))\n",
        "        p_two_sided = 2.0 * (1.0 - normal_cdf(abs(Z)))\n",
        "        p_two_sided = min(1.0, max(0.0, p_two_sided))\n",
        "        # One-sided (mean reversion): VR < 1  =>  Z < 0\n",
        "        p_one_sided_less = float(normal_cdf(Z))\n",
        "\n",
        "        return {\"VR\": VR, \"Z\": float(Z), \"p_two_sided\": float(p_two_sided),\n",
        "                \"p_one_sided_less\": p_one_sided_less, \"q\": q}\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        rng = np.random.default_rng(7)\n",
        "        n = 800\n",
        "\n",
        "        # Synthetic mean-reverting returns: AR(1) with phi < 0 and fat-tailed shocks\n",
        "        phi = -0.20\n",
        "        e = 0.01 * rng.standard_t(df=3, size=n)\n",
        "        r = np.empty(n, dtype=float)\n",
        "        r[0] = e[0]\n",
        "        for t in range(1, n):\n",
        "            r[t] = phi * r[t-1] + e[t]\n",
        "\n",
        "        # (i) AR(1) one-sided test for phi < 0 (mean reversion)\n",
        "        ar1 = ar1_neg_meanrev_test_hac(r)\n",
        "        print(f\"[AR(1) mean-reversion] phi_hat={ar1['phi_hat']:.4f}, se={ar1['se_phi']:.4f}, \"\n",
        "              f\"z={ar1['z']:.3f}, p(one-sided)={ar1['p_value']:.6f}, L={ar1['L']}\")\n",
        "\n",
        "        # (ii) Variance–Ratio test at horizon q (VR<1 indicates mean reversion)\n",
        "        vr = variance_ratio_test(r, q=10)\n",
        "        print(f\"[Variance–Ratio q=10] VR={vr['VR']:.4f}, Z={vr['Z']:.3f}, \"\n",
        "              f\"p_two_sided={vr['p_two_sided']:.6f}, p_one_sided(VR<1)={vr['p_one_sided_less']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeJ8caXfjWrx",
        "outputId": "9e216eb3-4bf5-42df-d75a-c029a18cd791"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AR(1) mean-reversion] phi_hat=-0.1455, se=0.0338, z=-4.300, p(one-sided)=0.000009, L=6\n",
            "[Variance–Ratio q=10] VR=0.6862, Z=-74.533, p_two_sided=0.000000, p_one_sided(VR<1)=0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ljung–Box portmanteau test"
      ],
      "metadata": {
        "id": "y8K1W0tQh0ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Dict, Iterable, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Minimal normal CDF\n",
        "def _normal_cdf(z: float) -> float:\n",
        "    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
        "\n",
        "def _default_m(n: int) -> int:\n",
        "    # Rule of thumb mentioned in text: m ≈ ln(n)\n",
        "    return max(1, int(round(math.log(n))))\n",
        "\n",
        "def ljung_box(\n",
        "    x: Iterable[float],\n",
        "    m: Optional[int] = None,\n",
        "    center: bool = True,\n",
        "    pval: str = \"chi2\",          # 'chi2' (Wilson–Hilferty) or 'mc'\n",
        "    nsim: int = 2000,\n",
        "    rng: Optional[np.random.Generator] = None\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Ljung–Box Q = n(n+2) * sum_{k=1}^m (rho_k^2 / (n-k)).\n",
        "    p-values:\n",
        "      - 'chi2' : Wilson–Hilferty approximation to Chi^2_m\n",
        "      - 'mc'   : Monte Carlo under N(0,1) white noise (same n)\n",
        "    \"\"\"\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    n = x.size\n",
        "    if n < 5:\n",
        "        raise ValueError(\"Series too short for Ljung–Box.\")\n",
        "    if m is None:\n",
        "        m = _default_m(n)\n",
        "\n",
        "    # Center if requested\n",
        "    d = x - np.mean(x) if center else x.copy()\n",
        "    denom = float(np.dot(d, d))\n",
        "    if denom <= 0.0:\n",
        "        return {\"Q\": 0.0, \"df\": m, \"p_value\": 1.0, \"method\": \"degenerate\"}\n",
        "\n",
        "    # Autocorrelations rho_1..rho_m (using denominator sum d^2)\n",
        "    rhos = np.empty(m, dtype=float)\n",
        "    for k in range(1, m + 1):\n",
        "        rhos[k-1] = float(np.dot(d[k:], d[:-k])) / denom if n > k else 0.0\n",
        "\n",
        "    Q = n * (n + 2.0) * float(np.sum((rhos ** 2) / (n - np.arange(1, m + 1))))\n",
        "\n",
        "    if pval == \"chi2\":\n",
        "        # Wilson–Hilferty cube-root normal approximation for Chi^2_m tail\n",
        "        df = m\n",
        "        if Q <= 0.0:\n",
        "            p = 1.0\n",
        "        else:\n",
        "            z = ((Q / df) ** (1.0 / 3.0) - (1.0 - 2.0 / (9.0 * df))) / math.sqrt(2.0 / (9.0 * df))\n",
        "            p = 1.0 - _normal_cdf(z)  # upper tail\n",
        "        method = \"chi2 (Wilson–Hilferty)\"\n",
        "    elif pval == \"mc\":\n",
        "        if rng is None:\n",
        "            rng = np.random.default_rng(12345)\n",
        "        ge = 0\n",
        "        for _ in range(nsim):\n",
        "            z = rng.standard_normal(n)\n",
        "            dz = z - z.mean() if center else z\n",
        "            denom_z = float(np.dot(dz, dz))\n",
        "            if denom_z <= 0.0:\n",
        "                continue\n",
        "            rhos_z = []\n",
        "            for k in range(1, m + 1):\n",
        "                rhos_z.append(float(np.dot(dz[k:], dz[:-k])) / denom_z if n > k else 0.0)\n",
        "            Qz = n * (n + 2.0) * float(np.sum((np.array(rhos_z) ** 2) / (n - np.arange(1, m + 1))))\n",
        "            if Qz >= Q:\n",
        "                ge += 1\n",
        "        p = (ge + 1.0) / (nsim + 1.0)\n",
        "        method = f\"monte-carlo (nsim={nsim})\"\n",
        "    else:\n",
        "        raise ValueError(\"pval must be 'chi2' or 'mc'.\")\n",
        "\n",
        "    return {\"Q\": float(Q), \"df\": int(m), \"p_value\": float(min(max(p, 0.0), 1.0)), \"method\": method}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = 600\n",
        "\n",
        "    # (A) White noise (null): should NOT reject\n",
        "    r_null = 0.01 * rng.standard_normal(n)\n",
        "    outA = ljung_box(r_null, m=10, pval=\"chi2\")\n",
        "    print(f\"[Null] Q={outA['Q']:.2f}, df={outA['df']}, p={outA['p_value']:.4f} ({outA['method']})\")\n",
        "\n",
        "    # (B) AR(1) persistence (trend-like): likely to reject\n",
        "    phi = 0.20\n",
        "    e = 0.01 * rng.standard_t(df=4, size=n)\n",
        "    r = np.empty(n); r[0] = e[0]\n",
        "    for t in range(1, n):\n",
        "        r[t] = phi * r[t-1] + e[t]\n",
        "    outB = ljung_box(r, m=10, pval=\"chi2\")\n",
        "    print(f\"[AR(1) phi=0.20] Q={outB['Q']:.2f}, df={outB['df']}, p={outB['p_value']:.6f} ({outB['method']})\")\n",
        "\n",
        "    # (C) Volatility clustering with little mean autocorr: test on squared returns\n",
        "    # Simple GARCH-like volatility; returns are mostly uncorrelated, but r^2 is\n",
        "    omega, alpha, beta = 1e-6, 0.10, 0.86\n",
        "    sig2 = np.empty(n); sig2[0] = 1e-4\n",
        "    z = rng.standard_normal(n)\n",
        "    r_vol = np.empty(n)\n",
        "    for t in range(n):\n",
        "        if t > 0:\n",
        "            sig2[t] = omega + alpha * (r_vol[t-1] ** 2) + beta * sig2[t-1]\n",
        "        r_vol[t] = math.sqrt(sig2[t]) * z[t]\n",
        "\n",
        "    # Ljung–Box on returns (often insignificant) vs on squared, which shows dependence\n",
        "    outC_r = ljung_box(r_vol, m=10, pval=\"chi2\")\n",
        "    outC_s = ljung_box((r_vol**2 - np.mean(r_vol**2)), m=10, pval=\"chi2\")\n",
        "    print(f\"[Vol cluster] r_t:  Q={outC_r['Q']:.2f}, p={outC_r['p_value']:.4f} | \"\n",
        "          f\"(r_t^2): Q={outC_s['Q']:.2f}, p={outC_s['p_value']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnJF_i4Dh0uj",
        "outputId": "e38daa49-16d3-455b-df38-bbd822b52737"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Null] Q=13.59, df=10, p=0.1919 (chi2 (Wilson–Hilferty))\n",
            "[AR(1) phi=0.20] Q=24.90, df=10, p=0.005659 (chi2 (Wilson–Hilferty))\n",
            "[Vol cluster] r_t:  Q=10.41, p=0.4050 | (r_t^2): Q=197.50, p=0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Engle–Granger two-step cointegration test"
      ],
      "metadata": {
        "id": "LHBsNnxDl3tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Dict, Iterable, Optional\n",
        "\n",
        "def _ols_beta(X: np.ndarray, y: np.ndarray):\n",
        "    # OLS via least squares; returns beta, residuals, sigma2 (SSR/T), and (X'X)^(-1)\n",
        "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    resid = y - X @ beta\n",
        "    T, k = X.shape\n",
        "    SSR = float(resid.T @ resid)\n",
        "    sigma2 = SSR / T\n",
        "    XtX_inv = np.linalg.inv(X.T @ X)\n",
        "    return beta, resid, sigma2, XtX_inv\n",
        "\n",
        "def _adf_design(resid: np.ndarray, p: int):\n",
        "    \"\"\"\n",
        "    Build ADF design for Δe_t = γ e_{t-1} + Σ_{i=1}^p φ_i Δe_{t-i} + u_t (no intercept).\n",
        "    Returns Y (length T-p) and X with columns [e_{t-1}, Δe_{t-1}, ..., Δe_{t-p}].\n",
        "    \"\"\"\n",
        "    e = resid\n",
        "    de = np.diff(e)          # length T = n-1\n",
        "    T = de.size\n",
        "    if p < 0 or p > T - 1:\n",
        "        raise ValueError(\"Invalid lag order p for ADF design.\")\n",
        "\n",
        "    # Target vector: Δe_t for t = p+1..T  => indices p..T-1\n",
        "    Y = de[p:]               # length T - p\n",
        "\n",
        "    # Regressor 1: e_{t-1} aligned with Y  => e indices p..T-1 (i.e., e[p:T])\n",
        "    col_e = e[p:T][:, None]  # length T - p\n",
        "\n",
        "    if p == 0:\n",
        "        return Y, col_e\n",
        "\n",
        "    # Regressors 2..(p+1): Δe_{t-i}, i=1..p, aligned with Y\n",
        "    lag_cols = [de[p - i : T - i] for i in range(1, p + 1)]  # each length T - p\n",
        "    X = np.column_stack([col_e] + lag_cols)\n",
        "    return Y, X\n",
        "\n",
        "def _bic(SSR: float, T_eff: int, k: int) -> float:\n",
        "    # BIC = T * ln(SSR/T) + k * ln(T)\n",
        "    return T_eff * np.log(SSR / T_eff) + k * np.log(T_eff)\n",
        "\n",
        "def _select_adf_lag(resid: np.ndarray, p_max: int) -> int:\n",
        "    # Choose p in 0..p_max by BIC\n",
        "    best_p, best_bic = 0, np.inf\n",
        "    for p in range(p_max + 1):\n",
        "        Y, X = _adf_design(resid, p)\n",
        "        T_eff, k = X.shape\n",
        "        beta, r, _, _ = _ols_beta(X, Y)\n",
        "        SSR = float(r.T @ r)\n",
        "        val = _bic(SSR, T_eff, k)\n",
        "        if val < best_bic:\n",
        "            best_bic, best_p = val, p\n",
        "    return best_p\n",
        "\n",
        "def engle_granger(\n",
        "    y: Iterable[float],\n",
        "    x: Iterable[float],\n",
        "    p_max: Optional[int] = None,\n",
        "    nsim: int = 2000,\n",
        "    rng: Optional[np.random.Generator] = None\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Engle–Granger two-step cointegration test with BIC lag selection for ADF\n",
        "    and Monte Carlo p-value under the null (independent random walks).\n",
        "    Returns slope (hedge ratio), adf_t, chosen lag p, and p_mc.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng(12345)\n",
        "    y = np.asarray(list(y), dtype=float)\n",
        "    x = np.asarray(list(x), dtype=float)\n",
        "    mask = np.isfinite(y) & np.isfinite(x)\n",
        "    y, x = y[mask], x[mask]\n",
        "    n = y.size\n",
        "    if n < 30:\n",
        "        raise ValueError(\"Need at least ~30 observations for a meaningful EG test.\")\n",
        "\n",
        "    # Step 1: OLS Y on [1, X]\n",
        "    X1 = np.column_stack([np.ones(n), x])\n",
        "    b, e, _, _ = _ols_beta(X1, y)  # b[1] is hedge ratio\n",
        "    beta0, beta1 = float(b[0]), float(b[1])\n",
        "\n",
        "    # Step 2: ADF on residuals (no intercept)\n",
        "    if p_max is None:\n",
        "        p_max = max(0, min(12, int(np.floor(12 * (n / 100.0) ** 0.25))))  # compact ROT\n",
        "    p = _select_adf_lag(e, p_max)\n",
        "    Y, X = _adf_design(e, p)\n",
        "    T_eff, k = X.shape\n",
        "    beta_adf, resid_adf, SSR, XtX_inv = _ols_beta(X, Y)\n",
        "    # t-stat for gamma = coefficient on e_{t-1}\n",
        "    sigma_hat2 = float((resid_adf.T @ resid_adf) / (T_eff - k))\n",
        "    se = float(np.sqrt(sigma_hat2 * XtX_inv[0, 0]))\n",
        "    gamma = float(beta_adf[0])\n",
        "    t_stat = gamma / se\n",
        "\n",
        "    # Monte Carlo p-value under null: X, Y are independent RW, same n\n",
        "    # For each sim: regress Y on X, build residuals, run ADF with the same p_max and lag selection.\n",
        "    # Tail is \"more negative than observed\" => evidence for stationarity.\n",
        "    count = 0\n",
        "    for _ in range(nsim):\n",
        "        # Random walks with unit Gaussian innovations\n",
        "        u = rng.standard_normal(n)\n",
        "        v = rng.standard_normal(n)\n",
        "        Xrw = np.cumsum(u)\n",
        "        Yrw = np.cumsum(v)\n",
        "        B, E, _, _ = _ols_beta(np.column_stack([np.ones(n), Xrw]), Yrw)\n",
        "        # ADF on residuals\n",
        "        ps = _select_adf_lag(E, p_max)\n",
        "        Ys, Xs = _adf_design(E, ps)\n",
        "        Ts, ks = Xs.shape\n",
        "        bet_s, r_s, _, XtX_inv_s = _ols_beta(Xs, Ys)\n",
        "        sig2_s = float((r_s.T @ r_s) / (Ts - ks))\n",
        "        se_s = float(np.sqrt(sig2_s * XtX_inv_s[0, 0]))\n",
        "        t_s = float(bet_s[0] / se_s)\n",
        "        if t_s <= t_stat:\n",
        "            count += 1\n",
        "    p_mc = (count + 1.0) / (nsim + 1.0)\n",
        "\n",
        "    return {\n",
        "        \"beta0\": beta0,\n",
        "        \"beta1\": beta1,\n",
        "        \"adf_t\": t_stat,\n",
        "        \"lag_p\": int(p),\n",
        "        \"p_value_mc\": float(p_mc),\n",
        "        \"n\": int(n)\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rng = np.random.default_rng(7)\n",
        "    n = 600\n",
        "\n",
        "    # (A) Cointegrated pair: Y = 0.5 + 2.0 * X + u_t,  u_t stationary AR(1)\n",
        "    eps = rng.standard_normal(n)\n",
        "    X = np.cumsum(rng.standard_normal(n))                        # random walk\n",
        "    u = np.empty(n); u[0] = eps[0] * 0.3\n",
        "    phi = 0.6\n",
        "    for t in range(1, n):\n",
        "        u[t] = phi * u[t-1] + 0.3 * eps[t]                      # stationary\n",
        "    Y = 0.5 + 2.0 * X + u\n",
        "    eg_ok = engle_granger(Y, X, nsim=1500, rng=rng)\n",
        "    print(f\"[Cointegrated] beta1≈{eg_ok['beta1']:.3f}, t_adf={eg_ok['adf_t']:.3f}, \"\n",
        "          f\"p_mc≈{eg_ok['p_value_mc']:.4f}, p={eg_ok['lag_p']}\")\n",
        "\n",
        "    # (B) Spurious correlation: independent random walks\n",
        "    Xs = np.cumsum(rng.standard_normal(n))\n",
        "    Ys = np.cumsum(rng.standard_normal(n))\n",
        "    eg_spur = engle_granger(Ys, Xs, nsim=1500, rng=rng)\n",
        "    print(f\"[Spurious RW]  beta1≈{eg_spur['beta1']:.3f}, t_adf={eg_spur['adf_t']:.3f}, \"\n",
        "          f\"p_mc≈{eg_spur['p_value_mc']:.4f}, p={eg_spur['lag_p']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8vRh18Ml3z-",
        "outputId": "97f2bd44-7837-4b63-b1cf-5f9daa3c5dd9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Cointegrated] beta1≈2.002, t_adf=-12.536, p_mc≈0.0007, p=0\n",
            "[Spurious RW]  beta1≈-0.754, t_adf=-1.773, p_mc≈0.6356, p=0\n"
          ]
        }
      ]
    }
  ]
}
