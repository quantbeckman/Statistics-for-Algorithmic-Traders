{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "OLS in simple regression"
      ],
      "metadata": {
        "id": "pft8e--BezUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# 1) Generate synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(2025)\n",
        "n = 1200\n",
        "beta0_true, beta1_true = 0.02, 0.75\n",
        "X = rng.normal(0.0, 1.2, n)\n",
        "eps = rng.normal(0.0, 0.45, n)\n",
        "Y = beta0_true + beta1_true * X + eps\n",
        "\n",
        "df = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "\n",
        "# 2) Closed-form OLS estimators\n",
        "# ----------------------------\n",
        "xbar, ybar = df[\"X\"].mean(), df[\"Y\"].mean()\n",
        "Sxx = float(((df[\"X\"] - xbar)**2).sum())\n",
        "Sxy = float(((df[\"X\"] - xbar)*(df[\"Y\"] - ybar)).sum())\n",
        "\n",
        "beta1_hat = Sxy / Sxx\n",
        "beta0_hat = ybar - beta1_hat * xbar\n",
        "\n",
        "# Fitted values, residuals, variance estimate\n",
        "Y_hat = beta0_hat + beta1_hat * df[\"X\"].to_numpy()\n",
        "resid = df[\"Y\"].to_numpy() - Y_hat\n",
        "df_resid = n - 2\n",
        "sigma2_hat = (resid @ resid) / df_resid\n",
        "\n",
        "# Standard errors\n",
        "se_beta1 = np.sqrt(sigma2_hat / Sxx)\n",
        "se_beta0 = np.sqrt(sigma2_hat * (1.0/n + (xbar**2)/Sxx))\n",
        "\n",
        "# t-stats, two-sided 95% CIs\n",
        "t_beta1 = beta1_hat / se_beta1\n",
        "t_beta0 = beta0_hat / se_beta0\n",
        "tcrit = stats.t.ppf(0.975, df_resid)\n",
        "ci_beta1 = (beta1_hat - tcrit*se_beta1, beta1_hat + tcrit*se_beta1)\n",
        "ci_beta0 = (beta0_hat - tcrit*se_beta0, beta0_hat + tcrit*se_beta0)\n",
        "\n",
        "# Goodness of fit\n",
        "TSS = float(((df[\"Y\"] - ybar)**2).sum())\n",
        "RSS = float((resid**2).sum())\n",
        "R2 = 1.0 - RSS / TSS\n",
        "adj_R2 = 1.0 - (1.0 - R2) * (n - 1) / (n - 2)\n",
        "\n",
        "# 3) statsmodels cross-check\n",
        "# ----------------------------\n",
        "X1 = sm.add_constant(df[\"X\"].to_numpy(), has_constant=\"add\")\n",
        "ols = sm.OLS(df[\"Y\"].to_numpy(), X1).fit()\n",
        "ols_hc1 = ols.get_robustcov_results(cov_type=\"HC1\")\n",
        "\n",
        "# 4) Mean-response CI and prediction interval at x0\n",
        "#    mean CI: yhat ± tcrit * sqrt( sigma^2 * [ 1/n + (x0 - xbar)^2 / Sxx ] )\n",
        "#    pred PI: yhat ± tcrit * sqrt( sigma^2 * [ 1 + 1/n + (x0 - xbar)^2 / Sxx ] )\n",
        "# ----------------------------\n",
        "x0 = 1.00\n",
        "y0_hat = beta0_hat + beta1_hat * x0\n",
        "se_mean = np.sqrt(sigma2_hat * (1.0/n + ((x0 - xbar)**2)/Sxx))\n",
        "se_pred = np.sqrt(sigma2_hat * (1.0 + 1.0/n + ((x0 - xbar)**2)/Sxx))\n",
        "ci_mean = (y0_hat - tcrit*se_mean, y0_hat + tcrit*se_mean)\n",
        "pi_pred = (y0_hat - tcrit*se_pred, y0_hat + tcrit*se_pred)\n",
        "\n",
        "# statsmodels check via get_prediction\n",
        "pred = ols.get_prediction(exog=[1.0, x0])\n",
        "ci_mean_sm = tuple(pred.summary_frame(alpha=0.05)[[\"mean_ci_lower\",\"mean_ci_upper\"]].iloc[0])\n",
        "pi_pred_sm = tuple(pred.summary_frame(alpha=0.05)[[\"obs_ci_lower\",\"obs_ci_upper\"]].iloc[0])\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Simple Linear Regression — OLS (closed-form) vs statsmodels\")\n",
        "print(\"=\"*72)\n",
        "print(f\"True betas      : beta0={beta0_true:.3f}, beta1={beta1_true:.3f}\")\n",
        "print(f\"Closed-form OLS : beta0={beta0_hat:.3f} (se={se_beta0:.3f}, t={beta0_hat/se_beta0:.2f}, CI95={ci_beta0})\")\n",
        "print(f\"                  beta1={beta1_hat:.3f} (se={se_beta1:.3f}, t={t_beta1:.2f}, CI95={ci_beta1})\")\n",
        "print(f\"Fit             : R^2={R2:.4f}, adj R^2={adj_R2:.4f}\")\n",
        "print(\"-\"*72)\n",
        "print(\"statsmodels OLS (homoskedastic):\")\n",
        "print(f\"  beta0={ols.params[0]:.3f} (se={ols.bse[0]:.3f}), \"\n",
        "      f\"beta1={ols.params[1]:.3f} (se={ols.bse[1]:.3f}), R^2={ols.rsquared:.4f}\")\n",
        "print(\"statsmodels OLS (HC1 robust SE):\")\n",
        "print(f\"  beta0={ols_hc1.params[0]:.3f} (se={ols_hc1.bse[0]:.3f}), \"\n",
        "      f\"beta1={ols_hc1.params[1]:.3f} (se={ols_hc1.bse[1]:.3f})\")\n",
        "print(\"-\"*72)\n",
        "print(f\"Mean-response CI at x0={x0:.2f} : {ci_mean} (closed-form) vs {ci_mean_sm} (sm)\")\n",
        "print(f\"Prediction interval at x0={x0:.2f}: {pi_pred} (closed-form) vs {pi_pred_sm} (sm)\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7QtG9Vuez4h",
        "outputId": "db12b865-2346-4b42-ae16-c910b48ca8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Simple Linear Regression — OLS (closed-form) vs statsmodels\n",
            "========================================================================\n",
            "True betas      : beta0=0.020, beta1=0.750\n",
            "Closed-form OLS : beta0=0.003 (se=0.013, t=0.25, CI95=(np.float64(-0.022367787047077002), np.float64(0.028988837442635945)))\n",
            "                  beta1=0.758 (se=0.011, t=69.30, CI95=(np.float64(0.736394877958875), np.float64(0.7793051122689277)))\n",
            "Fit             : R^2=0.8004, adj R^2=0.8002\n",
            "------------------------------------------------------------------------\n",
            "statsmodels OLS (homoskedastic):\n",
            "  beta0=0.003 (se=0.013), beta1=0.758 (se=0.011), R^2=0.8004\n",
            "statsmodels OLS (HC1 robust SE):\n",
            "  beta0=0.003 (se=0.013), beta1=0.758 (se=0.011)\n",
            "------------------------------------------------------------------------\n",
            "Mean-response CI at x0=1.00 : (np.float64(0.7273899445498223), np.float64(0.7949310960735395)) (closed-form) vs (0.727389944549822, 0.7949310960735392) (sm)\n",
            "Prediction interval at x0=1.00: (np.float64(-0.12884543567577822), np.float64(1.65116647629914)) (closed-form) vs (-0.12884543567577866, 1.6511664762991398) (sm)\n",
            "========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence intervals of slope and intercept"
      ],
      "metadata": {
        "id": "8U3wxOQGe0Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "# 1) Synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(101)\n",
        "n = 1000\n",
        "beta0_true, beta1_true = 0.02, 0.80\n",
        "X = rng.normal(0.0, 1.0, n)\n",
        "# Heteroskedastic errors: variance increases with |X|\n",
        "sigma = 0.30 * (1.0 + 0.7 * np.abs(X))\n",
        "eps = rng.normal(0.0, sigma, n)\n",
        "Y = beta0_true + beta1_true * X + eps\n",
        "\n",
        "df = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "\n",
        "\n",
        "# 2) Classical OLS (closed-form) and CIs\n",
        "# ----------------------------\n",
        "xbar, ybar = df[\"X\"].mean(), df[\"Y\"].mean()\n",
        "Sxx = float(((df[\"X\"] - xbar)**2).sum())\n",
        "Sxy = float(((df[\"X\"] - xbar)*(df[\"Y\"] - ybar)).sum())\n",
        "\n",
        "beta1_hat = Sxy / Sxx\n",
        "beta0_hat = ybar - beta1_hat * xbar\n",
        "\n",
        "# Residual variance (homoskedastic OLS)\n",
        "Y_hat = beta0_hat + beta1_hat * df[\"X\"].to_numpy()\n",
        "resid = df[\"Y\"].to_numpy() - Y_hat\n",
        "df_resid = n - 2\n",
        "sigma2_hat = (resid @ resid) / df_resid\n",
        "\n",
        "# Standard errors\n",
        "se_beta1 = np.sqrt(sigma2_hat / Sxx)\n",
        "se_beta0 = np.sqrt(sigma2_hat * (1.0/n + (xbar**2)/Sxx))\n",
        "\n",
        "# 95% CIs using Student-t with (n-2) dof\n",
        "tcrit = stats.t.ppf(0.975, df_resid)\n",
        "ci_beta1_classical = (beta1_hat - tcrit*se_beta1, beta1_hat + tcrit*se_beta1)\n",
        "ci_beta0_classical = (beta0_hat - tcrit*se_beta0, beta0_hat + tcrit*se_beta0)\n",
        "\n",
        "\n",
        "# 3) HC1-robust CIs via statsmodels\n",
        "# ----------------------------\n",
        "X1 = sm.add_constant(df[\"X\"].to_numpy(), has_constant=\"add\")\n",
        "ols = sm.OLS(df[\"Y\"].to_numpy(), X1).fit()\n",
        "ols_hc1 = ols.get_robustcov_results(cov_type=\"HC1\")  # heteroskedasticity-robust\n",
        "\n",
        "beta0_sm, beta1_sm = float(ols_hc1.params[0]), float(ols_hc1.params[1])\n",
        "se_beta0_hc1, se_beta1_hc1 = float(ols_hc1.bse[0]), float(ols_hc1.bse[1])\n",
        "\n",
        "# Use the same t critical (large-sample normal would give a similar result)\n",
        "ci_beta1_hc1 = (beta1_sm - tcrit*se_beta1_hc1, beta1_sm + tcrit*se_beta1_hc1)\n",
        "ci_beta0_hc1 = (beta0_sm - tcrit*se_beta0_hc1, beta0_sm + tcrit*se_beta0_hc1)\n",
        "\n",
        "def fmt(ci): return f\"({ci[0]:.3f}, {ci[1]:.3f})\"\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Confidence Intervals for Simple Linear Regression Coefficients\")\n",
        "print(\"=\"*72)\n",
        "print(f\"True betas: beta0={beta0_true:.3f}, beta1={beta1_true:.3f}\\n\")\n",
        "\n",
        "print(\"Classical OLS (homoskedastic) estimates & 95% CIs:\")\n",
        "print(f\"  beta0_hat={beta0_hat:.3f}, CI95={fmt(ci_beta0_classical)}\")\n",
        "print(f\"  beta1_hat={beta1_hat:.3f}, CI95={fmt(ci_beta1_classical)}\\n\")\n",
        "\n",
        "print(\"HC1-robust (heteroskedasticity-robust) estimates & 95% CIs:\")\n",
        "print(f\"  beta0_sm ={beta0_sm:.3f},  CI95={fmt(ci_beta0_hc1)}\")\n",
        "print(f\"  beta1_sm ={beta1_sm:.3f},  CI95={fmt(ci_beta1_hc1)}\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMP57B9we0Sz",
        "outputId": "2cdb44a2-100b-49ab-d5c1-059f9d0839c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Confidence Intervals for Simple Linear Regression Coefficients\n",
            "========================================================================\n",
            "True betas: beta0=0.020, beta1=0.800\n",
            "\n",
            "Classical OLS (homoskedastic) estimates & 95% CIs:\n",
            "  beta0_hat=0.013, CI95=(-0.017, 0.044)\n",
            "  beta1_hat=0.842, CI95=(0.813, 0.871)\n",
            "\n",
            "HC1-robust (heteroskedasticity-robust) estimates & 95% CIs:\n",
            "  beta0_sm =0.013,  CI95=(-0.017, 0.044)\n",
            "  beta1_sm =0.842,  CI95=(0.804, 0.880)\n",
            "========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence interval of the mean response"
      ],
      "metadata": {
        "id": "jSkeOcPtpL5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# 1) Synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(314)\n",
        "n = 1000\n",
        "beta0_true, beta1_true = 0.02, 0.80\n",
        "X = rng.normal(0.0, 1.0, n)\n",
        "# Heteroskedastic errors: variance grows with |X|\n",
        "sigma = 0.35 * (1.0 + 0.7 * np.abs(X))\n",
        "eps = rng.normal(0.0, sigma, n)\n",
        "Y = beta0_true + beta1_true * X + eps\n",
        "\n",
        "df = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "\n",
        "\n",
        "# 2) Closed-form OLS estimates\n",
        "# ----------------------------\n",
        "xbar, ybar = df[\"X\"].mean(), df[\"Y\"].mean()\n",
        "Sxx = float(((df[\"X\"] - xbar)**2).sum())\n",
        "Sxy = float(((df[\"X\"] - xbar)*(df[\"Y\"] - ybar)).sum())\n",
        "\n",
        "beta1_hat = Sxy / Sxx\n",
        "beta0_hat = ybar - beta1_hat * xbar\n",
        "\n",
        "Y_hat = beta0_hat + beta1_hat * df[\"X\"].to_numpy()\n",
        "resid = df[\"Y\"].to_numpy() - Y_hat\n",
        "df_resid = n - 2\n",
        "sigma2_hat = (resid @ resid) / df_resid\n",
        "tcrit = stats.t.ppf(0.975, df_resid)  # 95% two-sided\n",
        "\n",
        "\n",
        "# 3) HC1-robust covariance for parameters\n",
        "# ----------------------------\n",
        "X1 = sm.add_constant(df[\"X\"].to_numpy(), has_constant=\"add\")\n",
        "ols = sm.OLS(df[\"Y\"].to_numpy(), X1).fit()\n",
        "ols_hc1 = ols.get_robustcov_results(cov_type=\"HC1\")\n",
        "Cov_hc1 = ols_hc1.cov_params()  # 2x2 covariance of [beta0, beta1]\n",
        "\n",
        "\n",
        "# 4) CI for mean response at selected X0 values\n",
        "# Classical: Var(ŷ(x0)) = σ^2 * [ 1/n + (x0 - xbar)^2 / Sxx ]\n",
        "# Robust:    Var(ŷ(x0)) = x0_vec' * Cov(beta) * x0_vec,  x0_vec=[1, x0]\n",
        "# ----------------------------\n",
        "def ci_mean_classical(x0: float):\n",
        "    yhat = beta0_hat + beta1_hat * x0\n",
        "    se = np.sqrt(sigma2_hat * (1.0/n + ((x0 - xbar)**2)/Sxx))\n",
        "    return yhat, (yhat - tcrit*se, yhat + tcrit*se), se\n",
        "\n",
        "def ci_mean_robust(x0: float):\n",
        "    xvec = np.array([1.0, x0])\n",
        "    yhat = float(np.dot(xvec, np.array([ols_hc1.params[0], ols_hc1.params[1]])))\n",
        "    se = float(np.sqrt(xvec @ Cov_hc1 @ xvec))\n",
        "    return yhat, (yhat - tcrit*se, yhat + tcrit*se), se\n",
        "\n",
        "# Choose points: below mean, at mean, above mean\n",
        "x0_list = [np.quantile(X, 0.15), xbar, np.quantile(X, 0.85)]\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Confidence intervals for the MEAN response E[Y|X0]\")\n",
        "print(\"=\"*72)\n",
        "for x0 in x0_list:\n",
        "    y_c, ci_c, se_c = ci_mean_classical(float(x0))\n",
        "    y_r, ci_r, se_r = ci_mean_robust(float(x0))\n",
        "    print(f\"X0 = {x0: .3f}\")\n",
        "    print(f\"  Classical ŷ={y_c: .3f},  CI95={ci_c},  SE={se_c:.4f}\")\n",
        "    print(f\"  Robust    ŷ={y_r: .3f},  CI95={ci_r},  SE={se_r:.4f}\")\n",
        "    print(\"-\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xEfXEYXpOnT",
        "outputId": "febee17c-f856-41b3-f7c6-52fbc1dd6dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Confidence intervals for the MEAN response E[Y|X0]\n",
            "========================================================================\n",
            "X0 = -0.895\n",
            "  Classical ŷ=-0.735,  CI95=(np.float64(-0.7825684980182602), np.float64(-0.6873777229876489)),  SE=0.0243\n",
            "  Robust    ŷ=-0.735,  CI95=(np.float64(-0.7918742713952023), np.float64(-0.6780719496107086)),  SE=0.0290\n",
            "------------------------------------------------------------------------\n",
            "X0 =  0.037\n",
            "  Classical ŷ= 0.019,  CI95=(np.float64(-0.015854160115375375), np.float64(0.05302095371959762)),  SE=0.0175\n",
            "  Robust    ŷ= 0.019,  CI95=(np.float64(-0.015854160115375444), np.float64(0.05302095371959758)),  SE=0.0175\n",
            "------------------------------------------------------------------------\n",
            "X0 =  1.038\n",
            "  Classical ŷ= 0.828,  CI95=(np.float64(0.7785160881547518), np.float64(0.8771207257613852)),  SE=0.0251\n",
            "  Robust    ŷ= 0.828,  CI95=(np.float64(0.7667670579810026), np.float64(0.888869755935136)),  SE=0.0311\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction interval of a new observation"
      ],
      "metadata": {
        "id": "YDVMCSE7xjYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "# 1) Synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(2718)\n",
        "n = 1000\n",
        "beta0_true, beta1_true = 0.02, 0.80\n",
        "X = rng.normal(0.0, 1.0, n)\n",
        "# Heteroskedastic errors: variance increases with |X|\n",
        "sigma = 0.35 * (1.0 + 0.7 * np.abs(X))\n",
        "eps = rng.normal(0.0, sigma, n)\n",
        "Y = beta0_true + beta1_true * X + eps\n",
        "\n",
        "df = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "\n",
        "\n",
        "# 2) Closed-form OLS estimates\n",
        "# ----------------------------\n",
        "xbar, ybar = df[\"X\"].mean(), df[\"Y\"].mean()\n",
        "Sxx = float(((df[\"X\"] - xbar)**2).sum())\n",
        "Sxy = float(((df[\"X\"] - xbar)*(df[\"Y\"] - ybar)).sum())\n",
        "\n",
        "beta1_hat = Sxy / Sxx\n",
        "beta0_hat = ybar - beta1_hat * xbar\n",
        "\n",
        "Y_hat = beta0_hat + beta1_hat * df[\"X\"].to_numpy()\n",
        "resid = df[\"Y\"].to_numpy() - Y_hat\n",
        "df_resid = n - 2\n",
        "sigma2_hat = (resid @ resid) / df_resid\n",
        "tcrit = stats.t.ppf(0.975, df_resid)  # 95% two-sided\n",
        "\n",
        "# 3) HC1-robust covariance for parameters\n",
        "# ----------------------------\n",
        "X1 = sm.add_constant(df[\"X\"].to_numpy(), has_constant=\"add\")\n",
        "ols = sm.OLS(df[\"Y\"].to_numpy(), X1).fit()\n",
        "ols_hc1 = ols.get_robustcov_results(cov_type=\"HC1\")\n",
        "Cov_hc1 = ols_hc1.cov_params()  # 2x2 covariance of [beta0, beta1]\n",
        "\n",
        "# 4) Prediction interval at selected X0 values\n",
        "# Classical PI variance:\n",
        "#   Var_pred_classical(x0) = σ^2 * [ 1 + 1/n + (x0 - xbar)^2 / Sxx ]\n",
        "# Robust PI variance (large-sample heuristic):\n",
        "#   Var_pred_robust(x0) = x0_vec' Cov_HC1(beta) x0_vec + σ^2\n",
        "# where x0_vec = [1, x0]\n",
        "# ----------------------------\n",
        "def pi_classical(x0: float):\n",
        "    yhat = beta0_hat + beta1_hat * x0\n",
        "    se_pred = np.sqrt(sigma2_hat * (1.0 + 1.0/n + ((x0 - xbar)**2)/Sxx))\n",
        "    return yhat, (yhat - tcrit*se_pred, yhat + tcrit*se_pred), se_pred\n",
        "\n",
        "def pi_robust(x0: float):\n",
        "    xvec = np.array([1.0, x0])\n",
        "    yhat = float(xvec @ np.array([ols_hc1.params[0], ols_hc1.params[1]]))\n",
        "    var_pred = float(xvec @ Cov_hc1 @ xvec) + sigma2_hat\n",
        "    se_pred = np.sqrt(var_pred)\n",
        "    return yhat, (yhat - tcrit*se_pred, yhat + tcrit*se_pred), se_pred\n",
        "\n",
        "# Optional sm cross-check for classical PI at a single point\n",
        "def pi_sm(x0: float):\n",
        "    fr = ols.get_prediction(exog=[1.0, x0]).summary_frame(alpha=0.05)\n",
        "    return float(fr[\"obs_ci_lower\"][0]), float(fr[\"obs_ci_upper\"][0])\n",
        "\n",
        "# Choose points: below mean, at mean, above mean\n",
        "x0_list = [np.quantile(X, 0.15), xbar, np.quantile(X, 0.85)]\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Prediction intervals for a NEW observation Y_new | X0\")\n",
        "print(\"=\"*72)\n",
        "for x0 in x0_list:\n",
        "    y_c, pi_c, se_c = pi_classical(float(x0))\n",
        "    y_r, pi_r, se_r = pi_robust(float(x0))\n",
        "    pi_sm_lo, pi_sm_hi = pi_sm(float(x0))\n",
        "    print(f\"X0 = {x0: .3f}\")\n",
        "    print(f\"  Classical ŷ={y_c: .3f},  PI95={pi_c},  SE_pred={se_c:.4f}  [sm check: ({pi_sm_lo:.3f}, {pi_sm_hi:.3f})]\")\n",
        "    print(f\"  Robust    ŷ={y_r: .3f},  PI95={pi_r},  SE_pred={se_r:.4f}\")\n",
        "    print(\"-\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO0JQs8-xm3c",
        "outputId": "0eb3b0c1-03f5-4ade-baae-989df032382e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Prediction intervals for a NEW observation Y_new | X0\n",
            "========================================================================\n",
            "X0 = -1.114\n",
            "  Classical ŷ=-0.863,  PI95=(np.float64(-2.0011642082059193), np.float64(0.2752796328380316)),  SE_pred=0.5800  [sm check: (-2.001, 0.275)]\n",
            "  Robust    ŷ=-0.863,  PI95=(np.float64(-2.0018114455082334), np.float64(0.275926870140346)),  SE_pred=0.5804\n",
            "------------------------------------------------------------------------\n",
            "X0 = -0.023\n",
            "  Classical ŷ= 0.016,  PI95=(np.float64(-1.121748792540356), np.float64(1.1534928169165177)),  SE_pred=0.5797  [sm check: (-1.122, 1.153)]\n",
            "  Robust    ŷ= 0.016,  PI95=(np.float64(-1.1217487925403558), np.float64(1.153492816916518)),  SE_pred=0.5797\n",
            "------------------------------------------------------------------------\n",
            "X0 =  1.032\n",
            "  Classical ŷ= 0.866,  PI95=(np.float64(-0.27242677352793354), np.float64(2.003939235990842)),  SE_pred=0.5800  [sm check: (-0.272, 2.004)]\n",
            "  Robust    ŷ= 0.866,  PI95=(np.float64(-0.27289336674327636), np.float64(2.004405829206185)),  SE_pred=0.5802\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coefficient of determination"
      ],
      "metadata": {
        "id": "Hxm_BtgUzzts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "def r2_metrics(y_true: np.ndarray, y_hat: np.ndarray, n_params: int) -> dict:\n",
        "    \"\"\"Compute SSE, SST, R^2, adjusted R^2 given y and fitted values.\"\"\"\n",
        "    y_true = np.asarray(y_true); y_hat = np.asarray(y_hat)\n",
        "    sse = float(np.sum((y_true - y_hat)**2))\n",
        "    sst = float(np.sum((y_true - y_true.mean())**2))\n",
        "    r2 = 1.0 - sse / sst\n",
        "    n = y_true.size\n",
        "    k = n_params  # number of slope parameters (exclude intercept)\n",
        "    adj_r2 = 1.0 - (1.0 - r2) * (n - 1) / (n - 1 - k)\n",
        "    return {\"SSE\": sse, \"SST\": sst, \"R2\": r2, \"adj_R2\": adj_r2}\n",
        "\n",
        "def fit_linear(X: np.ndarray, y: np.ndarray):\n",
        "    X1 = sm.add_constant(X, has_constant=\"add\")\n",
        "    model = sm.OLS(y, X1).fit()\n",
        "    yhat = model.fittedvalues\n",
        "    # n_params = number of slopes (exclude intercept) -> 1 here\n",
        "    return model, np.asarray(yhat), 1\n",
        "\n",
        "def fit_quadratic(X: np.ndarray, y: np.ndarray):\n",
        "    X2 = np.column_stack([X, X**2])\n",
        "    X2 = sm.add_constant(X2, has_constant=\"add\")\n",
        "    model = sm.OLS(y, X2).fit()\n",
        "    yhat = model.fittedvalues\n",
        "    # two slopes: X and X^2\n",
        "    return model, np.asarray(yhat), 2\n",
        "\n",
        "def oos_r2(model, X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
        "    \"\"\"Out-of-sample R^2 = 1 - SSE_test / SST_test (baseline is train-mean of y).\"\"\"\n",
        "    # Build the appropriate design matrix for prediction based on the model exog shape\n",
        "    if model.model.exog.shape[1] == 2:  # const + X\n",
        "        Xo = sm.add_constant(X_test, has_constant=\"add\")\n",
        "    else:  # const + X + X^2\n",
        "        Xo = sm.add_constant(np.column_stack([X_test, X_test**2]), has_constant=\"add\")\n",
        "    yhat = model.predict(Xo)\n",
        "    sse = float(np.sum((y_test - yhat)**2))\n",
        "    sst = float(np.sum((y_test - y_test.mean())**2))\n",
        "    return 1.0 - sse / sst\n",
        "\n",
        "# 1) Linear DGP\n",
        "# ----------------------------\n",
        "n = 1500\n",
        "X_lin = rng.normal(0.0, 1.0, n)\n",
        "eps_lin = rng.normal(0.0, 0.5, n)\n",
        "beta0_L, beta1_L = 0.5, 1.2\n",
        "Y_lin = beta0_L + beta1_L * X_lin + eps_lin\n",
        "\n",
        "# Fit simple linear model\n",
        "mL, yhatL, kL = fit_linear(X_lin, Y_lin)\n",
        "metL = r2_metrics(Y_lin, yhatL, n_params=kL)\n",
        "\n",
        "# Train/Test split and OOS R^2\n",
        "idx = rng.permutation(n)\n",
        "train = idx[: int(0.7*n)]\n",
        "test  = idx[int(0.7*n):]\n",
        "r2_oos_L = oos_r2(mL, X_lin[test], Y_lin[test])\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Coefficient of Determination — Linear DGP\")\n",
        "print(\"=\"*72)\n",
        "print(f\"In-sample: R^2 = {metL['R2']:.4f},  adj R^2 = {metL['adj_R2']:.4f}\")\n",
        "print(f\"Out-of-sample R^2 (test): {r2_oos_L:.4f}\")\n",
        "print(\"-\"*72)\n",
        "\n",
        "# 2) Nonlinear DGP (quadratic component)\n",
        "# ----------------------------\n",
        "X_nl = rng.normal(0.0, 1.2, n)\n",
        "eps_nl = rng.normal(0.0, 0.6, n)\n",
        "beta0_NL, beta1_NL, beta2_NL = 0.2, 0.3, 0.9\n",
        "Y_nl = beta0_NL + beta1_NL * X_nl + beta2_NL * (X_nl**2) + eps_nl\n",
        "\n",
        "# (a) Fit simple linear model (misspecified)\n",
        "mN_lin, yhatN_lin, kN_lin = fit_linear(X_nl, Y_nl)\n",
        "metN_lin = r2_metrics(Y_nl, yhatN_lin, n_params=kN_lin)\n",
        "\n",
        "# (b) Fit transformed model with quadratic term\n",
        "mN_quad, yhatN_quad, kN_quad = fit_quadratic(X_nl, Y_nl)\n",
        "metN_quad = r2_metrics(Y_nl, yhatN_quad, n_params=kN_quad)\n",
        "\n",
        "# OOS R^2 for both (same split logic)\n",
        "idx2 = rng.permutation(n)\n",
        "train2 = idx2[: int(0.7*n)]\n",
        "test2  = idx2[int(0.7*n):]\n",
        "r2_oos_N_lin  = oos_r2(mN_lin,  X_nl[test2], Y_nl[test2])\n",
        "r2_oos_N_quad = oos_r2(mN_quad, X_nl[test2], Y_nl[test2])\n",
        "\n",
        "print(\"Coefficient of Determination — Nonlinear DGP\")\n",
        "print(\"=\"*72)\n",
        "print(\"(Misspecified) Simple linear model on nonlinear data:\")\n",
        "print(f\"  In-sample: R^2 = {metN_lin['R2']:.4f}, adj R^2 = {metN_lin['adj_R2']:.4f}\")\n",
        "print(f\"  Out-of-sample R^2 (test): {r2_oos_N_lin:.4f}\")\n",
        "print(\"-\"*48)\n",
        "print(\"Transformed model with quadratic term:\")\n",
        "print(f\"  In-sample: R^2 = {metN_quad['R2']:.4f}, adj R^2 = {metN_quad['adj_R2']:.4f}\")\n",
        "print(f\"  Out-of-sample R^2 (test): {r2_oos_N_quad:.4f}\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9WQVtHtz1AJ",
        "outputId": "3ffec6d3-2266-4459-c454-4d9b99a5deac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Coefficient of Determination — Linear DGP\n",
            "========================================================================\n",
            "In-sample: R^2 = 0.8544,  adj R^2 = 0.8543\n",
            "Out-of-sample R^2 (test): 0.8357\n",
            "------------------------------------------------------------------------\n",
            "Coefficient of Determination — Nonlinear DGP\n",
            "========================================================================\n",
            "(Misspecified) Simple linear model on nonlinear data:\n",
            "  In-sample: R^2 = 0.0165, adj R^2 = 0.0159\n",
            "  Out-of-sample R^2 (test): 0.0073\n",
            "------------------------------------------------\n",
            "Transformed model with quadratic term:\n",
            "  In-sample: R^2 = 0.8872, adj R^2 = 0.8870\n",
            "  Out-of-sample R^2 (test): 0.8850\n",
            "========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformations to a straight line"
      ],
      "metadata": {
        "id": "W2qUIiNH3hdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "from statsmodels.stats.stattools import jarque_bera\n",
        "from scipy import stats\n",
        "\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _nanmin_safe(a):\n",
        "    a = np.asarray(a, float)\n",
        "    a = a[np.isfinite(a)]\n",
        "    return np.nan if a.size == 0 else np.min(a)\n",
        "\n",
        "def fit_transform_params(arr, kind):\n",
        "    \"\"\"Learn per-transform params on TRAIN data (shift to ensure positivity; lambda for Box-Cox).\"\"\"\n",
        "    arr = np.asarray(arr, float)\n",
        "    if kind == \"identity\":\n",
        "        return {\"shift\": 0.0, \"lambda\": None}\n",
        "    if kind in (\"log\", \"sqrt\", \"boxcox\"):\n",
        "        m = _nanmin_safe(arr)\n",
        "        shift = 0.0 if not np.isfinite(m) or m > 0 else (-m + 1e-6)\n",
        "        out = {\"shift\": float(shift), \"lambda\": None}\n",
        "        if kind == \"boxcox\":\n",
        "            # boxcox returns (transformed, lambda) when lmbda=None\n",
        "            _, lam = stats.boxcox(arr + shift)\n",
        "            out[\"lambda\"] = float(lam)\n",
        "        return out\n",
        "    if kind == \"inv\":\n",
        "        # tiny shift if zeros present\n",
        "        return {\"shift\": 1e-6 if np.any(np.isclose(arr, 0.0, atol=1e-6)) else 0.0, \"lambda\": None}\n",
        "    raise ValueError(kind)\n",
        "\n",
        "def apply_transform(arr, kind, params):\n",
        "    arr = np.asarray(arr, float)\n",
        "    a = arr + params.get(\"shift\", 0.0)\n",
        "    if kind == \"identity\": return arr\n",
        "    if kind == \"log\":      return np.log(a)\n",
        "    if kind == \"sqrt\":     return np.sqrt(np.where(a < 0, np.nan, a))\n",
        "    if kind == \"inv\":      return 1.0 / a\n",
        "    if kind == \"boxcox\":   return stats.boxcox(a, lmbda=params[\"lambda\"])\n",
        "    raise ValueError(kind)\n",
        "\n",
        "def invert_transform(vals, kind, params):\n",
        "    vals = np.asarray(vals, float)\n",
        "    s = params.get(\"shift\", 0.0)\n",
        "    if kind == \"identity\": return vals\n",
        "    if kind == \"log\":      return np.exp(vals) - s\n",
        "    if kind == \"sqrt\":     return (vals ** 2) - s\n",
        "    if kind == \"inv\":      return (1.0 / vals) - s\n",
        "    if kind == \"boxcox\":\n",
        "        lam = params[\"lambda\"]\n",
        "        if np.isclose(lam, 0.0): return np.exp(vals) - s\n",
        "        return np.power(lam * vals + 1.0, 1.0 / lam) - s\n",
        "    raise ValueError(kind)\n",
        "\n",
        "# Model + diagnostics\n",
        "# ----------------------------\n",
        "def fit_and_diagnose(y_t, x_t):\n",
        "    mask = np.isfinite(y_t) & np.isfinite(x_t)\n",
        "    y, x = y_t[mask], x_t[mask]\n",
        "    X = sm.add_constant(x, has_constant=\"add\")\n",
        "    m = sm.OLS(y, X).fit()\n",
        "\n",
        "    res, fit = m.resid, m.fittedvalues\n",
        "    aic, bic, r2a = float(m.aic), float(m.bic), float(m.rsquared_adj)\n",
        "    rmse_t = float(np.sqrt(np.nanmean(res**2)))\n",
        "\n",
        "    jb_stat, jb_p, _, _ = jarque_bera(res)\n",
        "    bp_stat, bp_p, _, _ = het_breuschpagan(res, sm.add_constant(fit))\n",
        "\n",
        "    try:\n",
        "        reset_p = float(sm.stats.linear_reset(m, power=2, use_f=True).pvalue)\n",
        "    except Exception:\n",
        "        reset_p = np.nan\n",
        "\n",
        "    return m, dict(aic=aic, bic=bic, r2_adj=r2a, rmse_t=rmse_t,\n",
        "                   jb_p=float(jb_p), bp_p=float(bp_p), reset_p=reset_p, nobs=int(m.nobs))\n",
        "\n",
        "def cv_rmse_original(y, x, yk, xk, k=5, seed=42):\n",
        "    y, x = np.asarray(y, float), np.asarray(x, float)\n",
        "    mask = np.isfinite(y) & np.isfinite(x)\n",
        "    y, x = y[mask], x[mask]\n",
        "    n = len(y)\n",
        "    if n < max(6, k + 1): return np.nan\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n); rng.shuffle(idx)\n",
        "    folds = np.array_split(idx, k)\n",
        "\n",
        "    errs = []\n",
        "    for i in range(k):\n",
        "        te = folds[i]; tr = np.concatenate([folds[j] for j in range(k) if j != i])\n",
        "        y_tr, x_tr = y[tr], x[tr]; y_te, x_te = y[te], x[te]\n",
        "\n",
        "        py, px = fit_transform_params(y_tr, yk), fit_transform_params(x_tr, xk)\n",
        "        yt, xt = apply_transform(y_tr, yk, py), apply_transform(x_tr, xk, px)\n",
        "\n",
        "        Xtr = sm.add_constant(xt, has_constant=\"add\")\n",
        "        m = sm.OLS(yt, Xtr, missing=\"drop\").fit()\n",
        "\n",
        "        xte_t = apply_transform(x_te, xk, px)\n",
        "        Xte = sm.add_constant(xte_t, has_constant=\"add\")\n",
        "        yhat_t = m.predict(Xte)\n",
        "        yhat = invert_transform(yhat_t, yk, py)\n",
        "\n",
        "        errs.append(float(np.sqrt(np.nanmean((y_te - yhat) ** 2))))\n",
        "    return float(np.mean(errs))\n",
        "\n",
        "def compare_transformations(X, Y, kfold=5, seed=42):\n",
        "    specs = [\n",
        "        (\"Y|X\",              \"identity\", \"identity\"),\n",
        "        (\"logY|X\",           \"log\",      \"identity\"),\n",
        "        (\"Y|logX\",           \"identity\", \"log\"),\n",
        "        (\"logY|logX\",        \"log\",      \"log\"),\n",
        "        (\"sqrtY|X\",          \"sqrt\",     \"identity\"),\n",
        "        (\"Y|sqrtX\",          \"identity\", \"sqrt\"),\n",
        "        (\"invY|X\",           \"inv\",      \"identity\"),\n",
        "        (\"Y|invX\",           \"identity\", \"inv\"),\n",
        "        (\"boxcoxY|X\",        \"boxcox\",   \"identity\"),\n",
        "        (\"boxcoxY|logX\",     \"boxcox\",   \"log\"),\n",
        "        (\"boxcoxY|sqrtX\",    \"boxcox\",   \"sqrt\"),\n",
        "        (\"boxcoxY|invX\",     \"boxcox\",   \"inv\"),\n",
        "    ]\n",
        "\n",
        "    rows, models = [], {}\n",
        "    X, Y = np.asarray(X, float).ravel(), np.asarray(Y, float).ravel()\n",
        "\n",
        "    for name, yk, xk in specs:\n",
        "        try:\n",
        "            py = fit_transform_params(Y, yk)\n",
        "            px = fit_transform_params(X, xk)\n",
        "            y_t = apply_transform(Y, yk, py)\n",
        "            x_t = apply_transform(X, xk, px)\n",
        "\n",
        "            m, d = fit_and_diagnose(y_t, x_t)\n",
        "            cv = cv_rmse_original(Y, X, yk, xk, k=kfold, seed=seed)\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": name, \"y_transform\": yk, \"x_transform\": xk,\n",
        "                \"y_shift\": py.get(\"shift\", 0.0), \"x_shift\": px.get(\"shift\", 0.0),\n",
        "                \"y_lambda\": py.get(\"lambda\", np.nan),\n",
        "                \"intercept\": float(m.params[0]), \"slope\": float(m.params[1]),\n",
        "                \"adj_R2\": d[\"r2_adj\"], \"RMSE_transformed\": d[\"rmse_t\"],\n",
        "                \"CV_RMSE_original\": cv, \"AIC\": d[\"aic\"], \"BIC\": d[\"bic\"],\n",
        "                \"JB_p\": d[\"jb_p\"], \"BP_p\": d[\"bp_p\"], \"RESET_p\": d[\"reset_p\"],\n",
        "                \"nobs\": d[\"nobs\"],\n",
        "            })\n",
        "            models[name] = m\n",
        "        except Exception as e:\n",
        "            rows.append({\"model\": name, \"y_transform\": yk, \"x_transform\": xk, \"error\": str(e)})\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\n",
        "        by=[\"CV_RMSE_original\", \"BIC\", \"AIC\"], ascending=[True, True, True],\n",
        "        na_position=\"last\"\n",
        "    ).reset_index(drop=True)\n",
        "    return df, models\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Synthetic data where sqrt transform suits Y ~ sqrt(X)\n",
        "    rng = np.random.default_rng(0)\n",
        "    X = np.linspace(1, 100, 500)\n",
        "    Y = 2.0 * np.sqrt(X) + rng.normal(0, 0.3, size=X.size)\n",
        "\n",
        "    summary, models = compare_transformations(X, Y, kfold=5, seed=42)\n",
        "    pd.set_option(\"display.float_format\", lambda v: f\"{v:,.6f}\")\n",
        "    print(summary.head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRNb5tH3kg7",
        "outputId": "990cf83b-a86c-4046-cf46-837aa68e4883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1352601626.py:53: RuntimeWarning: invalid value encountered in power\n",
            "  return np.power(lam * vals + 1.0, 1.0 / lam) - s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        model y_transform x_transform  y_shift  x_shift  y_lambda  intercept      slope   adj_R2  RMSE_transformed  CV_RMSE_original           AIC           BIC     JB_p     BP_p  RESET_p  nobs\n",
            "    logY|logX         log         log 0.000000 0.000000       NaN   0.693197   0.499772 0.995468          0.030066          0.303394 -2,081.412651 -2,072.983435 0.000000 0.000000 0.662048   500\n",
            "      Y|sqrtX    identity        sqrt 0.000000 0.000000       NaN   0.017925   1.996135 0.995617          0.303950          0.303424    232.047023    240.476240 0.045807 0.344933 0.623962   500\n",
            "boxcoxY|sqrtX      boxcox        sqrt 0.000000 0.000000  1.426591  -9.619935   5.731571 0.987176          1.499153          0.548640  1,827.838772  1,836.267989 0.000071 0.000012 0.000000   500\n",
            "    boxcoxY|X      boxcox    identity 0.000000 0.000000  1.426591   5.724549   0.459390 0.985477          1.595349          0.640978  1,890.030814  1,898.460030 0.000000 0.000121 0.000000   500\n",
            "          Y|X    identity    identity 0.000000 0.000000       NaN   5.494776   0.157362 0.961428          0.901648          0.906577  1,319.407548  1,327.836765 0.000000 0.000000 0.000000   500\n",
            "       Y|logX    identity         log 0.000000 0.000000       NaN  -4.671118   4.963791 0.929158          1.221926          1.226608  1,623.366642  1,631.795858 0.000000 0.000000 0.000000   500\n",
            "      sqrtY|X        sqrt    identity 0.000000 0.000000       NaN   2.434268   0.023082 0.905775          0.212950          1.288245   -123.761340   -115.332124 0.000000 0.000000 0.000000   500\n",
            " boxcoxY|logX      boxcox         log 0.000000 0.000000  1.426591 -21.900757  13.928507 0.879754          4.590546          1.308897  2,946.937405  2,955.366621 0.000000 0.000000 0.000000   500\n",
            "       logY|X         log    identity 0.000000 0.000000       NaN   1.805882   0.014078 0.812867          0.193195          1.912584   -221.114961   -212.685744 0.000000 0.000000 0.000000   500\n",
            " boxcoxY|invX      boxcox         inv 0.000000 0.000000  1.426591  32.806006 -81.788850 0.333612         10.806702          3.521469  3,803.104993  3,811.534209 0.005489 0.000000 0.000000   500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation (Pearson \\& Spearman) and its link to $R^2$"
      ],
      "metadata": {
        "id": "Ocm0fxIE6bbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# 1) Linear relation: verify r^2 = R^2\n",
        "# ----------------------------\n",
        "n = 1000\n",
        "beta0, beta1 = 0.5, 1.3\n",
        "X = rng.normal(0.0, 1.0, n)\n",
        "eps = rng.normal(0.0, 0.6, n)\n",
        "Y = beta0 + beta1 * X + eps\n",
        "\n",
        "r_pearson, pval = stats.pearsonr(X, Y)\n",
        "r2 = r_pearson**2\n",
        "\n",
        "X1 = sm.add_constant(X, has_constant=\"add\")\n",
        "ols = sm.OLS(Y, X1).fit()\n",
        "R2 = float(ols.rsquared)\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Pearson correlation and R^2 in simple linear regression\")\n",
        "print(\"=\"*72)\n",
        "print(f\"r (Pearson)   = {r_pearson:.4f}\")\n",
        "print(f\"r^2           = {r2:.4f}\")\n",
        "print(f\"R^2 (OLS)     = {R2:.4f}   <-- should match r^2 up to rounding\")\n",
        "print(\"-\"*72)\n",
        "\n",
        "# 2) Outlier sensitivity: Pearson vs Spearman\n",
        "# ----------------------------\n",
        "X_out = np.append(X, 4.0)\n",
        "Y_out = np.append(Y, 30.0)  # extreme outlier\n",
        "r_p_out, _ = stats.pearsonr(X_out, Y_out)\n",
        "r_s_out, _ = stats.spearmanr(X_out, Y_out)\n",
        "\n",
        "print(\"Outlier sensitivity\")\n",
        "print(f\"  Pearson r (no outlier)   = {r_pearson:.4f}\")\n",
        "print(f\"  Pearson r (with outlier) = {r_p_out:.4f}   <-- can shift notably\")\n",
        "print(f\"  Spearman rho (with outlier) = {r_s_out:.4f} (rank-based, more robust)\")\n",
        "print(\"-\"*72)\n",
        "\n",
        "# 3) Non-linear but monotonic: Spearman captures it better\n",
        "#    Y = exp(X) + noise  => monotonic non-linear\n",
        "# ----------------------------\n",
        "X_mono = rng.normal(0.0, 1.0, n)\n",
        "Y_mono = np.exp(0.8 * X_mono) + rng.normal(0.0, 0.2, n)\n",
        "\n",
        "r_p_mono, _ = stats.pearsonr(X_mono, Y_mono)\n",
        "r_s_mono, _ = stats.spearmanr(X_mono, Y_mono)\n",
        "\n",
        "print(\"Monotonic non-linear relationship\")\n",
        "print(f\"  Pearson r  = {r_p_mono:.4f}   (may understate strength)\")\n",
        "print(f\"  Spearman ρ = {r_s_mono:.4f}   (captures monotonic link)\")\n",
        "print(\"-\"*72)\n",
        "\n",
        "# 4) Spurious correlation in random walks (non-stationary levels)\n",
        "# ----------------------------\n",
        "T = 1500\n",
        "w1 = np.cumsum(rng.normal(0.0, 1.0, T))  # RW1 (price-like)\n",
        "w2 = np.cumsum(rng.normal(0.0, 1.0, T))  # RW2 (independent)\n",
        "\n",
        "r_levels, _ = stats.pearsonr(w1, w2)   # often |r| noticeably > 0 spuriously\n",
        "r_diff, _   = stats.pearsonr(np.diff(w1), np.diff(w2))  # near 0 when differenced\n",
        "\n",
        "print(\"Spurious correlation in non-stationary series\")\n",
        "print(f\"  Pearson r (levels)     = {r_levels:.4f}   (can be misleadingly high)\")\n",
        "print(f\"  Pearson r (differences)= {r_diff:.4f}   (near zero for independent increments)\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zj2uusP6dEj",
        "outputId": "be0ee9b4-1d1c-4652-b01d-5d106fea2abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Pearson correlation and R^2 in simple linear regression\n",
            "========================================================================\n",
            "r (Pearson)   = 0.9032\n",
            "r^2           = 0.8158\n",
            "R^2 (OLS)     = 0.8158   <-- should match r^2 up to rounding\n",
            "------------------------------------------------------------------------\n",
            "Outlier sensitivity\n",
            "  Pearson r (no outlier)   = 0.9032\n",
            "  Pearson r (with outlier) = 0.8181   <-- can shift notably\n",
            "  Spearman rho (with outlier) = 0.8926 (rank-based, more robust)\n",
            "------------------------------------------------------------------------\n",
            "Monotonic non-linear relationship\n",
            "  Pearson r  = 0.8506   (may understate strength)\n",
            "  Spearman ρ = 0.9625   (captures monotonic link)\n",
            "------------------------------------------------------------------------\n",
            "Spurious correlation in non-stationary series\n",
            "  Pearson r (levels)     = -0.7369   (can be misleadingly high)\n",
            "  Pearson r (differences)= -0.0226   (near zero for independent increments)\n",
            "========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolating true alpha from systematic betas"
      ],
      "metadata": {
        "id": "1Wa40_bQPzng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIjTWZj3PuMV",
        "outputId": "17d165c3-02d9-433a-cc8e-3e28ee78340c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Newey–West Factor Alpha Test (synthetic data)\n",
            "========================================================================\n",
            "Observations:       2000\n",
            "Newey–West lags:    18\n",
            "R^2 / Adj R^2:      0.8631 / 0.8628\n",
            "------------------------------------------------------------------------\n",
            "True alpha (annual):        4.00%\n",
            "Estimated alpha (annual):   2.52%\n",
            "Lower bound (annual):       -2.23%\n",
            "Hurdle (annual):            2.00%\n",
            "Deploy?                     False\n",
            "------------------------------------------------------------------------\n",
            "Alpha (per period):\n",
            "  alpha_hat   = 0.000100\n",
            "  se(alpha)   = 0.000114\n",
            "  t(alpha)    = 0.873\n",
            "  p_one_sided = 0.1915\n",
            "------------------------------------------------------------------------\n",
            "Factor loadings (HAC):\n",
            "       beta      se         t  beta_true\n",
            "MKT  0.7900  0.0075  105.9068     0.8000\n",
            "SMB  0.2785  0.0142   19.6338     0.3000\n",
            "HML  0.2018  0.0123   16.4462     0.2000\n",
            "MOM -0.1038  0.0096  -10.8553    -0.1000\n",
            "========================================================================\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "\n",
        "# Helpers\n",
        "def _nw_lags(n: int) -> int:\n",
        "    \"\"\"Automatic Newey–West lag: floor(1.5 * n^(1/3)); at least 1.\"\"\"\n",
        "    return max(1, int(np.floor(1.5 * (n ** (1/3)))))\n",
        "\n",
        "def annualize_mean(mu: float, periods_per_year: int, compounding: bool = False) -> float:\n",
        "    \"\"\"Annualize a per-period mean return.\"\"\"\n",
        "    if compounding:\n",
        "        return (1.0 + mu) ** periods_per_year - 1.0\n",
        "    return periods_per_year * mu\n",
        "\n",
        "def newey_west_alpha_test(\n",
        "    df: pd.DataFrame,\n",
        "    factors: list[str],\n",
        "    ret_col: str = \"ret\",\n",
        "    rf_col: str = \"rf\",\n",
        "    periods_per_year: int = 252,\n",
        "    alpha_sig: float = 0.05,\n",
        "    hurdle_annual: float = 0.0,\n",
        "    compound_annualization: bool = False,\n",
        ") -> dict:\n",
        "    \"\"\"Estimate alpha via OLS with Newey–West (HAC) SEs, one-sided test H0: alpha = 0,\n",
        "    and compute the one-sided lower confidence bound. Decide deploy if lower_annual > hurdle_annual.\n",
        "    \"\"\"\n",
        "    # 1) Prepare y (excess returns) and X (factors + constant)\n",
        "    cols = [ret_col, rf_col] + factors\n",
        "    data = df[cols].dropna()\n",
        "    y = data[ret_col] - data[rf_col]\n",
        "    X = sm.add_constant(data[factors], has_constant=\"add\")\n",
        "\n",
        "    # 2) Fit OLS with HAC covariance (Newey–West) and automatic lag choice\n",
        "    n = int(y.shape[0])\n",
        "    lags = _nw_lags(n)\n",
        "    model = sm.OLS(y.values, X.values)\n",
        "    res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": lags})\n",
        "\n",
        "    # 3) Extract alpha (intercept) and HAC inference\n",
        "    alpha_hat = float(res.params[0])   # const is first column after add_constant\n",
        "    se_alpha  = float(res.bse[0])\n",
        "    t_alpha   = float(res.tvalues[0])\n",
        "    df_resid  = int(res.df_resid)\n",
        "\n",
        "    # 4) One-sided p-value for H_a: alpha > 0\n",
        "    p_one_sided = 1.0 - stats.t.cdf(t_alpha, df_resid)\n",
        "\n",
        "    # 5) One-sided lower (1 - alpha_sig) bound for alpha\n",
        "    tcrit = stats.t.ppf(1.0 - alpha_sig, df_resid)\n",
        "    lower_alpha = alpha_hat - tcrit * se_alpha\n",
        "\n",
        "    # 6) Annualize alpha and compare to hurdle\n",
        "    alpha_annual = annualize_mean(alpha_hat, periods_per_year, compound_annualization)\n",
        "    lower_alpha_annual = annualize_mean(lower_alpha, periods_per_year, compound_annualization)\n",
        "    decision_deploy = bool(lower_alpha_annual > hurdle_annual)\n",
        "\n",
        "    # 7) Package results (betas with HAC t-stats)\n",
        "    betas = pd.Series(res.params[1:], index=factors, name=\"beta\")\n",
        "    betas_se = pd.Series(res.bse[1:], index=factors, name=\"se\")\n",
        "    betas_t  = pd.Series(res.tvalues[1:], index=factors, name=\"t\")\n",
        "\n",
        "    return {\n",
        "        \"n_obs\": n,\n",
        "        \"nw_lags\": lags,\n",
        "        \"alpha_per_period\": alpha_hat,\n",
        "        \"alpha_se\": se_alpha,\n",
        "        \"alpha_t\": t_alpha,\n",
        "        \"alpha_p_one_sided\": p_one_sided,\n",
        "        \"alpha_lower_bound_per_period\": lower_alpha,\n",
        "        \"alpha_annual\": alpha_annual,\n",
        "        \"alpha_lower_bound_annual\": lower_alpha_annual,\n",
        "        \"hurdle_annual\": hurdle_annual,\n",
        "        \"deploy_if_lower_gt_hurdle\": decision_deploy,\n",
        "        \"rsquared\": float(res.rsquared),\n",
        "        \"adj_rsquared\": float(res.rsquared_adj),\n",
        "        \"betas\": pd.concat([betas, betas_se, betas_t], axis=1),  # columns: beta, se, t\n",
        "    }\n",
        "\n",
        "# Synthetic data creation\n",
        "def _ar1(mu: float, phi: float, sigma: float, size: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"AR(1) generator used for factors.\"\"\"\n",
        "    e = rng.normal(0.0, sigma, size)\n",
        "    x = np.empty(size, dtype=float)\n",
        "    x[0] = mu\n",
        "    for t in range(1, size):\n",
        "        x[t] = mu + phi * (x[t - 1] - mu) + e[t]\n",
        "    return x\n",
        "\n",
        "def create_synthetic_df(\n",
        "    T: int = 2000,\n",
        "    seed: int = 42,\n",
        "    periods_per_year: int = 252,\n",
        "    alpha_annual_true: float = 0.04,\n",
        ") -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"Create DataFrame `df` with columns: ret, rf, MKT, SMB, HML, MOM.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Risk-free (constant daily)\n",
        "    rf_annual = 0.015\n",
        "    rf = np.full(T, rf_annual / periods_per_year, dtype=float)\n",
        "\n",
        "    # Factors: AR(1) with small positive drifts\n",
        "    MKT = _ar1(mu=0.00040, phi=0.20, sigma=0.010, size=T, rng=rng)  # ~10% annual mean\n",
        "    SMB = _ar1(mu=0.00010, phi=0.15, sigma=0.006, size=T, rng=rng)\n",
        "    HML = _ar1(mu=0.00010, phi=0.10, sigma=0.006, size=T, rng=rng)\n",
        "    MOM = _ar1(mu=0.00015, phi=0.15, sigma=0.008, size=T, rng=rng)\n",
        "\n",
        "    # True parameters\n",
        "    alpha_pp = alpha_annual_true / periods_per_year\n",
        "    betas_true = np.array([0.80, 0.30, 0.20, -0.10])  # [MKT, SMB, HML, MOM]\n",
        "\n",
        "    # Residuals: AR(1) with heteroskedastic scale depending on |MKT|\n",
        "    rho = 0.40\n",
        "    sigma_eps = 0.006\n",
        "    eps = np.empty(T, dtype=float)\n",
        "    eps[0] = rng.normal(0.0, sigma_eps)\n",
        "    for t in range(1, T):\n",
        "        scale_t = sigma_eps * (0.5 + 0.5 * abs(MKT[t]))  # state-dependent variance\n",
        "        eps[t] = rho * eps[t - 1] + rng.normal(0.0, scale_t)\n",
        "\n",
        "    # Strategy returns: ret = rf + alpha + factors @ beta + noise\n",
        "    factors_mat = np.column_stack([MKT, SMB, HML, MOM])\n",
        "    ret = rf + alpha_pp + factors_mat @ betas_true + eps\n",
        "\n",
        "    dates = pd.date_range(\"2015-01-01\", periods=T, freq=\"B\")\n",
        "    df = pd.DataFrame(\n",
        "        {\"ret\": ret, \"rf\": rf, \"MKT\": MKT, \"SMB\": SMB, \"HML\": HML, \"MOM\": MOM},\n",
        "        index=dates,\n",
        "    )\n",
        "\n",
        "    truth = {\n",
        "        \"true_alpha_annual\": alpha_annual_true,\n",
        "        \"true_betas\": pd.Series(betas_true, index=[\"MKT\", \"SMB\", \"HML\", \"MOM\"]),\n",
        "        \"periods_per_year\": periods_per_year,\n",
        "    }\n",
        "    return df, truth\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Config\n",
        "    T = 2000\n",
        "    PPY = 252\n",
        "    HURDLE = 0.02  # 2% annual lower-bound hurdle\n",
        "    SEED = 42\n",
        "\n",
        "    # Create synthetic DataFrame `df`\n",
        "    df, truth = create_synthetic_df(T=T, seed=SEED, periods_per_year=PPY, alpha_annual_true=0.04)\n",
        "    factors = [\"MKT\", \"SMB\", \"HML\", \"MOM\"]\n",
        "\n",
        "    # Run Newey–West alpha test\n",
        "    out = newey_west_alpha_test(\n",
        "        df,\n",
        "        factors=factors,\n",
        "        periods_per_year=PPY,\n",
        "        hurdle_annual=HURDLE,\n",
        "        alpha_sig=0.05,  # one-sided 95% lower bound\n",
        "    )\n",
        "\n",
        "    # Pretty print summary\n",
        "    def _fmt_pct(x: float) -> str:\n",
        "        return f\"{100 * x:.2f}%\"\n",
        "\n",
        "    print(\"=\" * 72)\n",
        "    print(\"Newey–West Factor Alpha Test (synthetic data)\")\n",
        "    print(\"=\" * 72)\n",
        "    print(f\"Observations:       {out['n_obs']}\")\n",
        "    print(f\"Newey–West lags:    {out['nw_lags']}\")\n",
        "    print(f\"R^2 / Adj R^2:      {out['rsquared']:.4f} / {out['adj_rsquared']:.4f}\")\n",
        "    print(\"-\" * 72)\n",
        "    print(f\"True alpha (annual):        {_fmt_pct(truth['true_alpha_annual'])}\")\n",
        "    print(f\"Estimated alpha (annual):   {_fmt_pct(out['alpha_annual'])}\")\n",
        "    print(f\"Lower bound (annual):       {_fmt_pct(out['alpha_lower_bound_annual'])}\")\n",
        "    print(f\"Hurdle (annual):            {_fmt_pct(out['hurdle_annual'])}\")\n",
        "    print(f\"Deploy?                     {out['deploy_if_lower_gt_hurdle']}\")\n",
        "    print(\"-\" * 72)\n",
        "    print(\"Alpha (per period):\")\n",
        "    print(f\"  alpha_hat   = {out['alpha_per_period']:.6f}\")\n",
        "    print(f\"  se(alpha)   = {out['alpha_se']:.6f}\")\n",
        "    print(f\"  t(alpha)    = {out['alpha_t']:.3f}\")\n",
        "    print(f\"  p_one_sided = {out['alpha_p_one_sided']:.4g}\")\n",
        "    print(\"-\" * 72)\n",
        "\n",
        "    # Betas table (with true betas for reference)\n",
        "    betas_tbl = out[\"betas\"].copy()\n",
        "    betas_tbl[\"beta_true\"] = truth[\"true_betas\"]\n",
        "    betas_tbl = betas_tbl[[\"beta\", \"se\", \"t\", \"beta_true\"]]\n",
        "    print(\"Factor loadings (HAC):\")\n",
        "    print(betas_tbl.to_string(float_format=lambda x: f\"{x: .4f}\"))\n",
        "    print(\"=\" * 72)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neutralizing unintended factor betas (minimum-variance hedge)"
      ],
      "metadata": {
        "id": "KcdVpclgTZ_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def ols_beta(y: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"OLS coefficients with intercept. Returns [alpha, beta_1,...,beta_k].\"\"\"\n",
        "    Xc = sm.add_constant(X, has_constant=\"add\")\n",
        "    return sm.OLS(y, Xc).fit().params\n",
        "\n",
        "def minvar_hedge_weights(beta_S: np.ndarray,\n",
        "                         B_S: np.ndarray,\n",
        "                         Sigma: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Minimum-variance hedge:\n",
        "        minimize w' Sigma w   s.t.   B_S' w = -beta_S\n",
        "    Closed-form: w* = Sigma^{-1} B_S (B_S' Sigma^{-1} B_S)^{-1} (-beta_S)\n",
        "    Shapes:\n",
        "      beta_S: (m,)           m = #factors to neutralize\n",
        "      B_S:    (n, m)         n = #hedge instruments, rows= instruments, cols= factors S\n",
        "      Sigma:  (n, n)         covariance of hedge instruments' returns\n",
        "    Returns w*: (n,)\n",
        "    \"\"\"\n",
        "    # numerical guards\n",
        "    I = np.eye(Sigma.shape[0])\n",
        "    Sinv = np.linalg.inv(Sigma + 1e-10 * I)\n",
        "    M = B_S.T @ Sinv @ B_S\n",
        "    M_inv = np.linalg.inv(M + 1e-10 * np.eye(M.shape[0]))\n",
        "    w = Sinv @ B_S @ (M_inv @ (-beta_S))\n",
        "    return w\n",
        "\n",
        "# Synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(7)\n",
        "T = 1500\n",
        "K = 3  # factors: MKT, SMB, HML\n",
        "dates = pd.date_range(\"2018-01-01\", periods=T, freq=\"B\")\n",
        "\n",
        "# factors (AR(1) with drift)\n",
        "def ar1(mu, phi, sigma, size):\n",
        "    e = rng.normal(0.0, sigma, size)\n",
        "    x = np.empty(size)\n",
        "    x[0] = mu\n",
        "    for t in range(1, size):\n",
        "        x[t] = mu + phi * (x[t-1] - mu) + e[t]\n",
        "    return x\n",
        "\n",
        "MKT = ar1(0.0003, 0.2, 0.01, T)\n",
        "SMB = ar1(0.0001, 0.2, 0.007, T)\n",
        "HML = ar1(0.0001, 0.2, 0.007, T)\n",
        "F = np.column_stack([MKT, SMB, HML])\n",
        "\n",
        "# strategy with unintended betas (needs hedging)\n",
        "alpha_pp = 0.04 / 252\n",
        "beta_true_strat = np.array([0.65, 0.35, 0.20])   # MKT, SMB, HML\n",
        "eps_s = rng.normal(0, 0.006, T)\n",
        "ret_strat = alpha_pp + F @ beta_true_strat + eps_s\n",
        "\n",
        "# hedge instruments: ES (MKT), IWM (SMB), VLUE (HML) with noisy exposures\n",
        "B_hedge_true = np.array([\n",
        "    [ 1.05,  0.05, -0.02],  # ES ~ market future\n",
        "    [ 0.20,  0.95,  0.05],  # IWM ~ small caps\n",
        "    [ 0.10,  0.05,  1.00],  # VLUE ~ value\n",
        "])  # shape (3 instruments × 3 factors)\n",
        "noise = rng.normal(0, 0.004, size=(T, 3))\n",
        "ret_hedge = F @ B_hedge_true.T + noise  # (T × 3)\n",
        "\n",
        "# pack DataFrames\n",
        "df = pd.DataFrame(F, index=dates, columns=[\"MKT\",\"SMB\",\"HML\"])\n",
        "df[\"RET_STRAT\"] = ret_strat\n",
        "df_hedge = pd.DataFrame(ret_hedge, index=dates, columns=[\"ES\",\"IWM\",\"VLUE\"])\n",
        "\n",
        "# Estimate betas by OLS\n",
        "# strategy betas\n",
        "# ----------------------------\n",
        "params_strat = ols_beta(df[\"RET_STRAT\"].values, df[[\"MKT\",\"SMB\",\"HML\"]].values)\n",
        "alpha_hat, beta_hat_strat = params_strat[0], params_strat[1:]\n",
        "\n",
        "# Hedge instrument betas (each instrument on the same factors)\n",
        "B_hat = []\n",
        "for col in df_hedge.columns:\n",
        "    params = ols_beta(df_hedge[col].values, df[[\"MKT\",\"SMB\",\"HML\"]].values)\n",
        "    B_hat.append(params[1:])  # skip intercept -> only factor loadings\n",
        "B_hat = np.vstack(B_hat)  # shape (n_instruments × K)\n",
        "\n",
        "# Choose factors to neutralize and compute weights\n",
        "# ----------------------------\n",
        "factors = [\"MKT\",\"SMB\",\"HML\"]\n",
        "neutralize = [\"MKT\",\"SMB\",\"HML\"]  # customize subset if desired (e.g., [\"MKT\",\"SMB\"])\n",
        "idx = [factors.index(f) for f in neutralize]\n",
        "\n",
        "beta_S = beta_hat_strat[idx]           # (m,)\n",
        "B_S    = B_hat[:, idx]                 # (n × m)\n",
        "Sigma  = np.cov(df_hedge.values, rowvar=False)  # (n × n)\n",
        "\n",
        "w_star = minvar_hedge_weights(beta_S, B_S, Sigma)  # hedge weights per unit strategy notional\n",
        "\n",
        "# Verify exposures before/after\n",
        "# ----------------------------\n",
        "pre_expo  = beta_hat_strat\n",
        "post_expo = beta_hat_strat + B_hat.T @ w_star  # new betas after overlay\n",
        "\n",
        "print(\"Estimated strategy betas (pre-hedge):\", dict(zip(factors, pre_expo.round(3))))\n",
        "print(\"Estimated hedge weights (ES, IWM, VLUE):\", np.round(w_star, 4))\n",
        "print(\"Estimated strategy betas (post-hedge):\", dict(zip(factors, post_expo.round(3))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT_TVUomT8N2",
        "outputId": "f2f606b1-f616-45b6-bad8-3e4b867545c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated strategy betas (pre-hedge): {'MKT': np.float64(0.627), 'SMB': np.float64(0.372), 'HML': np.float64(0.191)}\n",
            "Estimated hedge weights (ES, IWM, VLUE): [-0.5145 -0.3535 -0.1575]\n",
            "Estimated strategy betas (post-hedge): {'MKT': np.float64(0.0), 'SMB': np.float64(0.0), 'HML': np.float64(0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonalizing a signal against factors (Frisch–Waugh–Lovell)"
      ],
      "metadata": {
        "id": "CdWdPMfdVy7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _nw_lags(n: int) -> int:\n",
        "    return max(1, int(np.floor(1.5 * (n ** (1/3)))))\n",
        "\n",
        "def zscore(a: pd.Series) -> pd.Series:\n",
        "    return (a - a.mean()) / a.std(ddof=1)\n",
        "\n",
        "def hac_fit(y: pd.Series, X: pd.DataFrame):\n",
        "    n = len(y)\n",
        "    lags = _nw_lags(n)\n",
        "    res = sm.OLS(y.values, sm.add_constant(X.values, has_constant=\"add\")).fit(\n",
        "        cov_type=\"HAC\", cov_kwds={\"maxlags\": lags}\n",
        "    )\n",
        "    return res, lags\n",
        "\n",
        "def ar1(mu, phi, sigma, size, rng):\n",
        "    e = rng.normal(0.0, sigma, size)\n",
        "    x = np.empty(size)\n",
        "    x[0] = mu\n",
        "    for t in range(1, size):\n",
        "        x[t] = mu + phi * (x[t-1] - mu) + e[t]\n",
        "    return x\n",
        "\n",
        "\n",
        "# Synthetic data\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(123)\n",
        "T, PPY = 2000, 252\n",
        "dates = pd.date_range(\"2015-01-01\", periods=T, freq=\"B\")\n",
        "\n",
        "# Factor and latent edge\n",
        "MKT = ar1(0.00030, 0.20, 0.010, T, rng)      # dominant factor\n",
        "EDGE = ar1(0.00000, 0.35, 0.008, T, rng)     # latent true edge (independent)\n",
        "\n",
        "# Raw signal contaminated by MKT\n",
        "raw_signal = 0.7 * EDGE + 0.6 * MKT + rng.normal(0, 0.005, T)\n",
        "\n",
        "# Returns driven by MKT + lagged EDGE (tradable edge) + noise\n",
        "alpha_pp = 0.05 / PPY                         # ~5% annual edge strength\n",
        "ret_excess = 0.6 * MKT + alpha_pp * zscore(pd.Series(EDGE)).shift(1).fillna(0).values \\\n",
        "             + rng.normal(0, 0.006, T)\n",
        "\n",
        "rf = np.full(T, 0.015 / PPY)                  # constant risk-free (not used here directly)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"RET_XS\": ret_excess,  # excess returns\n",
        "    \"MKT\": MKT,\n",
        "    \"signal_raw\": raw_signal,\n",
        "}, index=dates)\n",
        "\n",
        "# Orthogonalize signal against factors (FWL residualization)\n",
        "# ----------------------------\n",
        "factors = [\"MKT\"]  # extend as needed, e.g., [\"MKT\",\"SMB\",\"HML\",\"MOM\"]\n",
        "# regress signal_raw on factors (contemporaneous) -> residuals\n",
        "res_sig = sm.OLS(df[\"signal_raw\"].values,\n",
        "                 sm.add_constant(df[factors].values, has_constant=\"add\")).fit()\n",
        "signal_ortho = pd.Series(res_sig.resid, index=df.index, name=\"signal_ortho\")\n",
        "\n",
        "# Standardize and apply a one-period trade lag to avoid lookahead\n",
        "s_raw_lag   = zscore(df[\"signal_raw\"]).shift(1)\n",
        "s_ortho_lag = zscore(signal_ortho).shift(1)\n",
        "\n",
        "# Prepare regression datasets (drop NaNs from lagging)\n",
        "data_base = pd.concat([df[\"RET_XS\"], df[factors], s_raw_lag.rename(\"S\")], axis=1).dropna()\n",
        "data_ortho = pd.concat([df[\"RET_XS\"], df[factors], s_ortho_lag.rename(\"S\")], axis=1).dropna()\n",
        "\n",
        "\n",
        "# HAC (Newey–West) regressions\n",
        "#   RET_XS ~ const + factors + S\n",
        "# ----------------------------\n",
        "res_base, lags_base = hac_fit(data_base[\"RET_XS\"], data_base[factors + [\"S\"]])\n",
        "res_ortho, lags_ortho = hac_fit(data_ortho[\"RET_XS\"], data_ortho[factors + [\"S\"]])\n",
        "\n",
        "# Extract signal coefficients (assumes order: const, factors..., S)\n",
        "coef_S_base = float(res_base.params[-1]);  t_S_base = float(res_base.tvalues[-1])\n",
        "coef_S_orth = float(res_ortho.params[-1]); t_S_orth = float(res_ortho.tvalues[-1])\n",
        "\n",
        "# Diagnostics: correlation of tradable signal with MKT (should drop to ~0 after ortho)\n",
        "corr_raw = float(np.corrcoef(s_raw_lag.dropna(), data_base[\"MKT\"])[0,1])\n",
        "corr_ort = float(np.corrcoef(s_ortho_lag.dropna(), data_ortho[\"MKT\"])[0,1])\n",
        "\n",
        "# One-sided p-values for H_a: beta_S > 0\n",
        "df_b = int(res_base.df_resid);  p_raw = 1.0 - stats.t.cdf(t_S_base, df_b)\n",
        "df_o = int(res_ortho.df_resid); p_ort = 1.0 - stats.t.cdf(t_S_orth, df_o)\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Refining Core Logic via Signal Orthogonalization (synthetic demo)\")\n",
        "print(\"=\"*72)\n",
        "print(f\"NW lags (base / ortho): {lags_base} / {lags_ortho}\")\n",
        "print(f\"Corr(S_raw_lag, MKT):   {corr_raw: .4f}\")\n",
        "print(f\"Corr(S_ortho_lag, MKT): {corr_ort: .4f}\")\n",
        "print(\"-\"*72)\n",
        "print(\"RET_XS ~ const + MKT + S\")\n",
        "print(f\"  Raw S   : beta={coef_S_base: .5f},  t_NW={t_S_base: .3f},  p_one_sided={p_raw:.4g}\")\n",
        "print(f\"  Ortho S : beta={coef_S_orth: .5f},  t_NW={t_S_orth: .3f},  p_one_sided={p_ort:.4g}\")\n",
        "print(\"-\"*72)\n",
        "print(f\"R^2 (base / ortho): {res_base.rsquared:.4f} / {res_ortho.rsquared:.4f}\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL1hhqXnVzBZ",
        "outputId": "f69e2cb2-f5d5-4293-ca0a-00bfd991bc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Refining Core Logic via Signal Orthogonalization (synthetic demo)\n",
            "========================================================================\n",
            "NW lags (base / ortho): 18 / 18\n",
            "Corr(S_raw_lag, MKT):    0.1193\n",
            "Corr(S_ortho_lag, MKT): -0.0208\n",
            "------------------------------------------------------------------------\n",
            "RET_XS ~ const + MKT + S\n",
            "  Raw S   : beta=-0.00003,  t_NW=-0.207,  p_one_sided=0.582\n",
            "  Ortho S : beta= 0.00007,  t_NW= 0.570,  p_one_sided=0.2844\n",
            "------------------------------------------------------------------------\n",
            "R^2 (base / ortho): 0.5052 / 0.5052\n",
            "========================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factor-aware multi-strategy allocation"
      ],
      "metadata": {
        "id": "135DD6VwZWjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def ols_alpha_beta(y: np.ndarray, X: np.ndarray):\n",
        "    \"\"\"Return (alpha, betas, resid) from OLS of y on [const, X].\"\"\"\n",
        "    Xc = sm.add_constant(X, has_constant=\"add\")\n",
        "    res = sm.OLS(y, Xc).fit()\n",
        "    alpha = float(res.params[0])\n",
        "    betas = res.params[1:].astype(float)\n",
        "    resid = y - res.fittedvalues\n",
        "    return alpha, betas, resid\n",
        "\n",
        "def nullspace(A: np.ndarray, atol: float = 1e-12, rtol: float = 0.0) -> np.ndarray:\n",
        "    \"\"\"Right-nullspace of A using SVD; columns form an orthonormal basis.\"\"\"\n",
        "    u, s, vh = np.linalg.svd(A, full_matrices=True)\n",
        "    tol = max(atol, rtol * s[0]) if s.size > 0 else atol\n",
        "    nnz = (s >= tol).sum()\n",
        "    return vh[nnz:].T  # shape: (ncols(A) x nullity)\n",
        "\n",
        "def project_to_nullspace(w: np.ndarray, A: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Orthogonal projection of vector w onto Null(A).\"\"\"\n",
        "    N = nullspace(A)\n",
        "    if N.size == 0:  # no degrees of freedom; return zero vector\n",
        "        return np.zeros_like(w)\n",
        "    return N @ (N.T @ w)  # since columns of N are orthonormal\n",
        "\n",
        "def annualize_mean(mu: float, ppy: int) -> float:\n",
        "    return mu * ppy\n",
        "\n",
        "def annualize_vol(sig: float, ppy: int) -> float:\n",
        "    return sig * np.sqrt(ppy)\n",
        "\n",
        "# Synthetic data: 4 strategies, 3 factors (MKT, SMB, HML)\n",
        "# ----------------------------\n",
        "rng = np.random.default_rng(11)\n",
        "T, PPY = 2500, 252\n",
        "dates = pd.date_range(\"2014-01-01\", periods=T, freq=\"B\")\n",
        "\n",
        "def ar1(mu, phi, sigma, n):\n",
        "    e = rng.normal(0.0, sigma, n)\n",
        "    x = np.empty(n)\n",
        "    x[0] = mu\n",
        "    for t in range(1, n):\n",
        "        x[t] = mu + phi * (x[t-1] - mu) + e[t]\n",
        "    return x\n",
        "\n",
        "# Factors\n",
        "MKT = ar1(0.00035, 0.20, 0.010, T)\n",
        "SMB = ar1(0.00010, 0.25, 0.007, T)\n",
        "HML = ar1(0.00010, 0.15, 0.006, T)\n",
        "F   = np.column_stack([MKT, SMB, HML])\n",
        "\n",
        "# True strategy alphas (annual) and betas (rows=strategies, cols=factors)\n",
        "alpha_ann_true = np.array([0.06, 0.04, 0.03, 0.05])\n",
        "alpha_pp_true  = alpha_ann_true / PPY\n",
        "B_true = np.array([\n",
        "    [ 0.80,  0.35,  0.10],  # S1: strong MKT, SMB tilt\n",
        "    [ 0.70,  0.30,  0.05],  # S2: similar hidden betas\n",
        "    [ 0.40,  0.15,  0.30],  # S3: more value exposure\n",
        "    [ 0.75,  0.25,  0.00],  # S4: market-heavy\n",
        "])  # shape (4 x 3)\n",
        "\n",
        "# Idiosyncratic noises (mild AR(1)) per strategy\n",
        "rho = np.array([0.30, 0.35, 0.25, 0.20])\n",
        "sig = np.array([0.007, 0.006, 0.006, 0.007])\n",
        "E = np.zeros((T, 4))\n",
        "E[0] = rng.normal(0, sig, 4)\n",
        "for t in range(1, T):\n",
        "    E[t] = rho * E[t-1] + rng.normal(0, sig, 4)\n",
        "\n",
        "# Strategy returns: r_i = alpha_i + F @ beta_i + eps_i   (use excess returns)\n",
        "R = (F @ B_true.T) + alpha_pp_true + E  # (T x 4)\n",
        "df = pd.DataFrame(R, index=dates, columns=[\"S1\",\"S2\",\"S3\",\"S4\"])\n",
        "df_f = pd.DataFrame(F, index=dates, columns=[\"MKT\",\"SMB\",\"HML\"])\n",
        "\n",
        "\n",
        "# Estimate betas and residuals for each strategy\n",
        "# ----------------------------\n",
        "alphas_hat, betas_hat, residuals = [], [], []\n",
        "for s in df.columns:\n",
        "    a, b, e = ols_alpha_beta(df[s].values, df_f.values)\n",
        "    alphas_hat.append(a); betas_hat.append(b); residuals.append(e)\n",
        "\n",
        "alphas_hat = np.array(alphas_hat)                 # (4,)\n",
        "B_hat      = np.vstack(betas_hat)                 # (4 x 3)\n",
        "E_hat      = np.column_stack(residuals)           # (T x 4)\n",
        "mu_resid   = E_hat.mean(axis=0)                   # (4,)\n",
        "Sigma_resid = np.cov(E_hat, rowvar=False)         # (4 x 4)\n",
        "\n",
        "\n",
        "# Build allocations\n",
        "# 1) Naive equal-weight (EW)\n",
        "# 2) Factor-aware: neutral to {MKT, SMB} and maximize residual Sharpe\n",
        "#    Unconstrained MV-SR direction: w_u ~ Σ^{-1} mu_resid\n",
        "#    Project w_u onto Null(B_subset' ) to enforce factor-neutrality\n",
        "# ----------------------------\n",
        "n_strats = B_hat.shape[0]\n",
        "EW = np.full(n_strats, 1.0 / n_strats)\n",
        "\n",
        "# Unconstrained direction (residual maximum Sharpe)\n",
        "w_u = np.linalg.solve(Sigma_resid + 1e-10 * np.eye(n_strats), mu_resid)\n",
        "\n",
        "# Factor subset to neutralize\n",
        "factors = [\"MKT\",\"SMB\",\"HML\"]\n",
        "subset  = [\"MKT\",\"SMB\"]         # make portfolio neutral to MKT & SMB\n",
        "idx = [factors.index(f) for f in subset]\n",
        "A = B_hat[:, idx].T             # constraints: A @ w = 0  (portfolio exposures)\n",
        "\n",
        "# Project onto Null(A)\n",
        "w_p = project_to_nullspace(w_u, A)\n",
        "\n",
        "# Normalize (net = 1 if possible; otherwise gross = 1)\n",
        "if abs(w_p.sum()) > 1e-8:\n",
        "    w_p = w_p / w_p.sum()\n",
        "else:\n",
        "    w_p = w_p / (np.abs(w_p).sum() + 1e-12)\n",
        "\n",
        "# Diagnostics and comparison\n",
        "# ----------------------------\n",
        "def port_metrics(w):\n",
        "    expo = B_hat.T @ w                         # factor exposures\n",
        "    mu_p = float(mu_resid @ w)                 # residual mean (per period)\n",
        "    sig_p = float(np.sqrt(w @ (Sigma_resid @ w)))  # residual vol (per period)\n",
        "    ann_mu  = annualize_mean(mu_p, PPY)\n",
        "    ann_vol = annualize_vol(sig_p, PPY)\n",
        "    ann_sr  = ann_mu / (ann_vol + 1e-12)\n",
        "    alpha_ann_est = annualize_mean(float(alphas_hat @ w), PPY)\n",
        "    return {\"w\": w, \"expo\": expo, \"mu_resid_ann\": ann_mu, \"vol_resid_ann\": ann_vol,\n",
        "            \"SR_resid_ann\": ann_sr, \"alpha_ann_est\": alpha_ann_est}\n",
        "\n",
        "m_ew = port_metrics(EW)\n",
        "m_p  = port_metrics(w_p)\n",
        "\n",
        "# Print results\n",
        "def fmt(x):\n",
        "    return np.array2string(x, precision=3, floatmode=\"fixed\")\n",
        "\n",
        "print(\"=\"*72)\n",
        "print(\"Factor-aware multi-strategy allocation (synthetic)\")\n",
        "print(\"=\"*72)\n",
        "print(\"Equal-Weight portfolio:\")\n",
        "print(\"  weights        :\", fmt(m_ew[\"w\"]))\n",
        "print(\"  exposures (MKT, SMB, HML):\", fmt(m_ew[\"expo\"]))\n",
        "print(f\"  residual mu/vol/SR (ann): {m_ew['mu_resid_ann']:.2%} / {m_ew['vol_resid_ann']:.2%} / {m_ew['SR_resid_ann']:.2f}\")\n",
        "print(f\"  alpha_est_annual: {m_ew['alpha_ann_est']:.2%}\")\n",
        "print(\"-\"*72)\n",
        "print(\"Factor-neutral (MKT & SMB) residual-optimized portfolio:\")\n",
        "print(\"  weights        :\", fmt(m_p[\"w\"]))\n",
        "print(\"  exposures (MKT, SMB, HML):\", fmt(m_p[\"expo\"]))\n",
        "print(f\"  residual mu/vol/SR (ann): {m_p['mu_resid_ann']:.2%} / {m_p['vol_resid_ann']:.2%} / {m_p['SR_resid_ann']:.2f}\")\n",
        "print(f\"  alpha_est_annual: {m_p['alpha_ann_est']:.2%}\")\n",
        "print(\"=\"*72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJjy9DkjaS54",
        "outputId": "b0a5c747-3094-426a-88a5-3282b1b9810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Factor-aware multi-strategy allocation (synthetic)\n",
            "========================================================================\n",
            "Equal-Weight portfolio:\n",
            "  weights        : [0.250 0.250 0.250 0.250]\n",
            "  exposures (MKT, SMB, HML): [0.656 0.251 0.135]\n",
            "  residual mu/vol/SR (ann): -0.00% / 5.34% / -0.00\n",
            "  alpha_est_annual: 2.36%\n",
            "------------------------------------------------------------------------\n",
            "Factor-neutral (MKT & SMB) residual-optimized portfolio:\n",
            "  weights        : [ 0.142 -0.081 -0.039 -0.056]\n",
            "  exposures (MKT, SMB, HML): [-4.857e-17 -1.388e-17 -2.665e-05]\n",
            "  residual mu/vol/SR (ann): 0.00% / 1.97% / 0.00\n",
            "  alpha_est_annual: 1.91%\n",
            "========================================================================\n"
          ]
        }
      ]
    }
  ]
}